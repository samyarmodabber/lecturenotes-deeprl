
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Bandits &#8212; Deep Reinforcement Learning</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-deeprl/2-tabular/1-Bandits.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Markov Decision Processes" href="2-MDP.html" />
    <link rel="prev" title="2. Math basics" href="../1-intro/2-Math.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-deeprl/2-tabular/1-Bandits.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Bandits" />
<meta property="og:description" content="Bandits  Slides: pdf  n-armed bandits  &lt;div class=&#39;embed-container&#39;&gt;&lt;iframe src=&#39;https://www.youtube.com/embed/PVKWm3rrIOc&#39; frameborder=&#39;0&#39; allowfullscreen&gt;&lt;/if" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-deeprl/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Deep Reinforcement Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Deep Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Tabular RL
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-MDP.html">
   2. Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-DP.html">
   3. Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-MC.html">
   4. Monte-Carlo (MC) methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-TD.html">
   5. Temporal Difference learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-FA.html">
   6. Function approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7-NN.html">
   7. Deep learning
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Model-free RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/1-DQN.html">
   1. Deep Q-Learning (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/2-BeyondDQN.html">
   2. Beyond DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/3-PG.html">
   3. Policy gradient (PG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/4-A3C.html">
   4. Advantage actor-critic (A2C, A3C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/5-DDPG.html">
   5. Deep Deterministic Policy Gradient (DDPG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/6-PPO.html">
   6. Natural gradients (TRPO, PPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/7-SAC.html">
   7. Maximum Entropy RL (SAC)
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Model-based RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-MB/1-MB.html">
   1. Model-based RL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-MB/2-LearnedModels.html">
   2. Learned world models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-MB/3-AlphaGo.html">
   3. AlphaGo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-MB/4-SR.html">
   4. Successor representations
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex3-Sampling.html">
   3. Sampling
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-Sampling.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-Sampling-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex4-Bandits.html">
   4. Bandits (part 1)
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-Bandits.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-Bandits-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex5-Bandits2.html">
   5. Bandits (part 2)
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Bandits2.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Bandits2-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex6-DP.html">
   6. Dynamic programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-DP.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-DP-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex7-Gym.html">
   7. Gym environments
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-Gym.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-Gym-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex8-MC.html">
   8. Monte-Carlo control
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MonteCarlo.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MonteCarlo-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex9-TD.html">
   9. Q-learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-TD.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-TD-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex10-Eligibilitytraces.html">
   10. Eligibility traces
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-Eligibilitytraces.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-Eligibilitytraces-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex11-Keras.html">
   11. Keras
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-Keras.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-Keras-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex12-DQN.html">
   12. DQN
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-DQN.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-DQN-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2-tabular/1-Bandits.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#n-armed-bandits">
   1.1. n-armed bandits
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-based-evaluation">
   1.2. Sampling-based evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#action-selection">
   1.3. Action selection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#greedy-action-selection">
     1.3.1. Greedy action selection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exploration-exploitation-dilemma">
     1.3.2. Exploration-exploitation dilemma
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#epsilon-greedy-action-selection">
     1.3.3.
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -greedy action selection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#softmax-action-selection">
     1.3.4. Softmax action selection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-of-action-selection-for-the-10-armed-bandit">
     1.3.5. Example of action selection for the 10-armed bandit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exploration-schedule">
     1.3.6. Exploration schedule
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimistic-initial-values">
     1.3.7. Optimistic initial values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reinforcement-comparison">
     1.3.8. Reinforcement comparison
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-bandit-algorithm">
     1.3.9. Gradient bandit algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#upper-confidence-bound-action-selection">
     1.3.10. Upper-Confidence-Bound action selection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary-of-evaluative-feedback-methods">
     1.3.11. Summary of evaluative feedback methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contextual-bandits">
   1.4. Contextual bandits
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bandits">
<h1><span class="section-number">1. </span>Bandits<a class="headerlink" href="#bandits" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/lectures/pdf/2.1-Bandits.pdf">pdf</a></p>
<div class="section" id="n-armed-bandits">
<h2><span class="section-number">1.1. </span>n-armed bandits<a class="headerlink" href="#n-armed-bandits" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/PVKWm3rrIOc' frameborder='0' allowfullscreen></iframe></div>
<p>RL evaluates actions through <strong>trial-and-error</strong> rather than comparing its predictions to the correct actions. This is called <strong>evaluative feedback</strong>, as the feedback depends completely on the action taken. Oppositely, supervised learning is a form of <strong>instructive feedback</strong>, as the targets (labels / ground truth) do not depend at all on the prediction.</p>
<p>Evaluative feedback indicates how good the action is, but not whether it is the best or worst action possible. Two forms of RL learning can be distinguished:</p>
<ul class="simple">
<li><p><strong>Associative learning</strong>: inputs are mapped to the best possible outputs (general RL).</p></li>
<li><p><strong>Non-associative learning</strong>: finds one best output, regardless of the current state or past history (bandits).</p></li>
</ul>
<p>The <strong>n-armed bandit</strong>  (or multi-armed bandit) is a non-associative evaluative feedback procedure. Learning and action selection take place in the same single state, while the <span class="math notranslate nohighlight">\(n\)</span> actions have different reward distributions. The goal is to find out through trial and error which action provides the most reward on average.</p>
<div class="figure align-default" id="id2">
<a class="reference internal image-reference" href="../_images/bandit-example.png"><img alt="../_images/bandit-example.png" src="../_images/bandit-example.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.13 </span><span class="caption-text">10-armed bandit with the true value of each action.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>We have the choice between <span class="math notranslate nohighlight">\(N\)</span> different actions <span class="math notranslate nohighlight">\((a_1, ..., a_N)\)</span>. Each action <span class="math notranslate nohighlight">\(a\)</span> taken at time <span class="math notranslate nohighlight">\(t\)</span> provides a <strong>reward</strong> <span class="math notranslate nohighlight">\(r_t\)</span> drawn from the action-specific probability distribution <span class="math notranslate nohighlight">\(r(a)\)</span>. The mathematical expectation of that distribution is the <strong>expected reward</strong>, called the <strong>true value</strong> of the action <span class="math notranslate nohighlight">\(Q^*(a)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    Q^*(a) = \mathbb{E} [r(a)]
\]</div>
<p>The reward distribution also has a <strong>variance</strong>: we usually ignore it in RL, as all we care about is the <strong>optimal action</strong> (but see distributional RL later). If we take the optimal action an infinity of times, we maximize the reward intake <strong>on average</strong>. The question is how to find out the optimal action through <strong>trial and error</strong>, i.e. without knowing the exact reward distribution <span class="math notranslate nohighlight">\(r(a)\)</span>.</p>
<p>The <strong>true value</strong> (or <strong>utility</strong>) of an action is:</p>
<div class="math notranslate nohighlight">
\[Q^*(a) = \mathbb{E} [r(a)]\]</div>
<p>We only have access to <strong>samples</strong> of <span class="math notranslate nohighlight">\(r(a)\)</span> by taking the action at time <span class="math notranslate nohighlight">\(t\)</span> (a <strong>trial</strong>, <strong>play</strong> or <strong>step</strong>). The received rewards <span class="math notranslate nohighlight">\(r_t\)</span> vary around the true value over time. We need to build <strong>estimates</strong> <span class="math notranslate nohighlight">\(Q_t(a)\)</span> of the value of each action based on the samples. These estimates will be very wrong at the beginning, but should get better over time.</p>
</div>
<div class="section" id="sampling-based-evaluation">
<h2><span class="section-number">1.2. </span>Sampling-based evaluation<a class="headerlink" href="#sampling-based-evaluation" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/mHyySub8wVc' frameborder='0' allowfullscreen></iframe></div>
<p>The expectation of the reward distribution can be approximated by the <strong>mean</strong> of its samples (sampling average):</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E} [r(a)] \approx  \frac{1}{N} \sum_{t=1}^N r_t |_{a_t = a}
\]</div>
<p>Suppose that the action <span class="math notranslate nohighlight">\(a\)</span> had been selected <span class="math notranslate nohighlight">\(t\)</span> times, producing rewards</p>
<div class="math notranslate nohighlight">
\[
    (r_1, r_2, ..., r_t)
\]</div>
<p>The estimated value of action <span class="math notranslate nohighlight">\(a\)</span> at play <span class="math notranslate nohighlight">\(t\)</span> is then:</p>
<div class="math notranslate nohighlight">
\[
    Q_t (a) = \frac{r_1 + r_2 + ... + r_t }{t}
\]</div>
<p>Over time, the estimated action-value converges to the true action-value:</p>
<div class="math notranslate nohighlight">
\[
   \lim_{t \to \infty} Q_t (a) = Q^* (a)
\]</div>
<div class="figure align-default" id="id3">
<a class="reference internal image-reference" href="../_images/bandit-samples2.png"><img alt="../_images/bandit-samples2.png" src="../_images/bandit-samples2.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.14 </span><span class="caption-text">The sampling average can correctly approximate the true value of an action given enough samples.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>The drawback of maintaining the mean of the received rewards is that it consumes a lot of memory:</p>
<div class="math notranslate nohighlight">
\[
    Q_t (a) = \frac{r_1 + r_2 + ... + r_t }{t} = \frac{1}{t} \, \sum_{i=1}^{t} r_i
\]</div>
<p>It is possible to update an estimate of the mean in an <strong>online</strong> or incremental manner:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    Q_{t+1}(a) &amp;= \frac{1}{t+1} \, \sum_{i=1}^{t+1} r_i \\
            &amp;= \frac{1}{t+1} \, (r_{t+1} + \sum_{i=1}^{t} r_i )\\
            &amp;= \frac{1}{t+1} \, (r_{t+1} + t \,  Q_{t}(a) ) \\
            &amp;= \frac{1}{t+1} \, (r_{t+1} + (t + 1) \,  Q_{t}(a) - Q_t(a)) 
\end{aligned} 
\end{split}\]</div>
<p>The estimate at time <span class="math notranslate nohighlight">\(t+1\)</span> depends on the previous estimate at time <span class="math notranslate nohighlight">\(t\)</span> and the last reward <span class="math notranslate nohighlight">\(r_{t+1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    Q_{t+1}(a) = Q_t(a) + \frac{1}{t+1} \, (r_{t+1} - Q_t(a)) 
\]</div>
<p>The problem with the exact mean is that it is only exact when the reward distribution is <strong>stationary</strong>, i.e. when the probability distribution does not change over time. If the reward distribution is <strong>non-stationary</strong>, the <span class="math notranslate nohighlight">\(\frac{1}{t+1}\)</span> term will become very small and prevent rapid updates of the mean.</p>
<div class="figure align-default" id="id4">
<a class="reference internal image-reference" href="../_images/bandit-nonstationary1.png"><img alt="../_images/bandit-nonstationary1.png" src="../_images/bandit-nonstationary1.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.15 </span><span class="caption-text">The sampling average have problems when the reward distribution is non-stationary.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>The solution is to replace <span class="math notranslate nohighlight">\(\frac{1}{t+1}\)</span> with a fixed parameter called the <strong>learning rate</strong> (or <strong>step size</strong>) <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    Q_{t+1}(a) &amp; = Q_t(a) + \alpha \, (r_{t+1} - Q_t(a)) \\
                &amp; \\
                &amp; = (1 - \alpha) \, Q_t(a) + \alpha \, r_{t+1}
\end{aligned}
\end{split}\]</div>
<p>The computed value is called a <strong>moving average</strong> (or sliding average), as if one used only a small window of the past history.</p>
<div class="figure align-default" id="id5">
<a class="reference internal image-reference" href="../_images/bandit-nonstationary2.png"><img alt="../_images/bandit-nonstationary2.png" src="../_images/bandit-nonstationary2.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.16 </span><span class="caption-text">The moving average is much more flexible for non-stationary distributions.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>The moving average adapts very fast to changes in the reward distribution and should be used in <strong>non-stationary problems</strong>. It is however not exact and sensible to noise. Choosing the right value for <span class="math notranslate nohighlight">\(\alpha\)</span> can be difficult.</p>
<p>The form of this <strong>update rule</strong> is very important to remember:</p>
<div class="math notranslate nohighlight">
\[
    \text{new estimate} = \text{old estimate} + \alpha \, (\text{target} - \text{old estimate}) 
\]</div>
<p>Estimates following this update rule track the mean of their sampled target values. <span class="math notranslate nohighlight">\(\text{target} - \text{old estimate}\)</span> is the <strong>prediction error</strong> between the target and the estimate.</p>
</div>
<div class="section" id="action-selection">
<h2><span class="section-number">1.3. </span>Action selection<a class="headerlink" href="#action-selection" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/yg3yI7YmEa8' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="greedy-action-selection">
<h3><span class="section-number">1.3.1. </span>Greedy action selection<a class="headerlink" href="#greedy-action-selection" title="Permalink to this headline">¶</a></h3>
<p>Let’s suppose we have formed reasonable estimates of the Q-values <span class="math notranslate nohighlight">\(Q_t(a)\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>. Which action should we do next? If we select the next action <span class="math notranslate nohighlight">\(a_{t+1}\)</span> randomly (<strong>random agent</strong>), we do not maximize the rewards we receive, but we can continue learning the Q-values. Choosing the action to perform next is called <strong>action selection</strong> and several schemes are possible.</p>
<div class="figure align-default" id="id6">
<a class="reference internal image-reference" href="../_images/bandit-estimates-greedy.png"><img alt="../_images/bandit-estimates-greedy.png" src="../_images/bandit-estimates-greedy.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.17 </span><span class="caption-text">Greedy action selection.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>The <strong>greedy action</strong> is the action whose estimated value is <strong>maximal</strong> at time <span class="math notranslate nohighlight">\(t\)</span> based on our current estimates:</p>
<div class="math notranslate nohighlight">
\[
    a^*_t = \text{argmax}_{a} Q_t(a)
\]</div>
<p>If our estimates <span class="math notranslate nohighlight">\(Q_t\)</span> are correct (i.e. close from <span class="math notranslate nohighlight">\(Q^*\)</span>), the greedy action is the <strong>optimal action</strong> and we maximize the rewards on average. If our estimates are wrong, the agent will perform <strong>sub-optimally</strong>.</p>
<p>This defines the <strong>greedy policy</strong>, where the probability of taking the greedy action is 1 and the probability of selecting another action is 0:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \pi(a) = \begin{cases} 1 \; \text{if} \; a = a_t^* \\ 0 \; \text{otherwise.} \end{cases}
\end{split}\]</div>
<p>The greedy policy is <strong>deterministic</strong>: the action taken is always the same for a fixed <span class="math notranslate nohighlight">\(Q_t\)</span>.</p>
</div>
<div class="section" id="exploration-exploitation-dilemma">
<h3><span class="section-number">1.3.2. </span>Exploration-exploitation dilemma<a class="headerlink" href="#exploration-exploitation-dilemma" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id7">
<a class="reference internal image-reference" href="../_images/bandit-estimates-greedy-problem1.png"><img alt="../_images/bandit-estimates-greedy-problem1.png" src="../_images/bandit-estimates-greedy-problem1.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.18 </span><span class="caption-text">Greedy action selection only works when the estimates are good enough.</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id8">
<a class="reference internal image-reference" href="../_images/bandit-estimates-greedy-problem2.png"><img alt="../_images/bandit-estimates-greedy-problem2.png" src="../_images/bandit-estimates-greedy-problem2.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.19 </span><span class="caption-text">Estimates are initially bad (e.g. 0 here), so an action is sampled randomly and a reward is received.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id9">
<a class="reference internal image-reference" href="../_images/bandit-estimates-greedy-problem3.png"><img alt="../_images/bandit-estimates-greedy-problem3.png" src="../_images/bandit-estimates-greedy-problem3.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.20 </span><span class="caption-text">The Q-value of that action becomes positive, so it becomes the greedy action.</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id10">
<a class="reference internal image-reference" href="../_images/bandit-estimates-greedy-problem4.png"><img alt="../_images/bandit-estimates-greedy-problem4.png" src="../_images/bandit-estimates-greedy-problem4.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.21 </span><span class="caption-text">Greedy action selection will always select that action, although the second one would have been better.</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>This <strong>exploration-exploitation</strong> dilemma is the hardest problem in RL:</p>
<ul class="simple">
<li><p><strong>Exploitation</strong> is using the current estimates to select an action: they might be wrong!</p></li>
<li><p><strong>Exploration</strong> is selecting non-greedy actions in order to improve their estimates: they might not be optimal!</p></li>
</ul>
<p>One has to balance exploration and exploitation over the course of learning:</p>
<ul class="simple">
<li><p>More exploration at the beginning of learning, as the estimates are initially wrong.</p></li>
<li><p>More exploitation at the end of learning, as the estimates get better.</p></li>
</ul>
</div>
<div class="section" id="epsilon-greedy-action-selection">
<h3><span class="section-number">1.3.3. </span><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection<a class="headerlink" href="#epsilon-greedy-action-selection" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id11">
<a class="reference internal image-reference" href="../_images/bandit-estimates-epsilongreedy.png"><img alt="../_images/bandit-estimates-epsilongreedy.png" src="../_images/bandit-estimates-epsilongreedy.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.22 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
<p><strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection</strong> ensures a trade-off between exploitation and exploration. The greedy action is selected with probability <span class="math notranslate nohighlight">\(1 - \epsilon\)</span> (with <span class="math notranslate nohighlight">\(0 &lt; \epsilon &lt;1\)</span>), the others with probability <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \pi(a) = \begin{cases} 1 - \epsilon \; \text{if} \; a = a_t^* \\ \frac{\epsilon}{|\mathcal{A}| - 1} \; \text{otherwise.} \end{cases}
\end{split}\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(\epsilon\)</span> controls the level of exploration: the higher <span class="math notranslate nohighlight">\(\epsilon\)</span>, the more exploration. One can set <span class="math notranslate nohighlight">\(\epsilon\)</span> high at the beginning of learning and progressively reduce it to exploit more. However, it chooses equally among all actions: the worst action is as likely to be selected as the next-to-best action.</p>
</div>
<div class="section" id="softmax-action-selection">
<h3><span class="section-number">1.3.4. </span>Softmax action selection<a class="headerlink" href="#softmax-action-selection" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id12">
<a class="reference internal image-reference" href="../_images/bandit-estimates-softmax.png"><img alt="../_images/bandit-estimates-softmax.png" src="../_images/bandit-estimates-softmax.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.23 </span><span class="caption-text">Softmax action selection</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Softmax action selection</strong>  defines the probability of choosing an action using all estimated value. It represents the policy using a Gibbs (or Boltzmann) distribution:</p>
<div class="math notranslate nohighlight">
\[
    \pi(a) = \frac{\exp \frac{Q_t(a)}{\tau}}{ \sum_b \exp \frac{Q_t(b)}{\tau}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau\)</span> is a positive parameter called the <strong>temperature</strong>.</p>
<p><img alt="" src="../_images/bandit-estimates-softmax2.png" /></p>
<p>Just as <span class="math notranslate nohighlight">\(\epsilon\)</span>, the temperature <span class="math notranslate nohighlight">\(\tau\)</span> controls the level of exploration:</p>
<ul class="simple">
<li><p>High temperature causes the actions to be nearly equiprobable (<strong>random agent</strong>).</p></li>
<li><p>Low temperature causes the greediest actions only to be selected (<strong>greedy agent</strong>).</p></li>
</ul>
</div>
<div class="section" id="example-of-action-selection-for-the-10-armed-bandit">
<h3><span class="section-number">1.3.5. </span>Example of action selection for the 10-armed bandit<a class="headerlink" href="#example-of-action-selection-for-the-10-armed-bandit" title="Permalink to this headline">¶</a></h3>
<p><strong>Procedure:</strong></p>
<ul class="simple">
<li><p>N = 10 possible actions with Q-values <span class="math notranslate nohighlight">\(Q^*(a_1), ... , Q^*(a_{10})\)</span> randomly chosen in <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>.</p></li>
<li><p>Each reward <span class="math notranslate nohighlight">\(r_t\)</span> is drawn from a normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(Q^*(a), 1)\)</span> depending on the selected action.</p></li>
<li><p>Estimates <span class="math notranslate nohighlight">\(Q_t(a)\)</span> are initialized to 0.</p></li>
<li><p>The algorithms run for 1000 plays, and the results are averaged 200 times.</p></li>
</ul>
<p><strong>Greedy action selection</strong></p>
<p>Greedy action selection allows to get rid quite early of the actions with negative rewards. However, it may stick with the first positive action it founds, probably not the optimal one. The more actions you have, the more likely you will get stuck in a <strong>suboptimal policy</strong>.</p>
<div class="figure align-default" id="id13">
<a class="reference internal image-reference" href="../_images/bandit-greedy.gif"><img alt="../_images/bandit-greedy.gif" src="../_images/bandit-greedy.gif" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.24 </span><span class="caption-text">Greedy action selection on a 10-armed bandit for 200 plays.</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<p><strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection</strong></p>
<p><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection continues to explore after finding a good (but often suboptimal) action. It is not always able to recognize the optimal action (it depends on the variance of the rewards).</p>
<div class="figure align-default" id="id14">
<a class="reference internal image-reference" href="../_images/bandit-egreedy.gif"><img alt="../_images/bandit-egreedy.gif" src="../_images/bandit-egreedy.gif" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.25 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection on a 10-armed bandit for 200 plays.</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Softmax action selection</strong></p>
<p>Softmax action selection explores more consistently the available actions. The estimated Q-values are much closer to the true values than with (<span class="math notranslate nohighlight">\(\epsilon\)</span>-)greedy methods.</p>
<div class="figure align-default" id="id15">
<a class="reference internal image-reference" href="../_images/bandit-softmax.gif"><img alt="../_images/bandit-softmax.gif" src="../_images/bandit-softmax.gif" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.26 </span><span class="caption-text">Softmax action selection on a 10-armed bandit for 200 plays.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Comparison</strong></p>
<div class="figure align-default" id="id16">
<a class="reference internal image-reference" href="../_images/bandit-comparison1.png"><img alt="../_images/bandit-comparison1.png" src="../_images/bandit-comparison1.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.27 </span><span class="caption-text">Average reward per step.</span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id17">
<a class="reference internal image-reference" href="../_images/bandit-comparison2.png"><img alt="../_images/bandit-comparison2.png" src="../_images/bandit-comparison2.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.28 </span><span class="caption-text">Frequency of selection of the optimal action.</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<p>The <strong>greedy</strong> method learns faster at the beginning, but get stuck in the long-term by choosing <strong>suboptimal</strong> actions (50% of trials). <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy methods perform better on the long term, because they continue to explore. High values for <span class="math notranslate nohighlight">\(\epsilon\)</span> provide more exploration, hence find the optimal action earlier, but also tend to deselect it more often: with a limited number of plays, it may collect less reward than smaller values of <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<div class="figure align-default" id="id18">
<a class="reference internal image-reference" href="../_images/bandit-comparison3.png"><img alt="../_images/bandit-comparison3.png" src="../_images/bandit-comparison3.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.29 </span><span class="caption-text">Average reward per step.</span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id19">
<a class="reference internal image-reference" href="../_images/bandit-comparison4.png"><img alt="../_images/bandit-comparison4.png" src="../_images/bandit-comparison4.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.30 </span><span class="caption-text">Frequency of selection of the optimal action.</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<p>The softmax does not necessarily find a better solution than <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy, but it tends to find it <strong>faster</strong> (depending on <span class="math notranslate nohighlight">\(\epsilon\)</span> or <span class="math notranslate nohighlight">\(\tau\)</span>), as it does not lose time exploring obviously bad solutions. <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy or softmax methods work best when the variance of rewards is high. If the variance is zero (always the same reward value), the greedy method would find the optimal action more rapidly: the agent only needs to try each action once.</p>
</div>
<div class="section" id="exploration-schedule">
<h3><span class="section-number">1.3.6. </span>Exploration schedule<a class="headerlink" href="#exploration-schedule" title="Permalink to this headline">¶</a></h3>
<p>A useful technique to cope with the <strong>exploration-exploitation dilemma</strong> is to slowly decrease the value of <span class="math notranslate nohighlight">\(\epsilon\)</span> or <span class="math notranslate nohighlight">\(\tau\)</span> with the number of plays. This allows for more exploration at the beginning of learning and more exploitation towards the end. It is however hard to find the right <strong>decay rate</strong> for the exploration parameters.</p>
<div class="figure align-default" id="id20">
<a class="reference internal image-reference" href="../_images/bandit-scheduling.png"><img alt="../_images/bandit-scheduling.png" src="../_images/bandit-scheduling.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.31 </span><span class="caption-text">Exploration parameters such as <span class="math notranslate nohighlight">\(\epsilon\)</span> or <span class="math notranslate nohighlight">\(\tau\)</span> can be scheduled to allow more exploration at the beginning of learning.</span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</div>
<p>The performance is worse at the beginning, as the agent explores with a high temperature. But as the agent becomes greedier and greedier, the performance become more <strong>optimal</strong> than with a fixed temperature.</p>
<div class="figure align-default" id="id21">
<a class="reference internal image-reference" href="../_images/bandit-comparison5.png"><img alt="../_images/bandit-comparison5.png" src="../_images/bandit-comparison5.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.32 </span><span class="caption-text">Average reward per step.</span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id22">
<a class="reference internal image-reference" href="../_images/bandit-comparison6.png"><img alt="../_images/bandit-comparison6.png" src="../_images/bandit-comparison6.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.33 </span><span class="caption-text">Frequency of selection of the optimal action.</span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="optimistic-initial-values">
<h3><span class="section-number">1.3.7. </span>Optimistic initial values<a class="headerlink" href="#optimistic-initial-values" title="Permalink to this headline">¶</a></h3>
<p>The problem with online evaluation is that it depends a lot on the initial estimates <span class="math notranslate nohighlight">\(Q_0\)</span>.</p>
<ul class="simple">
<li><p>If the initial estimates are already quite good (expert knowledge), the Q-values will converge very fast.</p></li>
<li><p>If the initial estimates are very wrong, we will need a lot of updates to correctly estimate the true values.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    &amp;Q_{t+1}(a) = (1 - \alpha) \, Q_t(a) + \alpha \, r_{t+1}  \\
    &amp;\\
    &amp; \rightarrow Q_1(a) = (1 - \alpha) \, Q_0(a) + \alpha \, r_1 \\
    &amp; \rightarrow Q_2(a) = (1 - \alpha) \, Q_1(a) + \alpha \, r_2 = (1- \alpha)^2 \, Q_0(a) + (1-\alpha)\alpha \, r_1 + \alpha r_2 \\
\end{aligned}
\end{split}\]</div>
<p>The influence of <span class="math notranslate nohighlight">\(Q_0\)</span> on <span class="math notranslate nohighlight">\(Q_t\)</span> <strong>fades</strong> quickly with <span class="math notranslate nohighlight">\((1-\alpha)^t\)</span>, but that can be lost time or lead to a suboptimal policy. However, we can use this at our advantage with <strong>optimistic initialization</strong>.</p>
<p>By choosing very high initial values for the estimates (they can only decrease), one can ensure that all possible actions will be selected during learning by the greedy method, solving the <strong>exploration problem</strong>. This leads however to an <strong>overestimation</strong> of the value of other actions.</p>
<div class="figure align-default" id="id23">
<a class="reference internal image-reference" href="../_images/bandit-optimistic.gif"><img alt="../_images/bandit-optimistic.gif" src="../_images/bandit-optimistic.gif" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.34 </span><span class="caption-text">Optimistic initialization.</span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="reinforcement-comparison">
<h3><span class="section-number">1.3.8. </span>Reinforcement comparison<a class="headerlink" href="#reinforcement-comparison" title="Permalink to this headline">¶</a></h3>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/C5xXGfgeDOk' frameborder='0' allowfullscreen></iframe></div>
<p>Actions followed by large rewards should be made more likely to recur, whereas actions followed by small rewards should be made less likely to recur. But what is a large/small reward? Is a reward of 5 large or small? <strong>Reinforcement comparison</strong> methods only maintain a <strong>preference</strong> <span class="math notranslate nohighlight">\(p_t(a)\)</span> for each action, which is not exactly its Q-value. The preference for an action is updated after each play, according to the update rule:</p>
<div class="math notranslate nohighlight">
\[
    p_{t+1}(a_t) =    p_{t}(a_t) + \beta \, (r_t - \tilde{r}_t)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{r}_t\)</span> is the moving average of the recently received rewards (regardless the action):</p>
<div class="math notranslate nohighlight">
\[
    \tilde{r}_{t+1} =  \tilde{r}_t + \alpha \, (r_t - \tilde{r}_t)
\]</div>
<ul class="simple">
<li><p>If an action brings more reward than usual (<strong>good surprise</strong>), we increase the preference for that action.</p></li>
<li><p>If an action brings less reward than usual (<strong>bad surprise</strong>), we decrease the preference for that action.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\beta &gt; 0\)</span> and <span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 1\)</span> are two constant parameters.</p>
<p>The preferences can be used to select the action using the softmax method just like the Q-values (without temperature):</p>
<div class="math notranslate nohighlight">
\[
    \pi_t (a) = \frac{\exp p_t(a)}{ \sum_b \exp p_t(b)}
\]</div>
<div class="figure align-default" id="id24">
<a class="reference internal image-reference" href="../_images/bandit-reinforcementcomparison.gif"><img alt="../_images/bandit-reinforcementcomparison.gif" src="../_images/bandit-reinforcementcomparison.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.35 </span><span class="caption-text">Reinforcement comparison.</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</div>
<p>Reinforcement comparison can be very effective, as it does not rely only on the rewards received, but also on their comparison with a <strong>baseline</strong>, the average reward. This idea is at the core of <strong>actor-critic</strong> architectures which we will see later. The initial average reward <span class="math notranslate nohighlight">\(\tilde{r}_{0}\)</span> can be set optimistically to encourage exploration.</p>
<div class="figure align-default" id="id25">
<a class="reference internal image-reference" href="../_images/bandit-comparison7.png"><img alt="../_images/bandit-comparison7.png" src="../_images/bandit-comparison7.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.36 </span><span class="caption-text">Average reward per step.</span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id26">
<a class="reference internal image-reference" href="../_images/bandit-comparison8.png"><img alt="../_images/bandit-comparison8.png" src="../_images/bandit-comparison8.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.37 </span><span class="caption-text">Frequency of selection of the optimal action.</span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="gradient-bandit-algorithm">
<h3><span class="section-number">1.3.9. </span>Gradient bandit algorithm<a class="headerlink" href="#gradient-bandit-algorithm" title="Permalink to this headline">¶</a></h3>
<p>One can even go further than reinforcement comparison: Instead of only increasing the preference for the executed action if it brings more usual than rewards, we could also decrease the preference for the other actions. The preferences are used to select an action <span class="math notranslate nohighlight">\(a_t\)</span> <em>via</em> softmax:</p>
<div class="math notranslate nohighlight">
\[
    \pi_t (a) = \frac{\exp p_t(a)}{ \sum_b \exp p_t(b)}
\]</div>
<p>The update rule for the <strong>action taken</strong> <span class="math notranslate nohighlight">\(a_t\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
    p_{t+1}(a_t) =    p_{t}(a_t) + \beta \, (r_t - \tilde{r}_t) \, (1 - \pi_t(a_t))
\]</div>
<p>The update rule for the <strong>other actions</strong> <span class="math notranslate nohighlight">\(a\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
    p_{t+1}(a) =    p_{t}(a) - \beta \, (r_t - \tilde{r}_t) \, \pi_t(a)
\]</div>
<p>The reward <strong>baseline</strong> is updated with:</p>
<div class="math notranslate nohighlight">
\[
    \tilde{r}_{t+1} =  \tilde{r}_t + \alpha \, (r_t - \tilde{r}_t)
\]</div>
<p>The preference can increase become quite high, making the policy greedy towards the end. No need for a temperature parameter!</p>
<div class="figure align-default" id="id27">
<a class="reference internal image-reference" href="../_images/bandit-gradientbandit.gif"><img alt="../_images/bandit-gradientbandit.gif" src="../_images/bandit-gradientbandit.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.38 </span><span class="caption-text">Reinforcement comparison.</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
<p>Gradient bandit is not always better than reinforcement comparison, but learns initially faster (depending on the parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>).</p>
<div class="figure align-default" id="id28">
<a class="reference internal image-reference" href="../_images/bandit-comparison9.png"><img alt="../_images/bandit-comparison9.png" src="../_images/bandit-comparison9.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.39 </span><span class="caption-text">Average reward per step.</span><a class="headerlink" href="#id28" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id29">
<a class="reference internal image-reference" href="../_images/bandit-comparison10.png"><img alt="../_images/bandit-comparison10.png" src="../_images/bandit-comparison10.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.40 </span><span class="caption-text">Frequency of selection of the optimal action.</span><a class="headerlink" href="#id29" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="upper-confidence-bound-action-selection">
<h3><span class="section-number">1.3.10. </span>Upper-Confidence-Bound action selection<a class="headerlink" href="#upper-confidence-bound-action-selection" title="Permalink to this headline">¶</a></h3>
<p>In the previous methods, <strong>exploration</strong> is controlled by an external parameter (<span class="math notranslate nohighlight">\(\epsilon\)</span> or <span class="math notranslate nohighlight">\(\tau\)</span>) which is <strong>global</strong> to each action an must be scheduled. A much better approach would be to decide whether to explore an action based on the <strong>uncertainty</strong> about its Q-value:</p>
<ul class="simple">
<li><p>If we are certain about the value of an action, there is no need to explore it further, we only have to exploit it if it is good.</p></li>
</ul>
<p>The <strong>central limit theorem</strong> tells us that the variance of a sampling estimator decreases with the number of samples:</p>
<ul class="simple">
<li><p>The distribution of sample averages is normally distributed with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\frac{\sigma^2}{N}\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[S_N \sim \mathcal{N}(\mu, \frac{\sigma}{\sqrt{N}})\]</div>
<p>The more you explore an action <span class="math notranslate nohighlight">\(a\)</span>, the smaller the variance of <span class="math notranslate nohighlight">\(Q_t(a)\)</span>, the more certain you are about the estimation, the less you need to explore it.</p>
<p><strong>Upper-Confidence-Bound</strong> (UCB) action selection is a <strong>greedy</strong> action selection method that uses an <strong>exploration</strong> bonus:</p>
<div class="math notranslate nohighlight">
\[
    a^*_t = \text{argmax}_{a} \left( Q_t(a) + c \, \sqrt{\frac{\ln t}{N_t(a)}} \right)
\]</div>
<p><span class="math notranslate nohighlight">\(Q_t(a)\)</span> is the current estimated value of <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(N_t(a)\)</span> is the number of times the action <span class="math notranslate nohighlight">\(a\)</span> has already been selected. It realizes a balance between trusting the estimates <span class="math notranslate nohighlight">\(Q_t(a)\)</span> and exploring uncertain actions which have not been explored much yet.</p>
<p>The term <span class="math notranslate nohighlight">\(\sqrt{\frac{\ln t}{N_t(a)}}\)</span> is an estimate of the variance of <span class="math notranslate nohighlight">\(Q_t(a)\)</span>. The sum of both terms is an <strong>upper-bound</strong> of the true value <span class="math notranslate nohighlight">\(\mu + \sigma\)</span>. When an action has not been explored much yet, the uncertainty term will dominate and the action be explored, although its estimated value might be low. When an action has been sufficiently explored, the uncertainty term goes to 0 and we greedily follow <span class="math notranslate nohighlight">\(Q_t(a)\)</span>. The <strong>exploration-exploitation</strong> trade-off is automatically adjusted by counting visits to an action.</p>
<div class="figure align-default" id="id30">
<a class="reference internal image-reference" href="../_images/bandit-ucb.gif"><img alt="../_images/bandit-ucb.gif" src="../_images/bandit-ucb.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.41 </span><span class="caption-text">Upper-Confidence-Bound action selection.</span><a class="headerlink" href="#id30" title="Permalink to this image">¶</a></p>
</div>
<p>The “smart” exploration of UCB allows to find the optimal action faster.</p>
<div class="figure align-default" id="id31">
<a class="reference internal image-reference" href="../_images/bandit-comparison11.png"><img alt="../_images/bandit-comparison11.png" src="../_images/bandit-comparison11.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.42 </span><span class="caption-text">Average reward per step.</span><a class="headerlink" href="#id31" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id32">
<a class="reference internal image-reference" href="../_images/bandit-comparison12.png"><img alt="../_images/bandit-comparison12.png" src="../_images/bandit-comparison12.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.43 </span><span class="caption-text">Frequency of selection of the optimal action.</span><a class="headerlink" href="#id32" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="summary-of-evaluative-feedback-methods">
<h3><span class="section-number">1.3.11. </span>Summary of evaluative feedback methods<a class="headerlink" href="#summary-of-evaluative-feedback-methods" title="Permalink to this headline">¶</a></h3>
<p>Greedy, <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy, softmax, reinforcement comparison, gradient bandit and UCB all have their own advantages and disadvantages depending on the type of problem: stationary or not, high or low reward variance, etc…</p>
<p>These simple techniques are the most useful ones for bandit-like problems: more sophisticated ones exist, but they either make too restrictive assumptions, or are computationally intractable.</p>
<p><strong>Take home messages:</strong></p>
<ol class="simple">
<li><p>RL tries to <strong>estimate values</strong> based on sampled rewards.</p></li>
<li><p>One has to balance <strong>exploitation and exploration</strong> throughout learning with the right <strong>action selection scheme</strong>.</p></li>
<li><p>Methods exploring more find <strong>better policies</strong>, but are initially slower.</p></li>
</ol>
</div>
</div>
<div class="section" id="contextual-bandits">
<h2><span class="section-number">1.4. </span>Contextual bandits<a class="headerlink" href="#contextual-bandits" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/hI9CzfE8xtA' frameborder='0' allowfullscreen></iframe></div>
<p>In contextual bandits, the obtained rewards do not only depend on the action <span class="math notranslate nohighlight">\(a\)</span>, but also on the <strong>state</strong> or <strong>context</strong> <span class="math notranslate nohighlight">\(s\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    r_{t+1} \sim r(s, a)
\]</div>
<p>For example, the n-armed bandit could deliver rewards with different probabilities depending on who plays, the time of the year or the availability of funds in the casino. The problem is simply to estimate <span class="math notranslate nohighlight">\(Q(s, a)\)</span> instead of <span class="math notranslate nohighlight">\(Q(a)\)</span>…</p>
<div class="figure align-default" id="id33">
<a class="reference internal image-reference" href="../_images/bandit-hierarchy.png"><img alt="../_images/bandit-hierarchy.png" src="../_images/bandit-hierarchy.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.44 </span><span class="caption-text">Contextual bandits are an intermediary problem between bandits and the full RL setup, as rewards depend on the state, but the state is not influenced by the action. Source: <a class="reference external" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c</a></span><a class="headerlink" href="#id33" title="Permalink to this image">¶</a></p>
</div>
<p>Contextual bandits are for example useful for <strong>Recommender systems</strong>: actions are the display of an  advertisement, the context (or state) represents the user features / identity (possibly learned by an autoencoder), and the reward represents whether the user clicks on the ad or not.</p>
<div class="figure align-default" id="id34">
<a class="reference internal image-reference" href="../_images/contextualbandit.gif"><img alt="../_images/contextualbandit.gif" src="../_images/contextualbandit.gif" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.45 </span><span class="caption-text">Contextual bandits are used in recommender systems. Source: <a class="reference external" href="https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/">https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/</a></span><a class="headerlink" href="#id34" title="Permalink to this image">¶</a></p>
</div>
<p>Some efficient algorithms have been developed recently, for example <a class="bibtex reference internal" href="../zreferences.html#agarwal2014" id="id1">[Agarwal et al., 2014]</a>, but we will not go into details in this course.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2-tabular"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../1-intro/2-Math.html" title="previous page"><span class="section-number">2. </span>Math basics</a>
    <a class='right-next' id="next-link" href="2-MDP.html" title="next page"><span class="section-number">2. </span>Markov Decision Processes</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>