

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Dynamic Programming &#8212; Deep Reinforcement Learning</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-deeprl/2-tabular/3-DP.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Monte-Carlo (MC) methods" href="4-MC.html" />
    <link rel="prev" title="2. Markov Decision Processes" href="2-MDP.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-deeprl/2-tabular/3-DP.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Dynamic Programming" />
<meta property="og:description" content="Dynamic Programming  Slides: pdf    Generalized Policy Iteration. Source: [SB98]  Dynamic Programming (DP) belongs to Generalized Policy Iteration (GPI) family " />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-deeprl/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Deep Reinforcement Learning</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Deep Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tabular RL
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1-Bandits.html">
   1. Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-MDP.html">
   2. Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-MC.html">
   4. Monte-Carlo (MC) methods
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex3-Sampling.html">
   3. Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex4-Bandits.html">
   4. Bandits (part 1)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex5-Bandits2.html">
   5. Bandits (part 2)
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2-tabular/3-DP.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-iteration">
   3.1. Policy iteration
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-evaluation">
     3.1.1. Policy evaluation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iterative-policy-evaluation">
     3.1.2. Iterative policy evaluation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-improvement">
     3.1.3. Policy improvement
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     3.1.4. Policy iteration
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gridworld-example">
     3.1.5. Gridworld example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#value-iteration">
   3.2. Value iteration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comparison-of-policy-and-value-iteration">
   3.3. Comparison of Policy- and Value-iteration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#asynchronous-dynamic-programming">
   3.4. Asynchronous dynamic programming
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#curse-of-dimensionality">
   3.5. Curse of dimensionality
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="dynamic-programming">
<h1><span class="section-number">3. </span>Dynamic Programming<a class="headerlink" href="#dynamic-programming" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/lectures/pdf/2.3-DP.pdf">pdf</a></p>
<div class="figure align-default" id="id4">
<a class="reference internal image-reference" href="../_images/gpi-scheme.png"><img alt="../_images/gpi-scheme.png" src="../_images/gpi-scheme.png" style="width: 30%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.1 </span><span class="caption-text">Generalized Policy Iteration. Source: <a class="bibtex reference internal" href="../zreferences.html#sutton1998" id="id1">[SB98]</a></span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>Dynamic Programming (DP) belongs to Generalized Policy Iteration (GPI) family as it iterates over two steps:</p>
<ol>
<li><p><strong>Policy evaluation</strong></p>
<ul class="simple">
<li><p>For a given policy <span class="math notranslate nohighlight">\(\pi\)</span>, the value of all states <span class="math notranslate nohighlight">\(V^\pi(s)\)</span> or all state-action pairs <span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span> is calculated based on the Bellman equations:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
      V^{\pi} (s)  = \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{\pi} (s') ]
    \]</div>
</li>
<li><p><strong>Policy improvement</strong></p>
<ul class="simple">
<li><p>From the current estimated values <span class="math notranslate nohighlight">\(V^\pi(s)\)</span> or <span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span>, a new <strong>better</strong> policy <span class="math notranslate nohighlight">\(\pi\)</span> is derived.</p></li>
</ul>
</li>
</ol>
<p>After enough iterations, the policy converges to the <strong>optimal policy</strong> (if the states are Markov).</p>
<div class="section" id="policy-iteration">
<h2><span class="section-number">3.1. </span>Policy iteration<a class="headerlink" href="#policy-iteration" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/9LS_pD728yo' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="policy-evaluation">
<h3><span class="section-number">3.1.1. </span>Policy evaluation<a class="headerlink" href="#policy-evaluation" title="Permalink to this headline">¶</a></h3>
<p>Remember the Bellman equation for the state <span class="math notranslate nohighlight">\(s\)</span> and a fixed policy <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      V^{\pi} (s)  = \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{\pi} (s') ]
\]</div>
<p>Let’s note <span class="math notranslate nohighlight">\(\mathcal{P}_{ss'}^\pi\)</span> the transition probability between <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(s'\)</span> (dependent on the policy <span class="math notranslate nohighlight">\(\pi\)</span>) and <span class="math notranslate nohighlight">\(\mathcal{R}_{s}^\pi\)</span> the expected reward in <span class="math notranslate nohighlight">\(s\)</span> (also dependent):</p>
<div class="math notranslate nohighlight">
\[
  \mathcal{P}_{ss'}^\pi = \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} \, p(s' | s, a)
\]</div>
<div class="math notranslate nohighlight">
\[
  \mathcal{R}_{s}^\pi = \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} \, p(s' | s, a) \ r(s, a, s')
\]</div>
<p>The Bellman equation becomes:</p>
<div class="math notranslate nohighlight">
\[
      V^{\pi} (s)  = \mathcal{R}_{s}^\pi + \gamma \, \sum_{s' \in \mathcal{S}} \, \mathcal{P}_{ss'}^\pi \, V^{\pi} (s')
\]</div>
<p>As we have a fixed policy during the evaluation (Markov Reward Process), the Bellman equation is simplified.</p>
<p>Let’s now put the Bellman equations in a matrix-vector form.</p>
<div class="math notranslate nohighlight">
\[
      V^{\pi} (s)  = \mathcal{R}_{s}^\pi + \gamma \, \sum_{s' \in \mathcal{S}} \, \mathcal{P}_{ss'}^\pi \, V^{\pi} (s')
\]</div>
<p>We first define the <strong>vector of state values</strong> <span class="math notranslate nohighlight">\(\mathbf{V}^\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \mathbf{V}^\pi = \begin{bmatrix}
      V^\pi(s_1) \\ V^\pi(s_2) \\ \vdots \\ V^\pi(s_n) \\
  \end{bmatrix}
\end{split}\]</div>
<p>and the <strong>vector of expected reward</strong> <span class="math notranslate nohighlight">\(\mathcal{R}^\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \mathcal{R}^\pi = \begin{bmatrix}
      \mathcal{R}^\pi(s_1) \\ \mathcal{R}^\pi(s_2) \\ \vdots \\ \mathcal{R}^\pi(s_n) \\
  \end{bmatrix}
\end{split}\]</div>
<p>The <strong>state transition matrix</strong> <span class="math notranslate nohighlight">\(\mathcal{P}^\pi\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \mathcal{P}^\pi = \begin{bmatrix}
      \mathcal{P}_{s_1 s_1}^\pi &amp; \mathcal{P}_{s_1 s_2}^\pi &amp; \ldots &amp; \mathcal{P}_{s_1 s_n}^\pi \\
      \mathcal{P}_{s_2 s_1}^\pi &amp; \mathcal{P}_{s_2 s_2}^\pi &amp; \ldots &amp; \mathcal{P}_{s_2 s_n}^\pi \\
      \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
      \mathcal{P}_{s_n s_1}^\pi &amp; \mathcal{P}_{s_n s_2}^\pi &amp; \ldots &amp; \mathcal{P}_{s_n s_n}^\pi \\
  \end{bmatrix}
\end{split}\]</div>
<p>You can simply check that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{bmatrix}
      V^\pi(s_1) \\ V^\pi(s_2) \\ \vdots \\ V^\pi(s_n) \\
  \end{bmatrix} = 
  \begin{bmatrix}
      \mathcal{R}^\pi(s_1) \\ \mathcal{R}^\pi(s_2) \\ \vdots \\ \mathcal{R}^\pi(s_n) \\
  \end{bmatrix}
  + \gamma \, \begin{bmatrix}
      \mathcal{P}_{s_1 s_1}^\pi &amp; \mathcal{P}_{s_1 s_2}^\pi &amp; \ldots &amp; \mathcal{P}_{s_1 s_n}^\pi \\
      \mathcal{P}_{s_2 s_1}^\pi &amp; \mathcal{P}_{s_2 s_2}^\pi &amp; \ldots &amp; \mathcal{P}_{s_2 s_n}^\pi \\
      \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
      \mathcal{P}_{s_n s_1}^\pi &amp; \mathcal{P}_{s_n s_2}^\pi &amp; \ldots &amp; \mathcal{P}_{s_n s_n}^\pi \\
  \end{bmatrix} \times \begin{bmatrix}
      V^\pi(s_1) \\ V^\pi(s_2) \\ \vdots \\ V^\pi(s_n) \\
  \end{bmatrix}
\end{split}\]</div>
<p>leads to the same equations as:</p>
<div class="math notranslate nohighlight">
\[
      V^{\pi} (s)  = \mathcal{R}_{s}^\pi + \gamma \, \sum_{s' \in \mathcal{S}} \, \mathcal{P}_{ss'}^\pi \, V^{\pi} (s')
\]</div>
<p>for all states <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>The Bellman equations for all states <span class="math notranslate nohighlight">\(s\)</span> can therefore be written with a matrix-vector notation as:</p>
<div class="math notranslate nohighlight">
\[
  \mathbf{V}^\pi = \mathcal{R}^\pi + \gamma \, \mathcal{P}^\pi \, \mathbf{V}^\pi 
\]</div>
<p>or:</p>
<div class="math notranslate nohighlight">
\[
  (\mathbb{I} - \gamma \, \mathcal{P}^\pi ) \times \mathbf{V}^\pi = \mathcal{R}^\pi
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{I}\)</span> is the identity matrix.  If we know <span class="math notranslate nohighlight">\(\mathcal{P}^\pi\)</span> and <span class="math notranslate nohighlight">\(\mathcal{R}^\pi\)</span> (dynamics of the MDP for the policy <span class="math notranslate nohighlight">\(\pi\)</span>), we can simply obtain the state values with a matrix inversion:</p>
<div class="math notranslate nohighlight">
\[
  \mathbf{V}^\pi = (\mathbb{I} - \gamma \, \mathcal{P}^\pi )^{-1} \times \mathcal{R}^\pi
\]</div>
<p>Done! <strong>But</strong>, if we have <span class="math notranslate nohighlight">\(n\)</span> states, the matrix <span class="math notranslate nohighlight">\(\mathcal{P}^\pi\)</span> has <span class="math notranslate nohighlight">\(n^2\)</span> elements. Inverting <span class="math notranslate nohighlight">\(\mathbb{I} - \gamma \, \mathcal{P}^\pi\)</span> requires at least <span class="math notranslate nohighlight">\(\mathcal{O}(n^{2.37})\)</span> operations. Forget it if you have more than a thousand states (<span class="math notranslate nohighlight">\(1000^{2.37} \approx 13\)</span> million operations). In dynamic programming, we will therefore use <strong>iterative methods</strong> to estimate <span class="math notranslate nohighlight">\(\mathbf{V}^\pi\)</span> with (hopefully) less operations.</p>
</div>
<div class="section" id="iterative-policy-evaluation">
<h3><span class="section-number">3.1.2. </span>Iterative policy evaluation<a class="headerlink" href="#iterative-policy-evaluation" title="Permalink to this headline">¶</a></h3>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/DOGbCQjSIlY' frameborder='0' allowfullscreen></iframe></div>
<p>The idea of <strong>iterative policy evaluation</strong> (IPE) is to consider a sequence of consecutive state-value functions which should converge from initially wrong estimates <span class="math notranslate nohighlight">\(V_0(s)\)</span> towards the real state-value function <span class="math notranslate nohighlight">\(V^{\pi}(s)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
      V_0 \rightarrow V_1 \rightarrow V_2 \rightarrow \ldots \rightarrow V_k \rightarrow V_{k+1} \rightarrow \ldots \rightarrow V^\pi
\]</div>
<div class="figure align-default" id="id5">
<a class="reference internal image-reference" href="../_images/iterativepolicyevaluation2.png"><img alt="../_images/iterativepolicyevaluation2.png" src="../_images/iterativepolicyevaluation2.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2 </span><span class="caption-text">Backup diagram of iterative policy evaluation. Credit: David Silver.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>The value function at step <span class="math notranslate nohighlight">\(k+1\)</span> <span class="math notranslate nohighlight">\(V_{k+1}(s)\)</span> is computed using the previous estimates <span class="math notranslate nohighlight">\(V_{k}(s)\)</span> and the Bellman equation transformed into an <strong>update rule</strong>. In vector notation:</p>
<div class="math notranslate nohighlight">
\[
  \mathbf{V}_{k+1} = \mathcal{R}^\pi + \gamma \, \mathcal{P}^\pi \, \mathbf{V}_k
\]</div>
<p>Let’s start with dummy (e.g. random) initial estimates <span class="math notranslate nohighlight">\(V_0(s)\)</span> for the value of every state <span class="math notranslate nohighlight">\(s\)</span>. We can obtain new estimates <span class="math notranslate nohighlight">\(V_1(s)\)</span> which are slightly less wrong by applying once the <strong>Bellman operator</strong>:</p>
<div class="math notranslate nohighlight">
\[
     V_{1} (s) \leftarrow \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V_0 (s') ] \quad \forall s \in \mathcal{S}
\]</div>
<p>Based on these estimates <span class="math notranslate nohighlight">\(V_1(s)\)</span>, we can obtain even better estimates <span class="math notranslate nohighlight">\(V_2(s)\)</span> by applying again the Bellman operator:</p>
<div class="math notranslate nohighlight">
\[
     V_{2} (s) \leftarrow \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V_1 (s') ] \quad \forall s \in \mathcal{S}
\]</div>
<p>Generally, state-value function estimates are improved iteratively through:</p>
<div class="math notranslate nohighlight">
\[
     V_{k+1} (s) \leftarrow \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V_k (s') ] \quad \forall s \in \mathcal{S}
\]</div>
<p><span class="math notranslate nohighlight">\(V_\infty = V^{\pi}\)</span> is a fixed point of this update rule because of the uniqueness of the solution to the Bellman equation.</p>
<p>The <strong>Bellman operator</strong> <span class="math notranslate nohighlight">\(\mathcal{T}^\pi\)</span> is a mapping between two vector spaces:</p>
<div class="math notranslate nohighlight">
\[
  \mathcal{T}^\pi (\mathbf{V}) = \mathcal{R}^\pi + \gamma \, \mathcal{P}^\pi \, \mathbf{V}
\]</div>
<p>If you apply repeatedly the Bellman operator on any initial vector <span class="math notranslate nohighlight">\(\mathbf{V}_0\)</span>, it converges towards the solution of the Bellman equations <span class="math notranslate nohighlight">\(\mathbf{V}^\pi\)</span>. Mathematically speaking, <span class="math notranslate nohighlight">\(\mathcal{T}^\pi\)</span> is a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction, i.e. it makes value functions closer by at least <span class="math notranslate nohighlight">\(\gamma\)</span>:</p>
<div class="math notranslate nohighlight">
\[
  || \mathcal{T}^\pi (\mathbf{V}) - \mathcal{T}^\pi (\mathbf{U})||_\infty \leq \gamma \, ||\mathbf{V} - \mathbf{U} ||_\infty
\]</div>
<p>The <strong>contraction mapping theorem</strong> ensures that <span class="math notranslate nohighlight">\(\mathcal{T}^\pi\)</span> converges to an unique fixed point: this proves the existence and uniqueness of the solution of the Bellman equations.</p>
<p>Iterative Policy Evaluation relies on <strong>full backups</strong>: it backs up the value of ALL possible successive states into the new value of a state.</p>
<div class="math notranslate nohighlight">
\[
     V_{k+1} (s) \leftarrow \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V_k (s') ] \quad \forall s \in \mathcal{S}
\]</div>
<p>The backups are <strong>synchronous</strong>: all states are backed up in parallel.</p>
<div class="math notranslate nohighlight">
\[
  \mathbf{V}_{k+1} = \mathcal{R}^\pi + \gamma \, \mathcal{P}^\pi \, \mathbf{V}_k
\]</div>
<p>The termination of iterative policy evaluation has to be controlled by hand, as the convergence of the algorithm is only at the limit. It is good practice to look at the variations on the values of the different states, and stop the iteration when this variation falls below a predefined threshold.</p>
<div class="admonition-iterative-policy-evaluation admonition">
<p class="admonition-title">Iterative Policy Evaluation</p>
<ul class="simple">
<li><p>For a fixed policy <span class="math notranslate nohighlight">\(\pi\)</span>, initialize <span class="math notranslate nohighlight">\(V(s)=0 \; \forall s \in \mathcal{S}\)</span>.</p></li>
<li><p><strong>while</strong> not converged:</p>
<ul>
<li><p><strong>for</strong> all states <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(V_\text{target}(s) = \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V (s') ]\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\delta =0\)</span></p></li>
<li><p><strong>for</strong> all states <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\delta = \max(\delta, |V(s) - V_\text{target}(s)|)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V(s) = V_\text{target}(s)\)</span></p></li>
</ul>
</li>
<li><p><strong>if</strong> <span class="math notranslate nohighlight">\(\delta &lt; \delta_\text{threshold}\)</span>:</p>
<ul>
<li><p>converged = True</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="policy-improvement">
<h3><span class="section-number">3.1.3. </span>Policy improvement<a class="headerlink" href="#policy-improvement" title="Permalink to this headline">¶</a></h3>
<p>The goal of finding the value function for a given policy <span class="math notranslate nohighlight">\(\pi\)</span> is to help improving this policy. For each state <span class="math notranslate nohighlight">\(s\)</span>, we would like to know if we should deterministically choose an action <span class="math notranslate nohighlight">\(a \neq \pi(s)\)</span> or not in order to improve the policy.</p>
<p>The value of an action <span class="math notranslate nohighlight">\(a\)</span> in the state <span class="math notranslate nohighlight">\(s\)</span> for the policy <span class="math notranslate nohighlight">\(\pi\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
     Q^{\pi} (s, a) = \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [r(s, a, s') + \gamma \, V^{\pi}(s') ]
\]</div>
<p>f the Q-value of an action <span class="math notranslate nohighlight">\(a\)</span> is higher than the one currently selected by the <strong>deterministic</strong> policy:</p>
<div class="math notranslate nohighlight">
\[Q^{\pi} (s, a) &gt; Q^{\pi} (s, \pi(s)) = V^{\pi}(s)\]</div>
<p>then it is better to select <span class="math notranslate nohighlight">\(a\)</span> once in <span class="math notranslate nohighlight">\(s\)</span> and thereafter follow <span class="math notranslate nohighlight">\(\pi\)</span>. If there is no better action, we keep the previous policy for this state. This corresponds to a <strong>greedy</strong> action selection over the Q-values, defining a <strong>deterministic</strong> policy <span class="math notranslate nohighlight">\(\pi(s)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\pi(s) \leftarrow \text{argmax}_a Q^{\pi} (s, a) = \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [r(s, a, s') + \gamma \, V^{\pi}(s') ]\]</div>
<p>After the policy improvement, the Q-value of each deterministic action <span class="math notranslate nohighlight">\(\pi(s)\)</span> has increased or stayed the same.</p>
<div class="math notranslate nohighlight">
\[\text{argmax}_a Q^{\pi} (s, a) = \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [r(s, a, s') + \gamma \, V^{\pi}(s') ] \geq Q^\pi(s, \pi(s))\]</div>
<p>This defines an <strong>improved</strong> policy <span class="math notranslate nohighlight">\(\pi'\)</span>, where all states and actions have a higher value than previously. <strong>Greedy action selection</strong> over the state value function implements policy improvement:</p>
<div class="math notranslate nohighlight">
\[\pi' \leftarrow \text{Greedy}(V^\pi)\]</div>
<div class="admonition-greedy-policy-improvement admonition">
<p class="admonition-title">Greedy policy improvement</p>
<ul class="simple">
<li><p><strong>for</strong> each state <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\pi(s) \leftarrow \text{argmax}_a \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [r(s, a, s') + \gamma \, V^{\pi}(s') ]\)</span></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="id2">
<h3><span class="section-number">3.1.4. </span>Policy iteration<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Once a policy <span class="math notranslate nohighlight">\(\pi\)</span> has been improved using <span class="math notranslate nohighlight">\(V^{\pi}\)</span> to yield a better policy <span class="math notranslate nohighlight">\(\pi'\)</span>, we can then compute <span class="math notranslate nohighlight">\(V^{\pi'}\)</span> and improve it again to yield an even better policy <span class="math notranslate nohighlight">\(\pi''\)</span>.</p>
<p>The algorithm <strong>policy iteration</strong> successively uses <strong>policy evaluation</strong> and <strong>policy improvement</strong> to find the optimal policy.</p>
<div class="math notranslate nohighlight">
\[
  \pi_0 \xrightarrow[]{E} V^{\pi_0} \xrightarrow[]{I} \pi_1 \xrightarrow[]{E} V^{\pi^1} \xrightarrow[]{I}  ... \xrightarrow[]{I} \pi^* \xrightarrow[]{E} V^{*}
\]</div>
<div class="figure align-default" id="id6">
<a class="reference internal image-reference" href="../_images/gpi.png"><img alt="../_images/gpi.png" src="../_images/gpi.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.3 </span><span class="caption-text">Policy Iteration. Source: <a class="bibtex reference internal" href="../zreferences.html#sutton1998" id="id3">[SB98]</a></span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>The <strong>optimal policy</strong> being deterministic, policy improvement can be greedy over the state values. If the policy does not change after policy improvement, the optimal policy has been found.</p>
<div class="admonition-policy-iteration admonition">
<p class="admonition-title">Policy iteration</p>
<ul class="simple">
<li><p>Initialize a deterministic policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> and set <span class="math notranslate nohighlight">\(V(s)=0 \; \forall s \in \mathcal{S}\)</span>.</p></li>
<li><p><strong>while</strong> <span class="math notranslate nohighlight">\(\pi\)</span> is not optimal:</p>
<ul>
<li><p><strong>while</strong> not converged: # Policy evaluation</p>
<ul>
<li><p><strong>for</strong> all states <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(V_\text{target}(s) = \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V (s') ]\)</span></p></li>
</ul>
</li>
<li><p><strong>for</strong> all states <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(V(s) = V_\text{target}(s)\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>for</strong> each state <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>: # Policy improvement</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\pi(s) \leftarrow \text{argmax}_a \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [r(s, a, s') + \gamma \, V^{\pi}(s') ]\)</span></p></li>
</ul>
</li>
<li><p><strong>if</strong> <span class="math notranslate nohighlight">\(\pi\)</span> has not changed: <strong>break</strong></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="gridworld-example">
<h3><span class="section-number">3.1.5. </span>Gridworld example<a class="headerlink" href="#gridworld-example" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id7">
<a class="reference internal image-reference" href="../_images/dp-gridworld1.png"><img alt="../_images/dp-gridworld1.png" src="../_images/dp-gridworld1.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.4 </span><span class="caption-text">Gridworld example. Credit: David Silver</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Gridworld</strong> is an undiscounted MDP (we can take <span class="math notranslate nohighlight">\(\gamma=1\)</span>). The states are the position in the grid, the actions are up, down, left, right. Transitions to a wall leave in the same state. The reward is always -1, except after being in the terminal states in gray (<span class="math notranslate nohighlight">\(r=0\)</span>). The initial policy is random:</p>
<div class="math notranslate nohighlight">
\[\pi(s, \text{up}) = \pi(s, \text{down})= \pi(s, \text{left}) = \pi(s, \text{right}) = 0.25\]</div>
<div class="figure align-default" id="id8">
<a class="reference internal image-reference" href="../_images/dp-gridworld2.png"><img alt="../_images/dp-gridworld2.png" src="../_images/dp-gridworld2.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.5 </span><span class="caption-text">First iterations. Credit: David Silver</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k=0\)</span>: The initial values <span class="math notranslate nohighlight">\(V_0\)</span> are set to 0 as the initial policy is random.</p></li>
<li><p><span class="math notranslate nohighlight">\(k=1\)</span>:  The random policy is evaluated: all states get the value of the average immediate reward in that state. -1, except the terminal states (0).  The greedy policy is already an improvement over the random policy: adjacent states to the terminal states would decide to go there systematically, as the value is 0 instead of -1.</p></li>
<li><p><span class="math notranslate nohighlight">\(k=2\)</span>: The previous estimates propagate: states adjacent to the terminal states get a higher value, as there will be less punishments after these states.</p></li>
</ul>
<div class="figure align-default" id="id9">
<a class="reference internal image-reference" href="../_images/dp-gridworld3.png"><img alt="../_images/dp-gridworld3.png" src="../_images/dp-gridworld3.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.6 </span><span class="caption-text">Later iterations. Credit: David Silver</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k=3\)</span>:  The values continue to propagate.  The greedy policy at that step of policy evaluation is already optimal.</p></li>
<li><p><span class="math notranslate nohighlight">\(k&gt;3\)</span>: The values continue to converge towards the true values. The greedy policy does not change. In this simple example, it is already the optimal policy.</p></li>
</ul>
<p>Two things to notice:</p>
<ul class="simple">
<li><p>There is no actually no need to wait until the end of policy evaluation to improve the policy, as the greedy policy might already be optimal.</p></li>
<li><p>There can be more than one optimal policy: some actions may have the same Q-value: choosing one or other is equally optimal.</p></li>
</ul>
</div>
</div>
<div class="section" id="value-iteration">
<h2><span class="section-number">3.2. </span>Value iteration<a class="headerlink" href="#value-iteration" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/lB_ouoRVWiE' frameborder='0' allowfullscreen></iframe></div>
<p><strong>Policy iteration</strong> can converge in a surprisingly small number of iterations. One drawback of <em>policy iteration</em> is that it uses a full policy evaluation, which can be computationally exhaustive as the convergence of <span class="math notranslate nohighlight">\(V_k\)</span> is only at the limit and the number of states can be huge.</p>
<p>The idea of <strong>value iteration</strong> is to interleave policy evaluation and policy improvement, so that the policy is improved after each iteration of policy evaluation, not after complete convergence. As policy improvement returns a deterministic greedy policy, updating of the value of a state is then simpler:</p>
<div class="math notranslate nohighlight">
\[
        V_{k+1}(s) = \max_a \sum_{s'} p(s' | s,a) [r(s, a, s') + \gamma \, V_k(s') ]
\]</div>
<p>Note that this is equivalent to turning the <strong>Bellman optimality equation</strong> into an update rule. Value iteration converges to <span class="math notranslate nohighlight">\(V^*\)</span>, faster than policy iteration, and should be stopped when the values do not change much anymore.</p>
<div class="admonition-value-iteration admonition">
<p class="admonition-title">Value iteration</p>
<ul class="simple">
<li><p>Initialize a deterministic policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> and set <span class="math notranslate nohighlight">\(V(s)=0 \; \forall s \in \mathcal{S}\)</span>.</p></li>
<li><p><strong>while</strong> not converged:</p>
<ul>
<li><p><strong>for</strong> all states <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(V_\text{target}(s) = \max_a \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V (s') ]\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\delta = 0\)</span></p></li>
<li><p><strong>for</strong> all states <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\delta = \max(\delta, |V(s) - V_\text{target}(s)|)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V(s) = V_\text{target}(s)\)</span></p></li>
</ul>
</li>
<li><p><strong>if</strong> <span class="math notranslate nohighlight">\(\delta &lt; \delta_\text{threshold}\)</span>:</p>
<ul>
<li><p>converged = True</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="comparison-of-policy-and-value-iteration">
<h2><span class="section-number">3.3. </span>Comparison of Policy- and Value-iteration<a class="headerlink" href="#comparison-of-policy-and-value-iteration" title="Permalink to this headline">¶</a></h2>
<p><strong>Full policy-evaluation backup</strong></p>
<div class="math notranslate nohighlight">
\[
    V_{k+1} (s) \leftarrow \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V_k (s') ]
\]</div>
<div class="figure align-default" id="id10">
<a class="reference internal image-reference" href="../_images/fullpe.png"><img alt="../_images/fullpe.png" src="../_images/fullpe.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.7 </span><span class="caption-text">Backup diagram of policy evaluation.</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Full value-iteration backup</strong></p>
<div class="math notranslate nohighlight">
\[
    V_{k+1} (s) \leftarrow \max_{a \in \mathcal{A}(s)} \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V_k (s') ]
\]</div>
<div class="figure align-default" id="id11">
<a class="reference internal image-reference" href="../_images/fullvi.png"><img alt="../_images/fullvi.png" src="../_images/fullvi.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.8 </span><span class="caption-text">Backup diagram of value evaluation.</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="asynchronous-dynamic-programming">
<h2><span class="section-number">3.4. </span>Asynchronous dynamic programming<a class="headerlink" href="#asynchronous-dynamic-programming" title="Permalink to this headline">¶</a></h2>
<p>Synchronous DP requires exhaustive sweeps of the entire state set (<strong>synchronous backups</strong>).</p>
<ul class="simple">
<li><p><strong>while</strong> not converged:</p>
<ul>
<li><p><strong>for</strong> all states <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(V_\text{target}(s) =  \max_a \,  \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V (s') ]\)</span></p></li>
</ul>
</li>
<li><p><strong>for</strong> all states <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(V(s) = V_\text{target}(s)\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Asynchronous DP updates instead each state independently and asynchronously (<strong>in-place</strong>):</p>
<ul>
<li><p><strong>while</strong> not converged:</p>
<ul class="simple">
<li><p>Pick a state <span class="math notranslate nohighlight">\(s\)</span> randomly (or following a heuristics).</p></li>
<li><p>Update the value of this state.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
      V(s) =  \max_a \,  \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V (s') ]
    \]</div>
</li>
</ul>
<p>We must still ensure that all states are visited, but their frequency and order is irrelevant. Is it possible to select the states to backup intelligently?</p>
<p><strong>Prioritized sweeping</strong> selects in priority the states with the largest remaining <strong>Bellman error</strong>:</p>
<div class="math notranslate nohighlight">
\[\delta = |\max_a \,  \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V (s') ] - V(s) |\]</div>
<p>A large Bellman error means that the current estimate <span class="math notranslate nohighlight">\(V(s)\)</span> is very different from the <strong>target</strong> <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="math notranslate nohighlight">
\[y = \max_a \,  \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V (s') ]\]</div>
<p>States with a high error should be updated in priority. If the Bellman error is small, this means that the current estimate <span class="math notranslate nohighlight">\(V(s)\)</span> is already close to what it should be: there is no hurry in evaluating this state. The main advantage is that the DP algorithm can be applied as the agent is actually experiencing its environment (no need for the dynamics of environment to be fully known).</p>
</div>
<div class="section" id="curse-of-dimensionality">
<h2><span class="section-number">3.5. </span>Curse of dimensionality<a class="headerlink" href="#curse-of-dimensionality" title="Permalink to this headline">¶</a></h2>
<p>Policy-iteration and value-iteration consist of alternations between policy evaluation and policy improvement, although at different frequencies. This principle is called <strong>Generalized Policy Iteration</strong> (GPI). Finding an optimal policy is polynomial in the number of states and actions: <span class="math notranslate nohighlight">\(\mathcal{O}(n^2 \, m)\)</span> (<span class="math notranslate nohighlight">\(n\)</span> is the number of states, <span class="math notranslate nohighlight">\(m\)</span> the number of actions).  However, the number of states is often astronomical, e.g., often growing exponentially with the number of state variables (what Bellman called <strong>“the curse of dimensionality”</strong>) In practice, classical DP can only be applied to problems with a few millions of states.</p>
<div class="figure align-default" id="id12">
<a class="reference internal image-reference" href="../_images/cursedimensionality.png"><img alt="../_images/cursedimensionality.png" src="../_images/cursedimensionality.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.9 </span><span class="caption-text">Curse of dimensionality. Source: <a class="reference external" href="https://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2">https://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2</a></span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p>If one variable can be represented by 5 discrete values:</p>
<ul class="simple">
<li><p>2 variables necessitate 25 states,</p></li>
<li><p>3 variables need 125 states, and so on…</p></li>
</ul>
<p>The number of states explodes exponentially with the number of dimensions of the problem…</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2-tabular"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="2-MDP.html" title="previous page"><span class="section-number">2. </span>Markov Decision Processes</a>
    <a class='right-next' id="next-link" href="4-MC.html" title="next page"><span class="section-number">4. </span>Monte-Carlo (MC) methods</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>