

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Markov Decision Processes &#8212; Deep Reinforcement Learning</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-deeprl/2-tabular/2-MDP.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1. Introduction to Python" href="../5-exercises/ex1-Python.html" />
    <link rel="prev" title="1. Bandits" href="1-Bandits.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-deeprl/2-tabular/2-MDP.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Markov Decision Processes" />
<meta property="og:description" content="Markov Decision Processes  Slides: pdf  From Finite State Machines to Markov Decision Process  &lt;div class=&#39;embed-container&#39;&gt;&lt;iframe src=&#39;https://www.youtube.com" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-deeprl/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Deep Reinforcement Learning</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Deep Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tabular RL
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1-Bandits.html">
   1. Bandits
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Markov Decision Processes
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex3-Sampling.html">
   3. Sampling
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2-tabular/2-MDP.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-finite-state-machines-to-markov-decision-process">
   2.1. From Finite State Machines to Markov Decision Process
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finite-state-machine-fsm">
     2.1.1. Finite State Machine (FSM)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-chain-mc">
     2.1.2. Markov Chain (MC)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#markov-property">
       2.1.2.1. Markov property
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#state-transition-matrix">
       2.1.2.2. State transition matrix
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-reward-process-mrp">
     2.1.3. Markov Reward Process (MRP)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-decision-process-mdp">
     2.1.4. Markov Decision Process (MDP)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       2.1.4.1. Markov property
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#returns">
       2.1.4.2. Returns
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-the-cartpole-balancing-task">
       2.1.4.3. Example: the cartpole balancing task
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-the-recycling-robot">
       2.1.4.4. Example: the recycling robot
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-policy">
       2.1.4.5. The policy
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#goal-of-reinforcement-learning">
       2.1.4.6. Goal of Reinforcement Learning
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bellman-equations">
   2.2. Bellman equations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#value-functions">
     2.2.1. Value Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     2.2.2. Bellman equations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bellman-optimality-equations">
   2.3. Bellman optimality equations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimal-policy">
     2.3.1. Optimal policy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     2.3.2. Bellman optimality equations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-policy-iteration">
   2.4. Generalized Policy Iteration
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="markov-decision-processes">
<h1><span class="section-number">2. </span>Markov Decision Processes<a class="headerlink" href="#markov-decision-processes" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/lectures/pdf/2.2-MDP.pdf">pdf</a></p>
<div class="section" id="from-finite-state-machines-to-markov-decision-process">
<h2><span class="section-number">2.1. </span>From Finite State Machines to Markov Decision Process<a class="headerlink" href="#from-finite-state-machines-to-markov-decision-process" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/u8_DgEuqo3g' frameborder='0' allowfullscreen></iframe></div>
<p>The kind of task that can be solved by RL is called a <strong>Markov Decision Process</strong> (MDP). For a MDP, the environment is <strong>fully observable</strong>, i.e. the current state <span class="math notranslate nohighlight">\(s_t\)</span> completely characterizes the process at time <span class="math notranslate nohighlight">\(t\)</span>. <strong>Actions</strong> <span class="math notranslate nohighlight">\(a_t\)</span> provoke transitions between two states <span class="math notranslate nohighlight">\(s_t\)</span> and <span class="math notranslate nohighlight">\(s_{t+1}\)</span>, according to <strong>transition probabilities</strong>. A <strong>reward</strong>  <span class="math notranslate nohighlight">\(r_{t+1}\)</span> is (probabilistically) associated to each transition.</p>
<div class="figure align-default" id="id8">
<a class="reference internal image-reference" href="../_images/rl-loop1.png"><img alt="../_images/rl-loop1.png" src="../_images/rl-loop1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.23 </span><span class="caption-text">Agent-environment interface for video games. Source: David Silver <a class="reference external" href="https://www.davidsilver.uk/teaching/">https://www.davidsilver.uk/teaching/</a>.
``</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
<div class="legend">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>n-armed bandits are MDPs with only one state.</p>
</div>
</div>
</div>
<div class="section" id="finite-state-machine-fsm">
<h3><span class="section-number">2.1.1. </span>Finite State Machine (FSM)<a class="headerlink" href="#finite-state-machine-fsm" title="Permalink to this headline">¶</a></h3>
<p>A <strong>finite state machine</strong> (or finite state automaton) is a mathematical model of computation. A FSM can only be in a single <strong>state</strong> <span class="math notranslate nohighlight">\(s\)</span> at any given time. <strong>Transitions</strong> between states are governed by external inputs, when some condition is met.</p>
<div class="figure align-default" id="id9">
<a class="reference internal image-reference" href="../_images/fsm.png"><img alt="../_images/fsm.png" src="../_images/fsm.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.24 </span><span class="caption-text">Finite state machine. Source:  <a class="reference external" href="https://web.stanford.edu/class/archive/cs/cs103/cs103.1142/button-fsm/">https://web.stanford.edu/class/archive/cs/cs103/cs103.1142/button-fsm/</a></span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<p>A FSM is fully defined by:</p>
<ul class="simple">
<li><p>The <strong>state set</strong> <span class="math notranslate nohighlight">\(\mathcal{S} = \{ s_i\}_{i=1}^N\)</span>.</p></li>
<li><p>Its initial state <span class="math notranslate nohighlight">\(S_0\)</span>.</p></li>
<li><p>A list of <strong>conditions</strong> for each transition.</p></li>
</ul>
<p>A FSM is usually implemented by a series of if/then/else statements:</p>
<ul class="simple">
<li><p>if state == “hover” and press == true:</p>
<ul>
<li><p>state = “pressed”</p></li>
</ul>
</li>
<li><p>elif …</p></li>
</ul>
</div>
<div class="section" id="markov-chain-mc">
<h3><span class="section-number">2.1.2. </span>Markov Chain (MC)<a class="headerlink" href="#markov-chain-mc" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id10">
<a class="reference internal image-reference" href="../_images/student-markovchain.png"><img alt="../_images/student-markovchain.png" src="../_images/student-markovchain.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.25 </span><span class="caption-text">Markov chain. Credit: David Silver <a class="reference external" href="https://www.davidsilver.uk/teaching/">https://www.davidsilver.uk/teaching/</a></span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>A first-order <strong>Markov Chain</strong> (or Markov process) is a stochastic process generated by a FSM, where transitions between states are governed by <strong>state transition probabilities</strong>.</p>
<p>A Markov Chain is defined by:</p>
<ul class="simple">
<li><p>The <strong>state set</strong> <span class="math notranslate nohighlight">\(\mathcal{S} = \{ s_i\}_{i=1}^N\)</span>.</p></li>
<li><p>The <strong>state transition probability function</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \mathcal{P}: \mathcal{S} \rightarrow &amp; P(\mathcal{S}) \\
    p(s' | s) &amp; =  P (s_{t+1} = s' | s_t = s) \\
\end{aligned}
\end{split}\]</div>
<div class="section" id="markov-property">
<h4><span class="section-number">2.1.2.1. </span>Markov property<a class="headerlink" href="#markov-property" title="Permalink to this headline">¶</a></h4>
<p>When the states have the <strong>Markov property</strong>, the state transition probabilities fully describe the MC. The Markov property states that:</p>
<blockquote>
<div><p>The future is independent of the past given the present.</p>
</div></blockquote>
<p>Formally, the state <span class="math notranslate nohighlight">\(s_t\)</span> (state at time <span class="math notranslate nohighlight">\(t\)</span>) is <strong>Markov</strong> (or Markovian) if and only if:</p>
<div class="math notranslate nohighlight">
\[
    P(s_{t+1} | s_t) = P(s_{t+1} | s_t, s_{t-1}, \ldots, s_0)
\]</div>
<p>The knowledge of the current state <span class="math notranslate nohighlight">\(s_t\)</span> is <strong>enough</strong> to predict in which state <span class="math notranslate nohighlight">\(s_{t+1}\)</span> the system will be at the next time step. We do not need the whole <strong>history</strong> <span class="math notranslate nohighlight">\(\{s_0, s_1, \ldots, s_t\}\)</span> of the system to predict what will happen.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we need only <span class="math notranslate nohighlight">\(s_{t-1}\)</span> and <span class="math notranslate nohighlight">\(s_t\)</span> to predict <span class="math notranslate nohighlight">\(s_{t+1}\)</span>, we have a second-order Markov chain.</p>
</div>
<p>For example, the probability 0.8 of transitioning from “Class 2” to “Class 3” is the same regardless we were in “Class 1” or “Pub” before. If this is not the case, the states are not Markov, and this is not a Markov chain. We would need to create two distinct states:</p>
<ul class="simple">
<li><p>“Class 2 coming from Class 1”</p></li>
<li><p>“Class 2 coming from the pub”</p></li>
</ul>
<p>Single <strong>video frames</strong> are not Markov states: you cannot generally predict what will happen based on a single image.  A simple solution is to <strong>stack</strong> or <strong>concatenate</strong> multiple frames: By measuring the displacement of the ball between two consecutive frames, we can predict where it is going. One can also <strong>learn</strong> state representations containing the history using recurrent neural networks (see later).</p>
</div>
<div class="section" id="state-transition-matrix">
<h4><span class="section-number">2.1.2.2. </span>State transition matrix<a class="headerlink" href="#state-transition-matrix" title="Permalink to this headline">¶</a></h4>
<p>Supposing that the states have the Markov property, the transitions in the system can be summarized by the <strong>state transition matrix</strong> <span class="math notranslate nohighlight">\(\mathcal{P}\)</span>:</p>
<div class="figure align-default" id="id11">
<a class="reference internal image-reference" href="../_images/student-transitionmatrix.png"><img alt="../_images/student-transitionmatrix.png" src="../_images/student-transitionmatrix.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.26 </span><span class="caption-text">State transition matrix. Credit: David Silver <a class="reference external" href="https://www.davidsilver.uk/teaching/">https://www.davidsilver.uk/teaching/</a></span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
<p>Each element of the state transition matrix corresponds to <span class="math notranslate nohighlight">\(p(s' | s)\)</span>. Each row of the state transition matrix sums to 1:</p>
<div class="math notranslate nohighlight">
\[\sum_{s'} p(s' | s)  = 1\]</div>
<p>The tuple <span class="math notranslate nohighlight">\(&lt;\mathcal{S}, \mathcal{P}&gt;\)</span> fully describes the Markov chain.</p>
</div>
</div>
<div class="section" id="markov-reward-process-mrp">
<h3><span class="section-number">2.1.3. </span>Markov Reward Process (MRP)<a class="headerlink" href="#markov-reward-process-mrp" title="Permalink to this headline">¶</a></h3>
<p>A <strong>Markov Reward Process</strong> is a Markov Chain where each transition is associated with a scalar <strong>reward</strong> <span class="math notranslate nohighlight">\(r\)</span>, coming from some probability distribution.</p>
<div class="figure align-default" id="id12">
<a class="reference internal image-reference" href="../_images/student-mrp.png"><img alt="../_images/student-mrp.png" src="../_images/student-mrp.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.27 </span><span class="caption-text">Markov Reward Process. Credit: David Silver <a class="reference external" href="https://www.davidsilver.uk/teaching/">https://www.davidsilver.uk/teaching/</a></span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p>A Markov Reward Process is defined by the tuple <span class="math notranslate nohighlight">\(&lt;\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma&gt;\)</span>.</p>
<ul class="simple">
<li><p>The finite <strong>state set</strong> <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p></li>
<li><p>The <strong>state transition probability function</strong>:
$<span class="math notranslate nohighlight">\(
\begin{aligned}
  \mathcal{P}: \mathcal{S} \rightarrow &amp; P(\mathcal{S}) \\
  p(s' | s) &amp; =  P (s_{t+1} = s' | s_t = s) \\
\end{aligned}
\)</span>$</p></li>
<li><p>The <strong>expected reward function</strong>:
$<span class="math notranslate nohighlight">\(
\begin{aligned}
  \mathcal{R}: \mathcal{S} \times \mathcal{S} \rightarrow &amp; \Re \\
  r(s, s') &amp;=  \mathbb{E} (r_{t+1} | s_t = s, s_{t+1} = s') \\
\end{aligned}
\)</span>$</p></li>
<li><p>The <strong>discount factor</strong> <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span>.</p></li>
</ul>
<p>As with n-armed bandits, we only care about the <strong>expected reward</strong> received during a transition <span class="math notranslate nohighlight">\(s \rightarrow s'\)</span> (<em>on average</em>), but the actual reward received <span class="math notranslate nohighlight">\(r_{t+1}\)</span> may vary around the expected value.</p>
<div class="math notranslate nohighlight">
\[r(s, s') =  \mathbb{E} (r_{t+1} | s_t = s, s_{t+1} = s')\]</div>
<p>The main difference with n-armed bandits is that the MRP will be in a sequence of states (possibly infinite):</p>
<div class="math notranslate nohighlight">
\[s_0 \rightarrow s_1 \rightarrow s_2  \rightarrow \ldots \rightarrow s_T\]</div>
<p>and collect a sequence of reward samples:</p>
<div class="math notranslate nohighlight">
\[r_1 \rightarrow r_2 \rightarrow r_3  \rightarrow \ldots \rightarrow r_{T}\]</div>
<p>In a MRP, we are interested in estimating the <strong>return</strong> <span class="math notranslate nohighlight">\(R_t\)</span>, i.e. the discounted sum of <strong>future</strong> rewards after the step <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    R_t = r_{t+1} + \gamma \, r_{t+2} + \gamma^2 \, r_{t+3} + \ldots = \sum_{k=0}^\infty \gamma^k \, r_{t+k+1}
\]</div>
<p>Of course, you never know the return at time <span class="math notranslate nohighlight">\(t\)</span>: transitions and rewards are probabilistic, so the received rewards in the future are not exactly predictable at <span class="math notranslate nohighlight">\(t\)</span>. <span class="math notranslate nohighlight">\(R_t\)</span> is therefore purely theoretical: RL is all about <strong>estimating</strong> the return.</p>
<p>The <strong>discount factor</strong> (or discount rate, or discount) <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span> is a very important parameter in RL: It defines the <strong>present value of future rewards</strong>. Receiving 10 euros now has a higher <strong>value</strong> than receiving 10 euros in ten years, although the reward is the same: you do not have to wait.</p>
<p>The value of receiving a reward <span class="math notranslate nohighlight">\(r\)</span> after <span class="math notranslate nohighlight">\(k+1\)</span> time steps is <span class="math notranslate nohighlight">\(\gamma^k \, r\)</span>. <strong>Immediate rewards</strong> are better than <strong>delayed rewards</strong>. When <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span>, <span class="math notranslate nohighlight">\(\gamma^k\)</span> tends to 0 when <span class="math notranslate nohighlight">\(k\)</span> goes to infinity: this makes sure that the return is always <strong>finite</strong>. This is particularly important when the MRP is cyclic / periodic. If all sequences terminate at some time step <span class="math notranslate nohighlight">\(T\)</span>, we can set <span class="math notranslate nohighlight">\(\gamma= 1\)</span>.</p>
</div>
<div class="section" id="markov-decision-process-mdp">
<h3><span class="section-number">2.1.4. </span>Markov Decision Process (MDP)<a class="headerlink" href="#markov-decision-process-mdp" title="Permalink to this headline">¶</a></h3>
<p>A <strong>Markov Decision Process</strong> is a MRP where transitions are influenced by <strong>actions</strong> <span class="math notranslate nohighlight">\(a \in \mathcal{A}\)</span>.</p>
<div class="figure align-default" id="id13">
<a class="reference internal image-reference" href="../_images/student-mdp.png"><img alt="../_images/student-mdp.png" src="../_images/student-mdp.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.28 </span><span class="caption-text">Markov decision process. Credit: David Silver <a class="reference external" href="https://www.davidsilver.uk/teaching/">https://www.davidsilver.uk/teaching/</a></span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<p>A finite MDP is defined by the tuple <span class="math notranslate nohighlight">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma&gt;\)</span>:</p>
<ul class="simple">
<li><p>The finite <strong>state set</strong> <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p></li>
<li><p>The finite <strong>action set</strong> <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>.</p></li>
<li><p>The <strong>state transition probability function</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow &amp; P(\mathcal{S}) \\
    p(s' | s, a) &amp; =  P (s_{t+1} = s' | s_t = s, a_t = a) \\
\end{aligned}
\end{split}\]</div>
<ul class="simple">
<li><p>The <strong>expected reward function</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow &amp; \Re \\
    r(s, a, s') &amp;=  \mathbb{E} (r_{t+1} | s_t = s, a_t = a, s_{t+1} = s') \\
\end{aligned}
\end{split}\]</div>
<ul class="simple">
<li><p>The <strong>discount factor</strong> <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span>.</p></li>
</ul>
<p>Why do we need transition probabilities in RL?</p>
<div class="math notranslate nohighlight">
\[ p(s' | s, a) =  P (s_{t+1} = s' | s_t = s, a_t = a)\]</div>
<p>Some RL tasks are <strong>deterministic</strong>: an action <span class="math notranslate nohighlight">\(a\)</span> in a state <span class="math notranslate nohighlight">\(s\)</span> always leads to the state <span class="math notranslate nohighlight">\(s'\)</span> (board games, video games…). Others are <strong>stochastic</strong>: the same action <span class="math notranslate nohighlight">\(a\)</span> can lead to different states <span class="math notranslate nohighlight">\(s'\)</span>: Casino games (throwing a dice, etc), two-opponent games (the next state depends on what the other player chooses), uncertainty (shoot at basketball, slippery wheels, robotic grasping)…</p>
<p>For a transition <span class="math notranslate nohighlight">\((s, a, s')\)</span>, the received reward can be also stochastic: casino games (armed bandit), incomplete information, etc. Most of the problems we will see in this course have deterministic rewards, but we only care about expectations anyway.</p>
<div class="section" id="id1">
<h4><span class="section-number">2.1.4.1. </span>Markov property<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>The state of the agent at step <span class="math notranslate nohighlight">\(t\)</span> refers to whatever information is available about its environment or its own “body”. The state can include immediate “sensations”, highly processed sensations, and structures built up over time from sequences of sensations. A state should summarize all past sensations so as to retain all essential information, i.e. it should have the <strong>Markov Property</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
     P( s_{t+1} = s, r_{t+1} = r &amp; | s_t, a_t, r_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P( s_{t+1} = s, r_{t+1} = r | s_t, a_t ) \\
     &amp;\text{for all s, r, and past histories} \quad (s_{t}, a_{t}, ..., s_0, a_0)
\end{aligned}\end{split}\]</div>
<p>This means that the current state representation <span class="math notranslate nohighlight">\(s\)</span> contains enough information to predict the probability of arriving in the next state <span class="math notranslate nohighlight">\(s'\)</span> given the chosen action <span class="math notranslate nohighlight">\(a\)</span>. When the Markovian property is not met, we have a <strong>Partially-Observable Markov Decision Process</strong> (POMDP).</p>
</div>
<div class="section" id="returns">
<h4><span class="section-number">2.1.4.2. </span>Returns<a class="headerlink" href="#returns" title="Permalink to this headline">¶</a></h4>
<p>Suppose the sequence of rewards obtained after step <span class="math notranslate nohighlight">\(t\)</span> (after being in state <span class="math notranslate nohighlight">\(s_t\)</span> and choosing action <span class="math notranslate nohighlight">\(a_t\)</span>) is:</p>
<div class="math notranslate nohighlight">
\[ r_{t+1}, r_{t+2}, r_{t+3}, ... \]</div>
<p>What we want to maximize is the <strong>return</strong> (reward-to-go) at each time step <span class="math notranslate nohighlight">\(t\)</span>, i.e. the sum of all future rewards:</p>
<div class="math notranslate nohighlight">
\[ 
    R_t = r_{t+1} + \gamma \, r_{t+2} +  \gamma^2 \, r_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k \, r_{t+k+1}
\]</div>
<p>More generally, for a trajectory (episode) <span class="math notranslate nohighlight">\(\tau = (s_0, a_0, r_1, s_1, a_1, \ldots, s_T)\)</span>, one can define its return as:</p>
<div class="math notranslate nohighlight">
\[ R(\tau) = \sum_{t=0}^{T} \gamma^t \, r_{t+1} \]</div>
<p>For <strong>episodic tasks</strong> (which break naturally into finite episodes of length <span class="math notranslate nohighlight">\(T\)</span>, e.g. plays of a game, trips through a maze), the return is always finite and easy to compute at the end of the episode. The discount factor can be set to 1:</p>
<div class="math notranslate nohighlight">
\[ 
    R_t = \sum_{k=0}^{T} r_{t+k+1}
\]</div>
<p>For <strong>continuing tasks</strong> (which can not be split into episodes), the return could become infinite if <span class="math notranslate nohighlight">\(\gamma = 1\)</span>. The discount factor has to be smaller than 1.</p>
<div class="math notranslate nohighlight">
\[ 
    R_t = \sum_{k=0}^{\infty} \gamma^k \, r_{t+k+1}
\]</div>
<p>The discount rate <span class="math notranslate nohighlight">\(\gamma\)</span> determines the relative importance of future rewards for the behavior:</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(\gamma\)</span> is close to 0, only the immediately available rewards will count: the agent is greedy or <strong>myopic</strong>.</p></li>
<li><p>if <span class="math notranslate nohighlight">\(\gamma\)</span> is close to 1, even far-distance rewards will be taken into account: the agent is <strong>farsighted</strong>.</p></li>
</ul>
<div class="admonition-why-the-reward-on-the-long-term admonition">
<p class="admonition-title">Why the reward on the long term?</p>
<p><img alt="" src="../_images/return-example.svg" /></p>
<p>Selecting the action <span class="math notranslate nohighlight">\(a_1\)</span> in <span class="math notranslate nohighlight">\(s_1\)</span> does not bring reward immediately (<span class="math notranslate nohighlight">\(r_1 = 0\)</span>) but allows to reach <span class="math notranslate nohighlight">\(s_5\)</span> in the future and get a reward of 10. Selecting <span class="math notranslate nohighlight">\(a_2\)</span> in <span class="math notranslate nohighlight">\(s_1\)</span> brings immediately a reward of 1, but that will be all. <span class="math notranslate nohighlight">\(a_1\)</span> is <strong>better</strong> than <span class="math notranslate nohighlight">\(a_2\)</span>, because it will bring more reward <strong>on the long term</strong>.</p>
<p>When selecting <span class="math notranslate nohighlight">\(a_1\)</span> in <span class="math notranslate nohighlight">\(s_1\)</span>, the discounted return is:</p>
<div class="math notranslate nohighlight">
\[
    R = 0 + \gamma \, 0 + \gamma^2 \, 0 + \gamma^3 \, 10 + \ldots = 10 \, \gamma^3
\]</div>
<p>while it is <span class="math notranslate nohighlight">\(R= 1\)</span> for the action <span class="math notranslate nohighlight">\(a_2\)</span>.</p>
<p>For small values of <span class="math notranslate nohighlight">\(\gamma\)</span> (e.g. 0.1), <span class="math notranslate nohighlight">\(10\, \gamma^3\)</span> becomes smaller than one, so the action <span class="math notranslate nohighlight">\(a_2\)</span> leads to a higher discounted return. The discount rate <span class="math notranslate nohighlight">\(\gamma\)</span> changes the behavior of the agent. It is usually taken somewhere between 0.9 and 0.999.</p>
</div>
</div>
<div class="section" id="example-the-cartpole-balancing-task">
<h4><span class="section-number">2.1.4.3. </span>Example: the cartpole balancing task<a class="headerlink" href="#example-the-cartpole-balancing-task" title="Permalink to this headline">¶</a></h4>
<div class="figure align-default" id="id14">
<a class="reference internal image-reference" href="../_images/cartpole-after1.gif"><img alt="../_images/cartpole-after1.gif" src="../_images/cartpole-after1.gif" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.29 </span><span class="caption-text">Cartpole balancing task</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p><strong>State:</strong> Position and velocity of the cart, angle and speed of the pole.</p></li>
<li><p><strong>Actions:</strong> Commands to the motors for going left or right.</p></li>
<li><p><strong>Reward function:</strong> Depends on whether we consider the task as episodic or continuing.</p></li>
</ul>
<p>The problem can be viewed both as an episodic or continuing task:</p>
<ul class="simple">
<li><p><strong>Episodic</strong> task where episode ends upon failure:</p>
<ul>
<li><p><strong>reward</strong> = +1 for every step before failure, 0 at failure.</p></li>
<li><p><strong>return</strong> = number of steps before failure.</p></li>
</ul>
</li>
<li><p><strong>Continuing</strong> task with discounted return:</p>
<ul>
<li><p><strong>reward</strong> = -1 at failure, 0 otherwise.</p></li>
<li><p><strong>return</strong> = <span class="math notranslate nohighlight">\(- \gamma^k\)</span> for <span class="math notranslate nohighlight">\(k\)</span> steps before failure.</p></li>
</ul>
</li>
</ul>
<p>In both cases, the goal is to maximize the return by maintaining the pole vertical as long as possible.</p>
</div>
<div class="section" id="example-the-recycling-robot">
<h4><span class="section-number">2.1.4.4. </span>Example: the recycling robot<a class="headerlink" href="#example-the-recycling-robot" title="Permalink to this headline">¶</a></h4>
<div class="figure align-default" id="id15">
<a class="reference internal image-reference" href="../_images/recyclingrobot.png"><img alt="../_images/recyclingrobot.png" src="../_images/recyclingrobot.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.30 </span><span class="caption-text">Recycling robot. Source: <a class="bibtex reference internal" href="../zreferences.html#sutton1998" id="id2">[SB98]</a></span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p>At each step, the recycling robot has to decide whether it should:</p>
<ol class="simple">
<li><p>actively search for a can,</p></li>
<li><p>wait for someone to bring it a can, or</p></li>
<li><p>go to home base and recharge.</p></li>
</ol>
<p>Searching is better (more reward) but runs down the battery (probability 1-<span class="math notranslate nohighlight">\(\alpha\)</span> to empty the battery): if the robot runs out of power while searching, he has to be rescued (which leads to punishment and should be avoided). Decisions must be made on basis of the current energy level: high, low. This will be the state of the robot. The return is the number of cans collected on the long term.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{S} = \{ \text{high}, \text{low} \}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}(\text{high} ) = \{ \text{search}, \text{wait} \}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}(\text{low} ) = \{ \text{search}, \text{wait}, \text{recharge} \}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(R^{\text{search}}\)</span> = expected number of cans while searching.</p></li>
<li><p><span class="math notranslate nohighlight">\(R^{\text{wait}}\)</span> = expected number of cans while waiting.</p></li>
<li><p><span class="math notranslate nohighlight">\(R^{\text{search}} &gt; R^{\text{wait}}\)</span></p></li>
</ul>
<p>The MDP is fully described by the following table:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p><span class="math notranslate nohighlight">\(s\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(s'\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(a\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(p(s' / s, a)\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(r(s, a, s')\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>high</p></td>
<td class="text-align:center"><p>high</p></td>
<td class="text-align:center"><p>search</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\alpha\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathcal{R}^\text{search}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>high</p></td>
<td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>search</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(1 - \alpha\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathcal{R}^\text{search}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>high</p></td>
<td class="text-align:center"><p>search</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(1 - \beta\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(-3\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>search</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathcal{R}^\text{search}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>high</p></td>
<td class="text-align:center"><p>high</p></td>
<td class="text-align:center"><p>wait</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathcal{R}^\text{wait}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>high</p></td>
<td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>wait</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathcal{R}^\text{wait}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>high</p></td>
<td class="text-align:center"><p>wait</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathcal{R}^\text{wait}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>wait</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathcal{R}^\text{wait}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>high</p></td>
<td class="text-align:center"><p>recharge</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>low</p></td>
<td class="text-align:center"><p>recharge</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="the-policy">
<h4><span class="section-number">2.1.4.5. </span>The policy<a class="headerlink" href="#the-policy" title="Permalink to this headline">¶</a></h4>
<p>The probability that an agent selects a particular action <span class="math notranslate nohighlight">\(a\)</span> in a given state <span class="math notranslate nohighlight">\(s\)</span> is called the <strong>policy</strong> <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \pi &amp;: \mathcal{S} \times \mathcal{A} \rightarrow P(\mathcal{S})\\
    (s, a) &amp;\rightarrow \pi(s, a)  = P(a_t = a | s_t = s) \\
\end{align}
\end{split}\]</div>
<p>The policy can be <strong>deterministic</strong> (one action has a probability of 1, the others 0) or <strong>stochastic</strong>. The goal of an agent is to find a policy that maximizes the sum of received rewards <strong>on the long term</strong>, i.e. the <strong>return</strong> <span class="math notranslate nohighlight">\(R_t\)</span> at each each time step. This policy is called the <strong>optimal policy</strong> <span class="math notranslate nohighlight">\(\pi^*\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{J}(\pi) = \mathbb{E}_{\rho_\pi} [R_t] \qquad
    \pi^* = \text{argmax} \, \mathcal{J}(\pi)
\]</div>
</div>
<div class="section" id="goal-of-reinforcement-learning">
<h4><span class="section-number">2.1.4.6. </span>Goal of Reinforcement Learning<a class="headerlink" href="#goal-of-reinforcement-learning" title="Permalink to this headline">¶</a></h4>
<p>RL is an <strong>adaptive optimal control</strong> method for Markov Decision Processes using (sparse) rewards as a partial feedback. At each time step <span class="math notranslate nohighlight">\(t\)</span>, the agent observes its Markov state <span class="math notranslate nohighlight">\(s_t \in \mathcal{S}\)</span>, produces an action <span class="math notranslate nohighlight">\(a_t \in \mathcal{A}(s_t)\)</span>, receives a reward according to this action <span class="math notranslate nohighlight">\(r_{t+1} \in \Re\)</span> and updates its state: <span class="math notranslate nohighlight">\(s_{t+1} \in \mathcal{S}\)</span>.</p>
<p>The agent generates trajectories <span class="math notranslate nohighlight">\(\tau = (s_0, a_0, r_1, s_1, a_1, \ldots, s_T)\)</span> depending on its policy <span class="math notranslate nohighlight">\(\pi(s ,a)\)</span>.</p>
<p>The return of a trajectory is the (discounted) sum of rewards accumulated during the sequence:</p>
<div class="math notranslate nohighlight">
\[ R(\tau) = \sum_{t=0}^{T} \gamma^t \, r_{t+1} \]</div>
<p>The goal is to find the <strong>optimal policy</strong> <span class="math notranslate nohighlight">\(\pi^* (s, a)\)</span> that maximizes in expectation the return of each possible trajectory under that policy:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{J}(\pi) = \mathbb{E}_{\tau \sim \rho_\pi} [R(\tau)] \qquad
    \pi^* = \text{argmax} \, \mathcal{J}(\pi)
\]</div>
</div>
</div>
</div>
<div class="section" id="bellman-equations">
<h2><span class="section-number">2.2. </span>Bellman equations<a class="headerlink" href="#bellman-equations" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/1Z5sMSCEMRo' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="value-functions">
<h3><span class="section-number">2.2.1. </span>Value Functions<a class="headerlink" href="#value-functions" title="Permalink to this headline">¶</a></h3>
<p>A central notion in RL is to estimate the <strong>value</strong> (or <strong>utility</strong>) of every state and action of the MDP. The value of a state <span class="math notranslate nohighlight">\(V^{\pi} (s)\)</span> is the expected return when starting from that state and thereafter following the agent’s current policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>The <strong>state-value function</strong> <span class="math notranslate nohighlight">\(V^{\pi} (s)\)</span> of a state <span class="math notranslate nohighlight">\(s\)</span> given the policy <span class="math notranslate nohighlight">\(\pi\)</span> is defined as the mathematical expectation of the return after that state:</p>
<div class="math notranslate nohighlight">
\[  V^{\pi} (s) = \mathbb{E}_{\rho_\pi} ( R_t | s_t = s) = \mathbb{E}_{\rho_\pi} ( \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} |s_t=s ) \]</div>
<p>The mathematical expectation operator <span class="math notranslate nohighlight">\(\mathbb{E}(\cdot)\)</span> is indexed by <span class="math notranslate nohighlight">\(\rho_\pi\)</span>, the probability distribution of states achievable with <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>Several trajectories are possible after the state <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul class="simple">
<li><p>The <strong>state transition probability function</strong> <span class="math notranslate nohighlight">\(p(s' | s, a)\)</span> leads to different states <span class="math notranslate nohighlight">\(s'\)</span>, even if the same actions are taken.</p></li>
<li><p>The <strong>expected reward function</strong> <span class="math notranslate nohighlight">\(r(s, a, s')\)</span> provides stochastic rewards, even if the transition <span class="math notranslate nohighlight">\((s, a, s')\)</span> is the same.</p></li>
<li><p>The <strong>policy</strong> <span class="math notranslate nohighlight">\(\pi\)</span> itself is stochastic.</p></li>
</ul>
<p>Only rewards that are obtained using the policy <span class="math notranslate nohighlight">\(\pi\)</span> should be taken into account, not the complete distribution of states and rewards.</p>
<p>The value of a state is not intrinsic to the state itself, it depends on the policy: One could be in a state which is very close to the goal (only one action left to win game), but if the policy is very bad, the “good” action will not be chosen and the state will have a small value.</p>
<p>The value of taking an action <span class="math notranslate nohighlight">\(a\)</span> in a state <span class="math notranslate nohighlight">\(s\)</span> under policy <span class="math notranslate nohighlight">\(\pi\)</span> is the expected return starting from that state, taking that action, and thereafter following the following <span class="math notranslate nohighlight">\(\pi\)</span>. The <strong>action-value function</strong> for a state-action pair <span class="math notranslate nohighlight">\((s, a)\)</span> under the policy <span class="math notranslate nohighlight">\(\pi\)</span> (or <strong>Q-value</strong>) is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    Q^{\pi} (s, a)  &amp; = \mathbb{E}_{\rho_\pi} ( R_t | s_t = s, a_t =a) \\
                    &amp; = \mathbb{E}_{\rho_\pi} ( \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} |s_t=s, a_t=a) \\
\end{align}
\end{split}\]</div>
<p>State- and action-value functions are linked to each other. The value of a state <span class="math notranslate nohighlight">\(V^{\pi}(s)\)</span> depends on the value <span class="math notranslate nohighlight">\(Q^{\pi} (s, a)\)</span> of the action that will be chosen by the policy <span class="math notranslate nohighlight">\(\pi\)</span> in <span class="math notranslate nohighlight">\(s\)</span>:</p>
<div class="math notranslate nohighlight">
\[
        V^{\pi}(s) = \mathbb{E}_{a \sim \pi(s,a)} [Q^{\pi} (s, a)] = \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, Q^{\pi} (s, a)
\]</div>
<p>If the policy <span class="math notranslate nohighlight">\(\pi\)</span> is deterministic (the same action is chosen every time), the value of the state is the same as the value of that action (same expected return). If the policy <span class="math notranslate nohighlight">\(\pi\)</span> is stochastic (actions are chosen with different probabilities), the value of the state is the expectation (weighted average) of the value of the actions. If the Q-values are known, the V-values can be found easily.</p>
<p>We can note that the return at time <span class="math notranslate nohighlight">\(t\)</span> depends on the <strong>immediate reward</strong> <span class="math notranslate nohighlight">\(r_{t+1}\)</span> and the return at the next time step <span class="math notranslate nohighlight">\(t+1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{eqnarray*}
    R_t &amp;=&amp; r_{t+1} + \gamma \, r_{t+2} +  \gamma^2  \, r_{t+3} + \dots + \gamma^k \, r_{t+k+1} + \dots \\
        &amp;=&amp; r_{t+1} + \gamma \, ( r_{t+2} +  \gamma \, r_{t+3} + \dots + \gamma^{k-1} \, r_{t+k+1} + \dots) \\
        &amp;=&amp; r_{t+1} + \gamma \,  R_{t+1} \\
\end{eqnarray*}
\end{split}\]</div>
<p>When taking the mathematical expectation of that identity, we obtain:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}_{\rho_\pi}[R_t] = r(s_t, a_t, s_{t+1}) + \gamma \, \mathbb{E}_{\rho_\pi}[R_{t+1}]
\]</div>
<p>It becomes clear that the value of an action depends on the immediate reward received just after the action, as well as the value of the next state:</p>
<div class="math notranslate nohighlight">
\[
        Q^{\pi}(s_t, a_t) = r(s_t, a_t, s_{t+1}) + \gamma \,  V^{\pi} (s_{t+1})
\]</div>
<p>But that is only for a fixed <span class="math notranslate nohighlight">\((s_t, a_t, s_{t+1})\)</span> transition. Taking transition probabilities into account, one can obtain the Q-values when the V-values are known:</p>
<div class="math notranslate nohighlight">
\[
        Q^{\pi}(s, a) = \mathbb{E}_{s' \sim p(s'|s, a)} [ r(s, a, s') + \gamma \, V^{\pi} (s') ] = \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{\pi} (s') ]
\]</div>
<p>The value of an action depends on:</p>
<ul class="simple">
<li><p>the states <span class="math notranslate nohighlight">\(s'\)</span> one can arrive after the action (with a probability <span class="math notranslate nohighlight">\(p(s' | s, a)\)</span>).</p></li>
<li><p>the value of that state <span class="math notranslate nohighlight">\(V^{\pi} (s')\)</span>, weighted by <span class="math notranslate nohighlight">\(\gamma\)</span> as it is one step in the future.</p></li>
<li><p>the reward received immediately after taking that action <span class="math notranslate nohighlight">\(r(s, a, s')\)</span> (as it is not included in the value of <span class="math notranslate nohighlight">\(s'\)</span>).</p></li>
</ul>
</div>
<div class="section" id="id3">
<h3><span class="section-number">2.2.2. </span>Bellman equations<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>A fundamental property of value functions used throughout reinforcement learning is that they satisfy a  particular recursive relationship:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        V^{\pi}(s)  &amp;= \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, Q^{\pi} (s, a)\\
                    &amp;= \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{\pi} (s') ]
\end{aligned}
\end{split}\]</div>
<p>This equation is called the <strong>Bellman equation</strong> for <span class="math notranslate nohighlight">\(V^{\pi}\)</span>. It expresses the relationship between the value of a state and the value of its successors, depending on the dynamics of the MDP (<span class="math notranslate nohighlight">\(p(s' | s, a)\)</span> and <span class="math notranslate nohighlight">\(r(s, a, s')\)</span>) and the current policy <span class="math notranslate nohighlight">\(\pi\)</span>. The interesting property of the Bellman equation for RL is that it admits one and only one solution <span class="math notranslate nohighlight">\(V^{\pi}(s)\)</span>.</p>
<p>The same recursive relationship stands for <span class="math notranslate nohighlight">\(Q^{\pi}(s, a)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{eqnarray*}
        Q^{\pi}(s, a)  &amp;=&amp; \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{\pi} (s') ] \\
                    &amp;=&amp;  \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, \sum_{a' \in \mathcal{A}(s')} \pi(s', a') \, Q^{\pi} (s', a')]
\end{eqnarray*}
\end{split}\]</div>
<p>which is called the <strong>Bellman equation</strong> for <span class="math notranslate nohighlight">\(Q^{\pi}\)</span>.</p>
<p>The following <strong>backup diagrams</strong> denote these recursive relationships.</p>
<div class="figure align-default" id="id16">
<a class="reference internal image-reference" href="../_images/backup-V.png"><img alt="../_images/backup-V.png" src="../_images/backup-V.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.31 </span><span class="caption-text">Backup diagrams of the Bellman equations. Left: the value of a state <span class="math notranslate nohighlight">\(s\)</span> depends on the policy <span class="math notranslate nohighlight">\(\pi\)</span> and the value of the succeeding states <span class="math notranslate nohighlight">\(s'\)</span>. Right: the value of an action depends on the value of the action that can be taken in the next states. Source: <a class="bibtex reference internal" href="../zreferences.html#sutton1998" id="id4">[SB98]</a></span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="bellman-optimality-equations">
<h2><span class="section-number">2.3. </span>Bellman optimality equations<a class="headerlink" href="#bellman-optimality-equations" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/mFucN5K351A' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="optimal-policy">
<h3><span class="section-number">2.3.1. </span>Optimal policy<a class="headerlink" href="#optimal-policy" title="Permalink to this headline">¶</a></h3>
<p>The optimal policy is the policy that gathers the maximum of reward on the long term. Value functions define a partial ordering over policies:</p>
<ul class="simple">
<li><p>a policy <span class="math notranslate nohighlight">\(\pi\)</span> is better than another policy <span class="math notranslate nohighlight">\(\pi'\)</span> if its expected return is greater or equal than that of <span class="math notranslate nohighlight">\(\pi'\)</span> for all states <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
        \pi &gt; \pi' \Leftrightarrow V^{\pi}(s) &gt; V^{\pi'}(s) \quad \forall s \in \mathcal{S}
\]</div>
<p>There exists at least one policy that is better than all the others: this is the <strong>optimal policy</strong> <span class="math notranslate nohighlight">\(\pi^*\)</span>. We note <span class="math notranslate nohighlight">\(V^*(s)\)</span> and <span class="math notranslate nohighlight">\(Q^*(s, a)\)</span> the optimal value of the different states and actions under <span class="math notranslate nohighlight">\(\pi^*\)</span>.</p>
<div class="math notranslate nohighlight">
\[
   V^* (s) = \max_{\pi} V^{\pi}(s) \quad \forall s \in \mathcal{S}
\]</div>
<div class="math notranslate nohighlight">
\[
    Q^* (s, a) = \max_{\pi} Q^{\pi}(s, a) \quad \forall s \in \mathcal{S}, \quad \forall a \in \mathcal{A}
\]</div>
<p>When the policy is optimal <span class="math notranslate nohighlight">\(\pi^*\)</span>, the link between the V and Q values is even easier. The V and Q values are maximal for the optimal policy: there is no better alternative.</p>
<div class="figure align-default" id="id17">
<a class="reference internal image-reference" href="../_images/fullvi.png"><img alt="../_images/fullvi.png" src="../_images/fullvi.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.32 </span><span class="caption-text">Backup diagrams of the Bellman optimality equations. Source: <a class="bibtex reference internal" href="../zreferences.html#sutton1998" id="id5">[SB98]</a></span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<p><strong>The optimal action <span class="math notranslate nohighlight">\(a^*\)</span> to perform in the state <span class="math notranslate nohighlight">\(s\)</span> is the one with the highest optimal Q-value <span class="math notranslate nohighlight">\(Q^*(s, a)\)</span></strong>.</p>
<div class="math notranslate nohighlight">
\[
    a^* = \text{argmax}_a \, Q^*(s, a)
\]</div>
<p>By definition, this action will bring the maximal return when starting in <span class="math notranslate nohighlight">\(s\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    Q^*(s, a) = \mathbb{E}_{\rho_{\pi^*}} [R_t]
\]</div>
<p>The optimal policy is <strong>greedy</strong> with respect to <span class="math notranslate nohighlight">\(Q^*(s, a)\)</span>, i.e. <strong>deterministic</strong>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \pi^*(s, a) = \begin{cases}
                1 \; \text{if} \; a = a^* \\
                0 \; \text{otherwise.}
                \end{cases}
\end{split}\]</div>
</div>
<div class="section" id="id6">
<h3><span class="section-number">2.3.2. </span>Bellman optimality equations<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>As the optimal policy is deterministic, the optimal value of a state is equal to the value of the optimal action:</p>
<div class="math notranslate nohighlight">
\[
    V^* (s)  = \max_{a \in \mathcal{A}(s)} Q^{\pi^*} (s, a)
\]</div>
<p>The expected return after being in <span class="math notranslate nohighlight">\(s\)</span> is the same as the expected return after being in <span class="math notranslate nohighlight">\(s\)</span> and choosing the optimal action <span class="math notranslate nohighlight">\(a^*\)</span>, as this is the only action that can be taken. This allows to find the <strong>Bellman optimality equation</strong> for <span class="math notranslate nohighlight">\(V^*\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    V^* (s)  = \max_{a \in \mathcal{A}(s)} \sum_{s' \in \mathcal{S}}  p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{*} (s') ]
\]</div>
<p>The same Bellman optimality equation stands for <span class="math notranslate nohighlight">\(Q^*\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    Q^* (s, a)  = \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [r(s, a, s')  + \gamma \max_{a' \in \mathcal{A}(s')} Q^* (s', a') ]
\]</div>
<p>The optimal value of <span class="math notranslate nohighlight">\((s, a)\)</span> depends on the optimal action in the next state <span class="math notranslate nohighlight">\(s'\)</span>.</p>
<p>The Bellman optimality equations for <span class="math notranslate nohighlight">\(V^*\)</span> form a system of equations:</p>
<ul class="simple">
<li><p>If there are <span class="math notranslate nohighlight">\(N\)</span> states <span class="math notranslate nohighlight">\(s\)</span>, there are <span class="math notranslate nohighlight">\(N\)</span> Bellman equations with <span class="math notranslate nohighlight">\(N\)</span> unknowns <span class="math notranslate nohighlight">\(V^*(s)\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    V^* (s)  = \max_{a \in \mathcal{A}(s)} \sum_{s' \in \mathcal{S}}  p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{*} (s') ]
\]</div>
<p>If the dynamics of the environment are known (<span class="math notranslate nohighlight">\(p(s' | s, a)\)</span> and <span class="math notranslate nohighlight">\(r(s, a, s')\)</span>), then in principle one can solve this system of equations using linear algebra. For finite MDPs, the Bellman optimality equation for <span class="math notranslate nohighlight">\(V^*\)</span> has a unique solution (one and only one): This is the principle of <strong>dynamic programming</strong>.</p>
<p>The same is true for the Bellman optimality equation for <span class="math notranslate nohighlight">\(Q^*\)</span>: If there are <span class="math notranslate nohighlight">\(N\)</span> states and <span class="math notranslate nohighlight">\(M\)</span> actions available, there are <span class="math notranslate nohighlight">\(N\times M\)</span> equations with <span class="math notranslate nohighlight">\(N\times M\)</span> unknowns <span class="math notranslate nohighlight">\(Q^*(s, a)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    Q^* (s, a)  = \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [r(s, a, s')  + \gamma \max_{a' \in \mathcal{A}(s')} Q^* (s', a') ]
\]</div>
<p><span class="math notranslate nohighlight">\(V^*\)</span> and <span class="math notranslate nohighlight">\(Q^*\)</span> are interdependent: one needs only to compute one of them.</p>
<div class="math notranslate nohighlight">
\[V^* (s)  = \max_{a \in \mathcal{A}(s)} \, Q^{*} (s, a)\]</div>
<div class="math notranslate nohighlight">
\[Q^* (s, a)  = \sum_{s' \in \mathcal{S}} \, p(s' | s, a) \, [r(s, a, s') + \gamma V^*(s') ] \]</div>
<p>If you only have <span class="math notranslate nohighlight">\(V^*(s)\)</span>, you need to perform a <strong>one-step-ahead</strong> search using the dynamics of the MDP:</p>
<div class="math notranslate nohighlight">
\[
    Q^* (s, a)  = \sum_{s' \in \mathcal{S}} \, p(s' | s, a) \, [r(s, a, s') + \gamma V^*(s') ]
\]</div>
<p>and then select the optimal action with the highest <span class="math notranslate nohighlight">\(Q^*\)</span>-value. Using the <span class="math notranslate nohighlight">\(V^*(s)\)</span> values is called <strong>model-based</strong>: you need to know the model of the environment to act, at least locally.</p>
<p>If you have all <span class="math notranslate nohighlight">\(Q^*(s, a)\)</span>, the optimal policy is straightforward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \pi^*(s, a) = \begin{cases}
                1 \; \text{if} \; a = \text{argmax}_a \, Q^*(s, a) \\
                0 \; \text{otherwise.}
                \end{cases}
\end{split}\]</div>
<p>Finding <span class="math notranslate nohighlight">\(Q^*\)</span> makes the selection of optimal actions easy:</p>
<ul class="simple">
<li><p>no need to iterate over all actions and to know the dynamics <span class="math notranslate nohighlight">\(p(s' | s, a)\)</span> and <span class="math notranslate nohighlight">\(r(s, a, s')\)</span>.</p></li>
<li><p>for any state <span class="math notranslate nohighlight">\(s\)</span>, it can simply find the action that maximizes <span class="math notranslate nohighlight">\(Q^*(s,a)\)</span>.</p></li>
</ul>
<p>The action-value function effectively <strong>caches</strong> the results of all one-step-ahead searches into a single value: <strong>model-free</strong>. At the cost of representing a function of all state-action pairs, the optimal action-value function allows optimal actions to be selected without having to know anything about the environment’s dynamics. But there are <span class="math notranslate nohighlight">\(N \times M\)</span> equations to solve instead of just <span class="math notranslate nohighlight">\(N\)</span>…</p>
<p>Finding an optimal policy by solving the <strong>Bellman optimality equations</strong> requires the following:</p>
<ul class="simple">
<li><p>accurate knowledge of environment dynamics <span class="math notranslate nohighlight">\(p(s' | s, a)\)</span> and <span class="math notranslate nohighlight">\(r(s, a, s')\)</span> for all transitions;</p></li>
<li><p>enough memory and time to do the computations;</p></li>
<li><p>the Markov property.</p></li>
</ul>
<p>How much space and time do we need? A solution requires an exhaustive search, looking ahead at all possibilities, computing their probabilities of occurrence and their desirability in terms of expected rewards. The number of states is often huge or astronomical (e.g., Go has about <span class="math notranslate nohighlight">\(10^{170}\)</span> states). <strong>Dynamic programming</strong> solves exactly the Bellman equations; <strong>Monte-Carlo</strong> and <strong>temporal-difference</strong> methods approximate them.</p>
</div>
</div>
<div class="section" id="generalized-policy-iteration">
<h2><span class="section-number">2.4. </span>Generalized Policy Iteration<a class="headerlink" href="#generalized-policy-iteration" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="id18">
<a class="reference internal image-reference" href="../_images/gpi-scheme.png"><img alt="../_images/gpi-scheme.png" src="../_images/gpi-scheme.png" style="width: 30%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.33 </span><span class="caption-text">Generalized Policy Iteration. Source: <a class="bibtex reference internal" href="../zreferences.html#sutton1998" id="id7">[SB98]</a></span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
<p>RL algorithms iterate over two steps:</p>
<ol class="simple">
<li><p><strong>Policy evaluation</strong></p>
<ul class="simple">
<li><p>For a given policy <span class="math notranslate nohighlight">\(\pi\)</span>, the value of all states <span class="math notranslate nohighlight">\(V^\pi(s)\)</span> or all state-action pairs <span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span> is calculated, either based on:</p>
<ul>
<li><p>the Bellman equations (Dynamic Programming)</p></li>
<li><p>sampled experience (Monte-Carlo and Temporal Difference)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Policy improvement</strong></p>
<ul class="simple">
<li><p>From the current estimated values <span class="math notranslate nohighlight">\(V^\pi(s)\)</span> or <span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span>, a new <strong>better</strong> policy <span class="math notranslate nohighlight">\(\pi\)</span> is derived.</p></li>
</ul>
</li>
</ol>
<p>After enough iterations, the policy converges to the <strong>optimal policy</strong> (if the states are Markov).</p>
<div class="admonition-different-notations-in-rl admonition">
<p class="admonition-title">Different notations in RL</p>
<p>Notations can vary depending on the source. The ones used in this course use what you can read in most modern deep RL papers (Deepmind, OpenAI), but beware that you can encounter <span class="math notranslate nohighlight">\(G_t\)</span> for the return…</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p></p></th>
<th class="text-align:center head"><p><strong>This course</strong></p></th>
<th class="text-align:center head"><p><strong>Sutton and Barto 1998</strong></p></th>
<th class="text-align:center head"><p><strong>Sutton and Barto 2017</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>Current state</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(s_t\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(s_t\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(S_t\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>Selected action</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(a_t\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(a_t\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(A_t\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>Sampled reward</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(r_{t+1}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(r_{t+1}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(R_{t+1}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>Transition probability</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(p(s' / s,a)\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathcal{P}_{ss'}^a\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(p(s'/s, a)\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>Expected reward</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(r(s,a, s')\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathcal{R}_{ss'}^a\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(r(s, a, s')\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>Return</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(R_t\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(R_t\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_t\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>State value function</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(V^\pi(s)\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(V^\pi(s)\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(v_\pi(s)\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>Action value function</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(q_\pi(s, a)\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2-tabular"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="1-Bandits.html" title="previous page"><span class="section-number">1. </span>Bandits</a>
    <a class='right-next' id="next-link" href="../5-exercises/ex1-Python.html" title="next page"><span class="section-number">1. </span>Introduction to Python</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>