

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Bibliography &#8212; Deep Reinforcement Learning</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-deeprl/zreferences.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="10.2. Eligibility traces" href="5-exercises/10-Eligibilitytraces-solution.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-deeprl/zreferences.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Bibliography" />
<meta property="og:description" content="Bibliography  Agarwal et al., 2014  Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., &amp; Schapire, R. E. (2014). Taming the Monster: A Fast and Simple Algori" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-deeprl/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Deep Reinforcement Learning</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Deep Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tabular RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="2-tabular/1-Bandits.html">
   1. Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-tabular/2-MDP.html">
   2. Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-tabular/3-DP.html">
   3. Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-tabular/4-MC.html">
   4. Monte-Carlo (MC) methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-tabular/5-TD.html">
   5. Temporal Difference learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-tabular/6-FA.html">
   6. Function approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-tabular/7-NN.html">
   7. Deep learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Model-free RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="3-MF/1-DQN.html">
   1. Deep Q-Learning (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-MF/2-BeyondDQN.html">
   2. Beyond DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-MF/3-PG.html">
   3. Policy gradient (PG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-MF/4-A3C.html">
   4. Advantage actor-critic (A2C, A3C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-MF/5-DDPG.html">
   5. Deep Deterministic Policy Gradient (DDPG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-MF/6-PPO.html">
   6. Natural gradients (TRPO, PPO)
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex3-Sampling.html">
   3. Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex4-Bandits.html">
   4. Bandits (part 1)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex5-Bandits2.html">
   5. Bandits (part 2)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex6-DP.html">
   6. Dynamic programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex7-Gym.html">
   7. Gym environments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex8-MC.html">
   8. Monte-Carlo control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex9-TD.html">
   9. Q-learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-exercises/ex10-Eligibilitytraces.html">
   10. Eligibility traces
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/zreferences.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            
        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bibliography">
<h1><span class="section-number">1. </span>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h1>
<p id="bibtex-bibliography-zreferences-0"><dl class="citation">
<dt class="bibtex label" id="agarwal2014"><span class="brackets">Agarwal et al., 2014</span></dt>
<dd><p>Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., &amp; Schapire, R. E. (2014). Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits. <em>Proceedings of the 31 St International Conference on Machine Learning</em> (p. 9). Beijing, China. URL: <a class="reference external" href="https://arxiv.org/abs/1402.0555">https://arxiv.org/abs/1402.0555</a></p>
</dd>
<dt class="bibtex label" id="bellemare2017"><span class="brackets">Bellemare et al., 2017</span></dt>
<dd><p>Bellemare, M. G., Dabney, W., &amp; Munos, R. (2017 , July). A Distributional Perspective on Reinforcement Learning. <em>arXiv:1707.06887 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1707.06887">http://arxiv.org/abs/1707.06887</a>, <a class="reference external" href="https://arxiv.org/abs/1707.06887">arXiv:1707.06887</a></p>
</dd>
<dt class="bibtex label" id="dabney2018"><span class="brackets">Dabney et al., 2018</span></dt>
<dd><p>Dabney, W., Ostrovski, G., Silver, D., &amp; Munos, R. (2018 , June). Implicit Quantile Networks for Distributional Reinforcement Learning. <em>arXiv:1806.06923 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1806.06923">http://arxiv.org/abs/1806.06923</a>, <a class="reference external" href="https://arxiv.org/abs/1806.06923">arXiv:1806.06923</a></p>
</dd>
<dt class="bibtex label" id="dabney2017"><span class="brackets">Dabney et al., 2017</span></dt>
<dd><p>Dabney, W., Rowland, M., Bellemare, M. G., &amp; Munos, R. (2017 , October). Distributional Reinforcement Learning with Quantile Regression. <em>arXiv:1710.10044 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1710.10044">http://arxiv.org/abs/1710.10044</a>, <a class="reference external" href="https://arxiv.org/abs/1710.10044">arXiv:1710.10044</a></p>
</dd>
<dt class="bibtex label" id="degris2012"><span class="brackets">Degris et al., 2012</span></dt>
<dd><p>Degris, T., White, M., &amp; Sutton, R. S. (2012 , May). Linear Off-Policy Actor-Critic. <em>Proceedings of the 2012 International Conference on Machine Learning</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1205.4839">http://arxiv.org/abs/1205.4839</a></p>
</dd>
<dt class="bibtex label" id="fortunato2017"><span class="brackets">Fortunato et al., 2017</span></dt>
<dd><p>Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., … Legg, S. (2017 , June). Noisy Networks for Exploration. <em>arXiv:1706.10295 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1706.10295">http://arxiv.org/abs/1706.10295</a>, <a class="reference external" href="https://arxiv.org/abs/1706.10295">arXiv:1706.10295</a></p>
</dd>
<dt class="bibtex label" id="fujimoto2018"><span class="brackets">Fujimoto et al., 2018</span></dt>
<dd><p>Fujimoto, S., Meger, D., &amp; Precup, D. (2018 , December). Off-Policy Deep Reinforcement Learning without Exploration. <em>arXiv:1812.02900 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1812.02900">http://arxiv.org/abs/1812.02900</a>, <a class="reference external" href="https://arxiv.org/abs/1812.02900">arXiv:1812.02900</a></p>
</dd>
<dt class="bibtex label" id="goodfellow2016"><span class="brackets">Goodfellow et al., 2016</span></dt>
<dd><p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p>
</dd>
<dt class="bibtex label" id="gruslys2017"><span class="brackets">Gruslys et al., 2017</span></dt>
<dd><p>Gruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., &amp; Munos, R. (2017 , April). The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning. <em>arXiv:1704.04651 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1704.04651">http://arxiv.org/abs/1704.04651</a>, <a class="reference external" href="https://arxiv.org/abs/1704.04651">arXiv:1704.04651</a></p>
</dd>
<dt class="bibtex label" id="hausknecht2015"><span class="brackets">Hausknecht &amp; Stone, 2015</span></dt>
<dd><p>Hausknecht, M., &amp; Stone, P. (2015 , July). Deep Recurrent Q-Learning for Partially Observable MDPs. <em>arXiv:1507.06527 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1507.06527">http://arxiv.org/abs/1507.06527</a>, <a class="reference external" href="https://arxiv.org/abs/1507.06527">arXiv:1507.06527</a></p>
</dd>
<dt class="bibtex label" id="hessel2017"><span class="brackets">Hessel et al., 2017</span></dt>
<dd><p>Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., … Silver, D. (2017 , October). Rainbow: Combining Improvements in Deep Reinforcement Learning. <em>arXiv:1707.06887 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1710.02298">http://arxiv.org/abs/1710.02298</a>, <a class="reference external" href="https://arxiv.org/abs/1707.06887">arXiv:1707.06887</a></p>
</dd>
<dt class="bibtex label" id="horgan2018"><span class="brackets">Horgan et al., 2018</span></dt>
<dd><p>Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H., &amp; Silver, D. (2018 , March). Distributed Prioritized Experience Replay. <em>arXiv:1803.00933 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1803.00933">http://arxiv.org/abs/1803.00933</a>, <a class="reference external" href="https://arxiv.org/abs/1803.00933">arXiv:1803.00933</a></p>
</dd>
<dt class="bibtex label" id="kakade2002"><span class="brackets">Kakade &amp; Langford, 2002</span></dt>
<dd><p>Kakade, S., &amp; Langford, J. (2002). Approximately Optimal Approximate Reinforcement Learning. <em>Proc. 19th International Conference on Machine Learning</em>, pp. 267–274. URL: <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601</a></p>
</dd>
<dt class="bibtex label" id="kapturowski2019"><span class="brackets">Kapturowski et al., 2019</span></dt>
<dd><p>Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., &amp; Dabney, W. (2019). Recurrent experience replay in distributed reinforcement learning. <em>ICLR</em> (p. 19). URL: <a class="reference external" href="https://openreview.net/pdf?id=r1lyTjAqYX">https://openreview.net/pdf?id=r1lyTjAqYX</a></p>
</dd>
<dt class="bibtex label" id="kendall2018"><span class="brackets">Kendall et al., 2018</span></dt>
<dd><p>Kendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., … Shah, A. (2018 , July). Learning to Drive in a Day. <em>arXiv:1807.00412 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1807.00412">http://arxiv.org/abs/1807.00412</a>, <a class="reference external" href="https://arxiv.org/abs/1807.00412">arXiv:1807.00412</a></p>
</dd>
<dt class="bibtex label" id="lillicrap2015"><span class="brackets">Lillicrap et al., 2015</span></dt>
<dd><p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … Wierstra, D. (2015). Continuous control with deep reinforcement learning. <em>CoRR</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1509.02971">http://arxiv.org/abs/1509.02971</a></p>
</dd>
<dt class="bibtex label" id="mao2016"><span class="brackets">Mao et al., 2016</span></dt>
<dd><p>Mao, H., Alizadeh, M., Menache, I., &amp; Kandula, S. (2016). Resource Management with Deep Reinforcement Learning. <em>Proceedings of the 15th ACM Workshop on Hot Topics in Networks - HotNets ‘16</em> (pp. 50–56). Atlanta, GA, USA: ACM Press. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?doid=3005745.3005750">http://dl.acm.org/citation.cfm?doid=3005745.3005750</a>, <a class="reference external" href="https://doi.org/10.1145/3005745.3005750">doi:10.1145/3005745.3005750</a></p>
</dd>
<dt class="bibtex label" id="mnih2016"><span class="brackets">Mnih et al., 2016</span></dt>
<dd><p>Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., … Kavukcuoglu, K. (2016 , February). Asynchronous Methods for Deep Reinforcement Learning. <em>Proc. ICML</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</a></p>
</dd>
<dt class="bibtex label" id="mnih2013"><span class="brackets">Mnih et al., 2013</span></dt>
<dd><p>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013 , December). Playing Atari with Deep Reinforcement Learning. <em>arXiv:1312.5602 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a>, <a class="reference external" href="https://arxiv.org/abs/1312.5602">arXiv:1312.5602</a></p>
</dd>
<dt class="bibtex label" id="moore1993"><span class="brackets">Moore &amp; Atkeson, 1993</span></dt>
<dd><p>Moore, A. W., &amp; Atkeson, C. G. (1993 , October). Prioritized sweeping: Reinforcement learning with less data and less time. <em>Machine Learning</em>, <em>13</em>(1), 103–130. URL: <a class="reference external" href="https://doi.org/10.1007/BF00993104">https://doi.org/10.1007/BF00993104</a>, <a class="reference external" href="https://doi.org/10.1007/BF00993104">doi:10.1007/BF00993104</a></p>
</dd>
<dt class="bibtex label" id="nair2015"><span class="brackets">Nair et al., 2015</span></dt>
<dd><p>Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., … Silver, D. (2015). Massively Parallel Methods for Deep Reinforcement Learning. <em>arXiv:1507.04296 [cs]</em>. URL: <a class="reference external" href="https://arxiv.org/pdf/1507.04296.pdf">https://arxiv.org/pdf/1507.04296.pdf</a>, <a class="reference external" href="https://arxiv.org/abs/1507.04296">arXiv:1507.04296</a></p>
</dd>
<dt class="bibtex label" id="niu2011"><span class="brackets">Niu et al., 2011</span></dt>
<dd><p>Niu, F., Recht, B., Re, C., &amp; Wright, S. J. (2011). HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. <em>Proc. Advances in Neural Information Processing Systems</em> (pp. 21–21). URL: <a class="reference external" href="http://arxiv.org/abs/1106.5730">http://arxiv.org/abs/1106.5730</a></p>
</dd>
<dt class="bibtex label" id="plappert2018"><span class="brackets">Plappert et al., 2018</span></dt>
<dd><p>Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y., Chen, X., … Andrychowicz, M. (2018 , January). Parameter Space Noise for Exploration. <em>arXiv:1706.01905 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1706.01905">http://arxiv.org/abs/1706.01905</a>, <a class="reference external" href="https://arxiv.org/abs/1706.01905">arXiv:1706.01905</a></p>
</dd>
<dt class="bibtex label" id="schaul2015"><span class="brackets">Schaul et al., 2015</span></dt>
<dd><p>Schaul, T., Quan, J., Antonoglou, I., &amp; Silver, D. (2015 , November). Prioritized Experience Replay. <em>arXiv:1511.05952 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1511.05952">http://arxiv.org/abs/1511.05952</a>, <a class="reference external" href="https://arxiv.org/abs/1511.05952">arXiv:1511.05952</a></p>
</dd>
<dt class="bibtex label" id="schulman2015"><span class="brackets">Schulman et al., 2015a</span></dt>
<dd><p>Schulman, J., Levine, S., Abbeel, P., Jordan, M., &amp; Moritz, P. (2015 , June). Trust Region Policy Optimization. <em>Proceedings of the 31 St International Conference on Machine Learning</em> (pp. 1889–1897). URL: <a class="reference external" href="http://proceedings.mlr.press/v37/schulman15.html">http://proceedings.mlr.press/v37/schulman15.html</a></p>
</dd>
<dt class="bibtex label" id="schulman2015a"><span class="brackets">Schulman et al., 2015b</span></dt>
<dd><p>Schulman, J., Moritz, P., Levine, S., Jordan, M., &amp; Abbeel, P. (2015 , June). High-Dimensional Continuous Control Using Generalized Advantage Estimation. <em>arXiv:1506.02438 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1506.02438">http://arxiv.org/abs/1506.02438</a>, <a class="reference external" href="https://arxiv.org/abs/1506.02438">arXiv:1506.02438</a></p>
</dd>
<dt class="bibtex label" id="schulman2017"><span class="brackets">Schulman et al., 2017</span></dt>
<dd><p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017 , July). Proximal Policy Optimization Algorithms. <em>arXiv:1707.06347 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a>, <a class="reference external" href="https://arxiv.org/abs/1707.06347">arXiv:1707.06347</a></p>
</dd>
<dt class="bibtex label" id="silver2014"><span class="brackets">Silver et al., 2014</span></dt>
<dd><p>Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., &amp; Riedmiller, M. (2014). Xing, E. P., &amp; Jebara, T. (Eds.). Deterministic Policy Gradient Algorithms. <em>Proc. ICML</em> (pp. 387–395). PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v32/silver14.html">http://proceedings.mlr.press/v32/silver14.html</a></p>
</dd>
<dt class="bibtex label" id="sutton1998"><span class="brackets">Sutton &amp; Barto, 1998</span></dt>
<dd><p>Sutton, R. S., &amp; Barto, A. G. (1998). <em>Reinforcement Learning: An Introduction</em>. Cambridge, MA: MIT press.</p>
</dd>
<dt class="bibtex label" id="sutton2017"><span class="brackets">Sutton &amp; Barto, 2017</span></dt>
<dd><p>Sutton, R. S., &amp; Barto, A. G. (2017). <em>Reinforcement Learning: An Introduction</em>. Second ed. Cambridge, MA: MIT Press.</p>
</dd>
<dt class="bibtex label" id="sutton1999"><span class="brackets">Sutton et al., 1999</span></dt>
<dd><p>Sutton, R. S., McAllester, D., Singh, S., &amp; Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. <em>Proceedings of the 12th International Conference on Neural Information Processing Systems</em> (pp. 1057–1063). MIT Press. URL: <a class="reference external" href="https://dl.acm.org/citation.cfm?id=3009806">https://dl.acm.org/citation.cfm?id=3009806</a></p>
</dd>
<dt class="bibtex label" id="tesauro1995"><span class="brackets">Tesauro, 1995</span></dt>
<dd><p>Tesauro, G. (1995). Murray, A. F. (Ed.). TD-Gammon: A Self-Teaching Backgammon Program. <em>Applications of Neural Networks</em> (pp. 267–285). Boston, MA: Springer US.</p>
</dd>
<dt class="bibtex label" id="uhlenbeck1930"><span class="brackets">Uhlenbeck &amp; Ornstein, 1930</span></dt>
<dd><p>Uhlenbeck, G. E., &amp; Ornstein, L.S. (1930). On the Theory of the Brownian Motion. <em>Physical Review</em>, <em>36</em>. <a class="reference external" href="https://doi.org/10.1103/PhysRev.36.823">doi:10.1103/PhysRev.36.823</a></p>
</dd>
<dt class="bibtex label" id="wang2017"><span class="brackets">Wang et al., 2017</span></dt>
<dd><p>Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., &amp; de Freitas, N. (2017 , November). Sample Efficient Actor-Critic with Experience Replay. <em>arXiv:1611.01224 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1611.01224">http://arxiv.org/abs/1611.01224</a>, <a class="reference external" href="https://arxiv.org/abs/1611.01224">arXiv:1611.01224</a></p>
</dd>
<dt class="bibtex label" id="wang2016"><span class="brackets">Wang et al., 2016</span></dt>
<dd><p>Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., &amp; de Freitas, N. (2016 , April). Dueling Network Architectures for Deep Reinforcement Learning. <em>arXiv:1511.06581 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1511.06581">http://arxiv.org/abs/1511.06581</a>, <a class="reference external" href="https://arxiv.org/abs/1511.06581">arXiv:1511.06581</a></p>
</dd>
<dt class="bibtex label" id="williams1992"><span class="brackets">Williams, 1992</span></dt>
<dd><p>Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. <em>Machine Learning</em>, <em>8</em>, 229–256.</p>
</dd>
<dt class="bibtex label" id="barth-maron2018"><span class="brackets">Barth-Maron et al., 2018</span></dt>
<dd><p>Barth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB, D., … Lillicrap, T. (2018 , April). Distributed Distributional Deterministic Policy Gradients. <em>arXiv:1804.08617 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1804.08617">http://arxiv.org/abs/1804.08617</a>, <a class="reference external" href="https://arxiv.org/abs/1804.08617">arXiv:1804.08617</a></p>
</dd>
<dt class="bibtex label" id="vanhasselt2015"><span class="brackets">van Hasselt et al., 2015</span></dt>
<dd><p>van Hasselt, H., Guez, A., &amp; Silver, D. (2015 , September). Deep Reinforcement Learning with Double Q-learning. <em>arXiv:1509.06461 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1509.06461">http://arxiv.org/abs/1509.06461</a>, <a class="reference external" href="https://arxiv.org/abs/1509.06461">arXiv:1509.06461</a></p>
</dd>
</dl>
</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="5-exercises/10-Eligibilitytraces-solution.html" title="previous page"><span class="section-number">10.2. </span>Eligibility traces</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>