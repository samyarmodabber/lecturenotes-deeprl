{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "rng = np.random.default_rng()\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def running_average(x, N):\n",
    "    cumsum = np.cumsum(np.insert(np.array(x), 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "### Interaction with the environment\n",
    "\n",
    "The goal of this exercise is to solve the **Gridworld** problem using Q-learning. The code is adapted from  <https://github.com/rlcode/reinforcement-learning-kr>\n",
    "\n",
    "The agent is represented by the red square: the **state** $s$ of the agent is its position in the 5x5 grid, i.e. a number between 0 and 24.\n",
    "\n",
    "The agent can move either to the left, right, top or bottom. When the agent tries to move outside of the environment, it stays at its current position. There are four **actions** $a$ available, which are deterministic.    \n",
    "\n",
    "Its goal is to reach the blue circle, while avoiding the green triangles. Actions leading to the blue circle receive a reward $r$ of +100, actions leading to a green triangle receive a reward of -100. The episode ends in those states. All other actions have a reward of -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows you to run a **random agent** in this environment for maximally 1000 episodes: at each time step, the action is selected randomly between 0 and 3 with `env.action_space.sample()`.\n",
    "\n",
    "The graphical interface and the dynamics of the environment are implemented in the file `Gridworld.py`. You do not have to read the file (but you can if you want...). It follows mainly the API of the `gym` library.\n",
    "\n",
    "The environment is created with:\n",
    "\n",
    "```python\n",
    "env = Gridworld(size, rewards)\n",
    "```\n",
    "\n",
    "`size` is the number of cells in the environment (here `(5, 5)`). `rewards` define the rewards received when hitting a blue circle, a green triangle or the default, respectively.\n",
    "\n",
    "`env.render()` allows to create and/or refresh the GUI. If you pass it a Q-table (`env.render(q_table)`), it will print the Q_value of each action at the corresponding location.\n",
    "\n",
    "`state = env.reset()` allows to start an episode. The agent is placed at the top-left of the grid (`state = 0`). \n",
    "\n",
    "`next_state, reward, done, info = env.step(action)` allows to perform an action on the environment. `action` must be a number between 0 and 3. It return the next state, the reward received during the transition, as well as a boolean `done` which is `True` when the episode is terminated (the agent moved to a blue circle or a green triangle), `False` otherwise. `info` is also a boolean, which is set to True when you close the window, \n",
    "\n",
    "**Q:** Understand and run the code. Does the agent succeed often? How complex is the task compared to Taxi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(gym.Env):\n",
    "    \n",
    "    def __init__(self, rewards=[100, -100, -1], size=5):\n",
    "        \"Initialize the environment, can accept additional parameters such as the number of states and actions.\"\n",
    "        \n",
    "        self.size = size\n",
    "        self.rewards = rewards\n",
    "        \n",
    "        # State space\n",
    "        self.observation_space = gym.spaces.Discrete(self.size**2)\n",
    "        \n",
    "        # Action space\n",
    "        self.action_space = gym.spaces.Discrete(4)    \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.state = self.reset()\n",
    "        self.t_max = 100\n",
    "        \n",
    "        self.target = (3, 2)\n",
    "        self.distractor1 = (2, 2)\n",
    "        self.distractor2 = (3, 3)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Resets the environment and starts from an initial state.\"\n",
    "        \n",
    "        # Sample one state randomly \n",
    "        self.state = self.rank((0, self.size-1))\n",
    "        self._t = 0\n",
    "        self.sum_rewards = 0.0\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Takes an action and returns a new state, a reward, a boolean (True for terminal states) \n",
    "        and a dictionary with additional info (optional).\n",
    "        \"\"\"\n",
    "        self._t +=1\n",
    "        self.done = False\n",
    "        \n",
    "        x, y = self.coordinates(self.state)\n",
    "        if action == 0: # up\n",
    "            y = max(min(y + 1, self.size-1), 0)\n",
    "        elif action == 1: # down\n",
    "            y = max(min(y - 1, self.size-1), 0)\n",
    "        elif action == 2: # left\n",
    "            x = max(min(x - 1, self.size-1), 0)\n",
    "        elif action == 3: # right\n",
    "            x = max(min(x + 1, self.size-1), 0)\n",
    "        \n",
    "        self.state = self.rank((x, y))\n",
    "        \n",
    "        if self.state == self.rank(self.target):\n",
    "            self.reward = self.rewards[0]\n",
    "            self.done = True\n",
    "        elif self.state == self.rank(self.distractor1) or self.state == self.rank(self.distractor2):\n",
    "            self.reward = self.rewards[1]\n",
    "            self.done = True\n",
    "        else:\n",
    "            self.reward = self.rewards[2]\n",
    "            \n",
    "        if self._t >= self.t_max:\n",
    "            self.done = True\n",
    "            \n",
    "        self.sum_rewards += self.reward\n",
    "        self.info = {'return': self.sum_rewards}\n",
    "        \n",
    "        return self.state, self.reward, self.done, self.info\n",
    "\n",
    "    def render(self, Q):\n",
    "        \"Displays the current state of the environment.\"\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # grid\n",
    "        for i in range(1, self.size):\n",
    "            plt.plot([i, i], [0, self.size], color=\"black\")\n",
    "        for i in range(1, self.size):\n",
    "            plt.plot([0, self.size], [i, i], color=\"black\")\n",
    "            \n",
    "        # Q-values\n",
    "        for s in range(self.size*self.size):\n",
    "            c = self.coordinates(s)\n",
    "            plt.text(c[0] + 0.5, c[1] + 0.9, f\"{Q[s, 0]:.2f}\", horizontalalignment='center', verticalalignment='center')\n",
    "            plt.text(c[0] + 0.5, c[1] + 0.1, f\"{Q[s, 1]:.2f}\", horizontalalignment='center', verticalalignment='center')\n",
    "            plt.text(c[0] + 0.05, c[1] + 0.5, f\"{Q[s, 2]:.2f}\", horizontalalignment='left', verticalalignment='center')\n",
    "            plt.text(c[0] + 0.95, c[1] + 0.5, f\"{Q[s, 3]:.2f}\", horizontalalignment='right', verticalalignment='center')\n",
    "            \n",
    "        # target\n",
    "        plt.scatter([self.target[0] +0.5], [self.target[1] + 0.5], s=5000, marker=\"o\", color=\"blue\")\n",
    "        plt.scatter([self.distractor1[0] +0.5], [self.distractor1[1] + 0.5], s=5000, marker=\"^\", color=\"green\")\n",
    "        plt.scatter([self.distractor2[0] +0.5], [self.distractor2[1] + 0.5], s=5000, marker=\"^\", color=\"green\")\n",
    "        \n",
    "        # state\n",
    "        s = self.coordinates(self.state)\n",
    "        plt.scatter([s[0] + 0.5], [s[1] + 0.5], s=5000, marker=\"s\", color=\"red\")\n",
    "        \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlim((0, self.size))\n",
    "        plt.ylim((0, self.size))\n",
    "        plt.title(\"Step \" + str(self._t) + \" - undiscounted return \" + str(self.sum_rewards))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "    \n",
    "    def coordinates(self, state):\n",
    "        \"Returns coordinates of a state.\"\n",
    "        return (state % self.size, int(state/self.size))\n",
    "    \n",
    "    def rank(self, coord):\n",
    "        \"Returns rank from coordinates.\"\n",
    "        return coord[0] + self.size*coord[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.Q = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"Selects an action randomly\"\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def train(self, nb_episodes, render=True):\n",
    "        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n",
    "\n",
    "        # Returns\n",
    "        returns = []\n",
    "\n",
    "        # Fixed number of episodes\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Reset\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            nb_steps = 0\n",
    "\n",
    "            # Store rewards\n",
    "            rewards = []\n",
    "\n",
    "            # Sample the episode\n",
    "            while not done:\n",
    "                \n",
    "                # Render the current state\n",
    "                if render:\n",
    "                    clear_output(wait=True)\n",
    "                    self.env.render(self.Q)\n",
    "                    time.sleep(0.1)\n",
    "\n",
    "                # Select an action \n",
    "                action = self.act(state)\n",
    "\n",
    "                # Perform the action\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                # Append reward\n",
    "                rewards.append(reward)\n",
    "\n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Increment time\n",
    "                nb_steps += 1\n",
    "                \n",
    "                if done: \n",
    "                    clear_output(wait=True)\n",
    "                    self.env.render(self.Q)\n",
    "\n",
    "            # Compute the discounted return of the episode.\n",
    "            return_episode = np.sum(rewards)    \n",
    "\n",
    "            # Store info\n",
    "            returns.append(return_episode)\n",
    "            \n",
    "        return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-183]\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = GridWorld()\n",
    "\n",
    "# Create the agent\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "returns = agent.train(1, render=True)\n",
    "        \n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-learning agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma, epsilon, decay_epsilon, alpha):\n",
    "        \"\"\"\n",
    "        :param env: gym-like environment\n",
    "        :param gamma: discount factor\n",
    "        :param epsilon: exploration parameter\n",
    "        :param decay_epsilon: exploration decay parameter\n",
    "        :param alpha: learning rate\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.decay_epsilon = decay_epsilon\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Q_table\n",
    "        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Returns an action using epsilon-greedy action selection.\"\n",
    "        \n",
    "        action = rng.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])\n",
    "        \n",
    "        if rng.random() < self.epsilon:\n",
    "            action = self.env.action_space.sample() \n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"Updates the agent using a single transition.\"\n",
    "        \n",
    "        # Bellman target\n",
    "        target = reward\n",
    "        \n",
    "        if not done:\n",
    "            target += self.gamma * self.Q[next_state, :].max()\n",
    "        \n",
    "        # Update the Q-value\n",
    "        self.Q[state, action] += self.alpha * (target - self.Q[state, action])\n",
    "            \n",
    "        # Decay epsilon\n",
    "        self.epsilon = self.epsilon * (1 - self.decay_epsilon)\n",
    "            \n",
    "    \n",
    "    def train(self, nb_episodes, render=True):\n",
    "        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n",
    "\n",
    "        # Returns\n",
    "        returns = []\n",
    "\n",
    "        # Fixed number of episodes\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Reset\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            nb_steps = 0\n",
    "\n",
    "            # Store rewards\n",
    "            rewards = []\n",
    "\n",
    "            # Sample the episode\n",
    "            while not done:\n",
    "                \n",
    "                # Render the current state\n",
    "                if render:\n",
    "                    clear_output(wait=True)\n",
    "                    self.env.render(self.Q)\n",
    "                    time.sleep(0.01)\n",
    "\n",
    "                # Select an action \n",
    "                action = self.act(state)\n",
    "\n",
    "                # Perform the action\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                # Append reward\n",
    "                rewards.append(reward)\n",
    "\n",
    "                # Learn from the transition\n",
    "                self.update(state, action, reward, next_state, done)\n",
    "\n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Increment time\n",
    "                nb_steps += 1\n",
    "                \n",
    "                if done: \n",
    "                    clear_output(wait=True)\n",
    "                    self.env.render(self.Q)\n",
    "\n",
    "            # Compute the discounted return of the episode.\n",
    "            return_episode = self.discounted_return(rewards)    \n",
    "\n",
    "            # Store info\n",
    "            returns.append(return_episode)\n",
    "            \n",
    "        return returns\n",
    "    \n",
    "    def test(self, render=True):\n",
    "        \"Performs a test episode without exploration.\"\n",
    "        previous_epsilon = self.epsilon\n",
    "        self.epsilon = 0.0\n",
    "        \n",
    "        # Reset\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        nb_steps = 0\n",
    "        rewards= 0\n",
    "\n",
    "        # Sample the episode\n",
    "        while not done:\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                self.env.render(self.Q)\n",
    "                time.sleep(0.05)\n",
    "            action = self.act(state)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            rewards += reward\n",
    "            state = next_state\n",
    "            nb_steps += 1\n",
    "            \n",
    "        self.epsilon = previous_epsilon\n",
    "            \n",
    "        return rewards, nb_steps\n",
    "    \n",
    "    def discounted_return(self, rewards):\n",
    "        \"Computes the discounted return of an episode based on the list of rewards.\"\n",
    "        ret = 0.0\n",
    "        for reward in reversed(rewards):\n",
    "            ret = reward + self.gamma*ret\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-aa695da14e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train the agent for 200 episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-8ed845122e2a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nb_episodes, render)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-94ab9f9cfde2>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, Q)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \"\"\"\n\u001b[1;32m    352\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backend_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m                 result = print_method(\n\u001b[0m\u001b[1;32m   2211\u001b[0m                     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m                     \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    511\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    406\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   1864\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2745\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2747\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_wrap_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtextobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;31m# now rotate the positions around the first (x, y) position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mxys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset_layout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moffsetx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsety\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mxys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "decay_epsilon = 0\n",
    "alpha = 0.1\n",
    "nb_episodes = 50\n",
    "\n",
    "# Create the environment\n",
    "env = GridWorld(rewards=[100, -100, -1])\n",
    "\n",
    "# Create the agent\n",
    "agent = QLearningAgent(env, gamma, epsilon, decay_epsilon, alpha)\n",
    "\n",
    "# Train the agent for 200 episodes\n",
    "returns = agent.train(nb_episodes, render=True)\n",
    "\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Modify your agent so that it uses **softmax action selection**, with a temperature $\\tau = 1.0$. What does it change?\n",
    "\n",
    "If you have time, write a generic class for the Q-learning agent where you can select the action selection method flexibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \n",
    "    def __init__(self, env, gamma, action_selection, alpha):\n",
    "        # Store data\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.action_selection = action_selection\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Q_table\n",
    "        self.q_table = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        if self.action_selection['type'] == \"egreedy\":\n",
    "            # epsilon-greedy\n",
    "            if np.random.uniform(0, 1, 1) < self.action_selection['epsilon']:\n",
    "                action = self.env.action_space.sample() \n",
    "            else:\n",
    "                action = np.random.choice(np.where(self.q_table[state, :] == self.q_table[state, :].max())[0])\n",
    "        else: # softmax\n",
    "            logits = np.exp((self.q_table[state, :] - self.q_table[state, :].max())/self.action_selection['tau'])\n",
    "            probas = logits / np.sum(logits)\n",
    "            action = np.random.choice(range(4), p=probas) \n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Bellman target\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += self.gamma * self.q_table[next_state, :].max()\n",
    "        \n",
    "        # Update the Q-value\n",
    "        self.q_table[state, action] += self.alpha * (target - self.q_table[state, action])\n",
    "            \n",
    "        # Decay exploration parameters\n",
    "        if self.action_selection['type'] == \"egreedy\":\n",
    "            self.action_selection['epsilon'] = self.action_selection['epsilon'] * (1 - self.action_selection['decay'])\n",
    "        else:\n",
    "            self.action_selection['tau'] = self.action_selection['tau'] * (1 - self.action_selection['decay'])\n",
    "        \n",
    "    \n",
    "    def train(self, nb_episodes):\n",
    "        \n",
    "        # Returns\n",
    "        training_returns = []\n",
    "        test_returns = []\n",
    "\n",
    "        # Fixed number of episodes\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Reset\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            nb_steps = 0\n",
    "\n",
    "            # Store rewards\n",
    "            rewards = []\n",
    "\n",
    "            # Sample the episode\n",
    "            while not done:\n",
    "                \n",
    "                # Render\n",
    "                self.env.render(self.q_table)\n",
    "\n",
    "                # Select an action \n",
    "                action = self.act(state)\n",
    "\n",
    "                # Perform the action\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                # Store the reward\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # Update the Q-learning agent\n",
    "                self.update(state, action, reward, next_state, done)   \n",
    "\n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Increment time\n",
    "                nb_steps += 1\n",
    "                \n",
    "            if info: # GUI closed\n",
    "                print(\"Done\")\n",
    "                self.env.close()\n",
    "                break\n",
    " \n",
    "            # Store info\n",
    "            return_episode = self.discounted_return(rewards)\n",
    "            training_returns.append(return_episode)\n",
    "                \n",
    "        self.env.close()\n",
    "        return training_returns\n",
    "    \n",
    "    def discounted_return(self, rewards):\n",
    "        \"Computes the discounted return from the list of rewards\"\n",
    "        ret = 0.0\n",
    "        for reward in reversed(rewards):\n",
    "            ret = reward + self.gamma*ret\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "#action_selection  = {'type': \"egreedy\", \"epsilon\": 0.1, \"decay\": 0.0}\n",
    "action_selection  = {'type': \"softmax\", \"tau\": 1.0, \"decay\": 0.0}\n",
    "alpha = 0.1\n",
    "nb_episodes = 200\n",
    "\n",
    "# Create the environment\n",
    "env = Gridworld()\n",
    "\n",
    "# Create the agent\n",
    "agent = QLearningAgent(env, gamma, action_selection, alpha)\n",
    "\n",
    "# Train the agent for 200 episodes\n",
    "training_returns = agent.train(nb_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the returns\n",
    "plt.plot(training_returns, label=\"Training\")\n",
    "plt.plot(running_mean(training_returns, 10), label=\"Training (smoothed)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The agent explores much less at the end of training, as the difference between the Q-values becomes high enough to become greedy. there is no real need to decay tau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Eligibility traces\n",
    "\n",
    "The main drawback of Q-learning is that it needs many episodes to converge (**sample complexity**).\n",
    "\n",
    "One way to speed up learning is to use eligibility traces, one per state-action pair:\n",
    "\n",
    "```python\n",
    "traces = np.zeros((nb_states, nb_actions))\n",
    "```\n",
    "\n",
    "After each transition $(s_t, a_t)$, Q($\\lambda$) updates a **trace** $e(s_t, a_t)$ and modifies all Q-values as:\n",
    "\n",
    "1.  The trace of the last transition is incremented from 1:\n",
    "    \n",
    "$$e(s_t, a_t) = e(s_t, a_t) +1$$\n",
    "    \n",
    "2. Q($\\lambda$)-learning is applied on **ALL** Q-values, using the TD error at time $t$:\n",
    "    \n",
    "$$Q(s, a) = Q(s, a) + \\alpha \\, (r_{t+1} + \\gamma \\, \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)) \\, e(s, a)$$\n",
    "    \n",
    "3. All traces are exponentially decreased using the trace parameter $\\lambda$ (e.g. 0.7):\n",
    "\n",
    "$$\n",
    "e(s, a) = \\lambda \\, \\gamma \\, e(s, a)\n",
    "$$\n",
    "\n",
    "All traces are reset to 0 at the beginning of an episode.\n",
    "\n",
    "**Q9:** Implement eligibility traces in your Q($\\lambda$)-learning agent and see if it improves convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLambdaLearningAgent:\n",
    "    \n",
    "    def __init__(self, env, gamma, lbda, action_selection, alpha):\n",
    "        # Store data\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lbda = lbda\n",
    "        self.action_selection = action_selection\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Q_table\n",
    "        self.q_table = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        self.traces = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        if self.action_selection['type'] == \"egreedy\":\n",
    "            # epsilon-greedy\n",
    "            if np.random.uniform(0, 1, 1) < self.action_selection['epsilon']:\n",
    "                action = self.env.action_space.sample() \n",
    "            else:\n",
    "                action = np.random.choice(np.where(self.q_table[state, :] == self.q_table[state, :].max())[0])\n",
    "        else: # softmax\n",
    "            logits = np.exp((self.q_table[state, :] - self.q_table[state, :].max())/self.action_selection['tau'])\n",
    "            probas = logits / np.sum(logits)\n",
    "            action = np.random.choice(range(4), p=probas) \n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Bellman target\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += self.gamma * self.q_table[next_state, :].max()\n",
    "        \n",
    "        # Update ALL Q-values\n",
    "        self.q_table += self.alpha * (target - self.q_table[state, action]) * self.traces\n",
    "            \n",
    "        # Decay exploration parameters\n",
    "        if self.action_selection['type'] == \"egreedy\":\n",
    "            self.action_selection['epsilon'] = self.action_selection['epsilon'] * (1 - self.action_selection['decay'])\n",
    "        else:\n",
    "            self.action_selection['tau'] = self.action_selection['tau'] * (1 - self.action_selection['decay'])\n",
    "        \n",
    "    \n",
    "    def train(self, nb_episodes):\n",
    "        \n",
    "        # Returns\n",
    "        training_returns = []\n",
    "        test_returns = []\n",
    "\n",
    "        # Fixed number of episodes\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Reset\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            nb_steps = 0\n",
    "            \n",
    "            # Reset traces\n",
    "            self.traces *= 0\n",
    "\n",
    "            # Store rewards\n",
    "            rewards = []\n",
    "\n",
    "            # Sample the episode\n",
    "            while not done:\n",
    "                \n",
    "                # Render\n",
    "                self.env.render(self.q_table)\n",
    "\n",
    "                # Select an action \n",
    "                action = self.act(state)\n",
    "\n",
    "                # Perform the action\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                # Store the reward\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # Increment trace\n",
    "                self.traces[state, action] += 1\n",
    "                \n",
    "                # Update the Q-learning agent\n",
    "                self.update(state, action, reward, next_state, done)   \n",
    "                \n",
    "                # Update all traces\n",
    "                self.traces *= self.gamma * self.lbda\n",
    "\n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Increment time\n",
    "                nb_steps += 1\n",
    "                \n",
    "            if info: # GUI closed\n",
    "                print(\"Done\")\n",
    "                self.env.close()\n",
    "                break\n",
    " \n",
    "            # Store info\n",
    "            return_episode = self.discounted_return(rewards)\n",
    "            training_returns.append(return_episode)\n",
    "                \n",
    "        self.env.close()\n",
    "        return training_returns\n",
    "    \n",
    "    def discounted_return(self, rewards):\n",
    "        \"Computes the discounted return from the list of rewards\"\n",
    "        ret = 0.0\n",
    "        for reward in reversed(rewards):\n",
    "            ret = reward + self.gamma*ret\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "lbda = 0.7\n",
    "#action_selection  = {'type': \"egreedy\", \"epsilon\": 0.1, \"decay\": 0.0}\n",
    "action_selection  = {'type': \"softmax\", \"tau\": 1.0, \"decay\": 0.0}\n",
    "alpha = 0.1\n",
    "nb_episodes = 200\n",
    "\n",
    "# Create the environment\n",
    "from Gridworld import Gridworld\n",
    "env = Gridworld(size=(5, 5), rewards=[100, -100, -1])\n",
    "\n",
    "# Create the agent\n",
    "agent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n",
    "\n",
    "# Train the agent for 200 episodes\n",
    "training_returns = agent.train(nb_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the returns\n",
    "plt.plot(training_returns, label=\"Training\")\n",
    "plt.plot(running_mean(training_returns, 10), label=\"Training (smoothed)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Vary the decay parameter $\\lambda$ and discuss its influence.\n",
    "\n",
    "**Q:** Increase the size of Gridworld to 10x10 and observe how long it takes to learn the optimal strategy.\n",
    "\n",
    "```python\n",
    "size = (10, 10)\n",
    "```\n",
    "\n",
    "*Note:* do not change it to anything else than square (i.e. not (10, 5)). There is a bug... If you find it and report it, you win a cookie.\n",
    "\n",
    "Comment on the **curse of dimensionality** and the interest of tabular RL for complex tasks with large state spaces and sparse rewards (e.g. robotics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "lbda = 0.7\n",
    "#action_selection  = {'type': \"egreedy\", \"epsilon\": 0.1, \"decay\": 0.0}\n",
    "action_selection  = {'type': \"softmax\", \"tau\": 1.0, \"decay\": 0.0}\n",
    "alpha = 0.1\n",
    "nb_episodes = 200\n",
    "\n",
    "# Create the environment\n",
    "from Gridworld import Gridworld\n",
    "env = Gridworld(size=(10, 10), rewards=[100, -100, -1])\n",
    "\n",
    "# Create the agent\n",
    "agent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n",
    "\n",
    "# Train the agent for 200 episodes\n",
    "training_returns = agent.train(nb_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
