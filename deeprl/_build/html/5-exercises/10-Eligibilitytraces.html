
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>10.1. Eligibility traces &#8212; Deep Reinforcement Learning</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-deeprl/5-exercises/10-Eligibilitytraces.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10.2. Eligibility traces" href="10-Eligibilitytraces-solution.html" />
    <link rel="prev" title="10. Eligibility traces" href="ex10-Eligibilitytraces.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-deeprl/5-exercises/10-Eligibilitytraces.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Eligibility traces" />
<meta property="og:description" content="Eligibility traces  import numpy as np import matplotlib.pyplot as plt import gym rng = np.random.default_rng() import time from IPython.display import clear_ou" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-deeprl/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Deep Reinforcement Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Deep Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Tabular RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/1-Bandits.html">
   1. Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/2-MDP.html">
   2. Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/3-DP.html">
   3. Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/4-MC.html">
   4. Monte-Carlo (MC) methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/5-TD.html">
   5. Temporal Difference learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/6-FA.html">
   6. Function approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/7-NN.html">
   7. Deep learning
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Model-free RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/1-DQN.html">
   1. Deep Q-Learning (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/2-BeyondDQN.html">
   2. Beyond DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/3-PG.html">
   3. Policy gradient (PG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/4-A3C.html">
   4. Advantage actor-critic (A2C, A3C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/5-DDPG.html">
   5. Deep Deterministic Policy Gradient (DDPG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/6-PPO.html">
   6. Natural gradients (TRPO, PPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/7-SAC.html">
   7. Maximum Entropy RL (SAC)
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Model-based RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-MB/1-MB.html">
   1. Model-based RL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-MB/2-LearnedModels.html">
   2. Learned world models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-MB/3-AlphaGo.html">
   3. AlphaGo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-MB/4-SR.html">
   4. Successor representations
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex1-Python.html">
   1. Introduction to Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex3-Sampling.html">
   3. Sampling
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="3-Sampling.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3-Sampling-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex4-Bandits.html">
   4. Bandits (part 1)
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="4-Bandits.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4-Bandits-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex5-Bandits2.html">
   5. Bandits (part 2)
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="5-Bandits2.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-Bandits2-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex6-DP.html">
   6. Dynamic programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="6-DP.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6-DP-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex7-Gym.html">
   7. Gym environments
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="7-Gym.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="7-Gym-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex8-MC.html">
   8. Monte-Carlo control
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="8-MonteCarlo.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8-MonteCarlo-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex9-TD.html">
   9. Q-learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="9-TD.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9-TD-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="ex10-Eligibilitytraces.html">
   10. Eligibility traces
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="10-Eligibilitytraces-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex11-Keras.html">
   11. Keras
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="11-Keras.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="11-Keras-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="ex12-DQN.html">
   12. DQN
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="12-DQN.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12-DQN-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/5-exercises/10-Eligibilitytraces.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/vitay/lecturenotes-deeprl/master?urlpath=tree/deeprl/5-exercises/10-Eligibilitytraces.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/vitay/lecturenotes-deeprl/blob/master/deeprl/5-exercises/10-Eligibilitytraces.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#q-learning-in-gridworld">
   10.1.1. Q-learning in Gridworld
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-interaction-with-the-environment">
     10.1.1.1. Random interaction with the environment
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   10.1.2. Eligibility traces
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="eligibility-traces">
<h1><span class="section-number">10.1. </span>Eligibility traces<a class="headerlink" href="#eligibility-traces" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>

<span class="k">def</span> <span class="nf">running_average</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="n">cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> 
    <span class="k">return</span> <span class="p">(</span><span class="n">cumsum</span><span class="p">[</span><span class="n">N</span><span class="p">:]</span> <span class="o">-</span> <span class="n">cumsum</span><span class="p">[:</span><span class="o">-</span><span class="n">N</span><span class="p">])</span> <span class="o">/</span> <span class="n">N</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="q-learning-in-gridworld">
<h2><span class="section-number">10.1.1. </span>Q-learning in Gridworld<a class="headerlink" href="#q-learning-in-gridworld" title="Permalink to this headline">¶</a></h2>
<div class="section" id="random-interaction-with-the-environment">
<h3><span class="section-number">10.1.1.1. </span>Random interaction with the environment<a class="headerlink" href="#random-interaction-with-the-environment" title="Permalink to this headline">¶</a></h3>
<p>The goal of this exercise is to solve the <strong>Gridworld</strong> problem using Q-learning. The code is adapted from  <a class="reference external" href="https://github.com/rlcode/reinforcement-learning-kr">https://github.com/rlcode/reinforcement-learning-kr</a></p>
<p>The agent is represented by the red square: the <strong>state</strong> <span class="math notranslate nohighlight">\(s\)</span> of the agent is its position in the 5x5 grid, i.e. a number between 0 and 24.</p>
<p>The agent can move either to the left, right, top or bottom. When the agent tries to move outside of the environment, it stays at its current position. There are four <strong>actions</strong> <span class="math notranslate nohighlight">\(a\)</span> available, which are deterministic.</p>
<p>Its goal is to reach the blue circle, while avoiding the green triangles. Actions leading to the blue circle receive a reward <span class="math notranslate nohighlight">\(r\)</span> of +100, actions leading to a green triangle receive a reward of -100. The episode ends in those states. All other actions have a reward of -1.</p>
<p>The following code allows you to run a <strong>random agent</strong> in this environment for 10 episodes: at each time step, the action is selected randomly between 0 and 3 with <code class="docutils literal notranslate"><span class="pre">env.action_space.sample()</span></code>.</p>
<p>The environment is created with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">Gridworld</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">env.render()</span></code> allows to display the Gridworld. If you pass it a Q-table (one row per state, one column per action) <code class="docutils literal notranslate"><span class="pre">env.render(Q)</span></code>, it will print the Q_value of each action at the corresponding location.</p>
<p><code class="docutils literal notranslate"><span class="pre">state</span> <span class="pre">=</span> <span class="pre">env.reset()</span></code> allows to start an episode. The agent is placed at the top-left of the grid.</p>
<p><code class="docutils literal notranslate"><span class="pre">next_state,</span> <span class="pre">reward,</span> <span class="pre">done,</span> <span class="pre">info</span> <span class="pre">=</span> <span class="pre">env.step(action)</span></code> allows to perform an action on the environment. <code class="docutils literal notranslate"><span class="pre">action</span></code> must be a number between 0 and 3. It return the next state, the reward received during the transition, as well as a boolean <code class="docutils literal notranslate"><span class="pre">done</span></code> which is <code class="docutils literal notranslate"><span class="pre">True</span></code> when the episode is terminated (the agent moved to a blue circle or a green triangle, or 100 steps have been done), <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise. <code class="docutils literal notranslate"><span class="pre">info</span></code> contains the current undiscounted return of the episode.</p>
<p><strong>Q:</strong> Understand and run the code. Does the agent succeed often? How complex is the task compared to Taxi?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GridWorld</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rewards</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="s2">&quot;Initialize the environment, can accept additional parameters such as the number of states and actions.&quot;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span>
        
        <span class="c1"># State space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Action space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>    
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_max</span> <span class="o">=</span> <span class="mi">100</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distractor1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distractor2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Resets the environment and starts from an initial state.&quot;</span>
        
        <span class="c1"># Sample one state randomly </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_t</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum_rewards</span> <span class="o">=</span> <span class="mf">0.0</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Takes an action and returns a new state, a reward, a boolean (True for terminal states) </span>
<span class="sd">        and a dictionary with additional info (optional).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_t</span> <span class="o">+=</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coordinates</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># up</span>
            <span class="n">y</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># down</span>
            <span class="n">y</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span> <span class="c1"># left</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span> <span class="c1"># right</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distractor1</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distractor2</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_t</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_max</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">True</span>
            
        <span class="bp">self</span><span class="o">.</span><span class="n">sum_rewards</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;return&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum_rewards</span><span class="p">}</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">done</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">info</span>

    <span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">):</span>
        <span class="s2">&quot;Displays the current state of the environment.&quot;</span>
        
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
        
        <span class="c1"># grid</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">],</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
            
        <span class="c1"># Q-values</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
            <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coordinates</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.9</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
            
        <span class="c1"># target</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">distractor1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">distractor1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;^&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">distractor2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">distractor2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;^&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">)</span>
        
        <span class="c1"># state</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coordinates</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Step &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_t</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; - undiscounted return &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sum_rewards</span><span class="p">))</span>
        
        <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">coordinates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="s2">&quot;Returns coordinates of a state.&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">state</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">state</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">rank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coord</span><span class="p">):</span>
        <span class="s2">&quot;Returns rank from coordinates.&quot;</span>
        <span class="k">return</span> <span class="n">coord</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="o">*</span><span class="n">coord</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="s2">&quot;Selects an action randomly&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_episodes</span><span class="p">,</span> <span class="n">render</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="s2">&quot;Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.&quot;</span>

        <span class="c1"># Returns</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Fixed number of episodes</span>
        <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_episodes</span><span class="p">):</span>

            <span class="c1"># Reset</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Store rewards</span>
            <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># Sample the episode</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                
                <span class="c1"># Render the current state</span>
                <span class="k">if</span> <span class="n">render</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">)</span>
                    
                <span class="c1"># Select an action </span>
                <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

                <span class="c1"># Perform the action</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
                
                <span class="c1"># Append reward</span>
                <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

                <span class="c1"># Go in the next state</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

                <span class="c1"># Increment time</span>
                <span class="n">nb_steps</span> <span class="o">+=</span> <span class="mi">1</span>
                
                <span class="k">if</span> <span class="n">done</span><span class="p">:</span> 
                    <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">)</span>

            <span class="c1"># Compute the discounted return of the episode.</span>
            <span class="n">return_episode</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>    

            <span class="c1"># Store info</span>
            <span class="n">returns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">return_episode</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">returns</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">GridWorld</span><span class="p">()</span>

<span class="c1"># Create the agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1"># Perform random episodes</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">render</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
<span class="nb">print</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/10-Eligibilitytraces_6_0.png" src="../_images/10-Eligibilitytraces_6_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-112, -142]
</pre></div>
</div>
</div>
</div>
<p><strong>Q:</strong> Adapt your Q-learning agent from last exercise to the problem. The main difference is the call to <code class="docutils literal notranslate"><span class="pre">render()</span></code>, the rest is similar. Train it for 100 episodes with the right hyperparameters and without rendering.</p>
<p><strong>Q:</strong> Train a Q-learning agent with rendering on. Observe in particular which Q-values are updates when the agent reaches the target. Is it efficient?</p>
<p><strong>Q:</strong> Modify your agent so that it uses <strong>softmax action selection</strong>, with a temperature <span class="math notranslate nohighlight">\(\tau = 1.0\)</span> and a suitable decay. What does it change?</p>
<p>If you have time, write a generic class for the Q-learning agent where you can select the action selection method flexibly.</p>
</div>
</div>
<div class="section" id="id1">
<h2><span class="section-number">10.1.2. </span>Eligibility traces<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>The main drawback of Q-learning is that it needs many episodes to converge (<strong>sample complexity</strong>).</p>
<p>One way to speed up learning is to use eligibility traces, one per state-action pair:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">traces</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_states</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">))</span>
</pre></div>
</div>
<p>After each transition <span class="math notranslate nohighlight">\((s_t, a_t)\)</span>, Q(<span class="math notranslate nohighlight">\(\lambda\)</span>) updates a <strong>trace</strong> <span class="math notranslate nohighlight">\(e(s_t, a_t)\)</span> and modifies all Q-values as:</p>
<ol class="simple">
<li><p>The trace of the last transition is incremented from 1:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[e(s_t, a_t) = e(s_t, a_t) +1\]</div>
<ol class="simple">
<li><p>Q(<span class="math notranslate nohighlight">\(\lambda\)</span>)-learning is applied on <strong>ALL</strong> Q-values, using the TD error at time <span class="math notranslate nohighlight">\(t\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[Q(s, a) = Q(s, a) + \alpha \, (r_{t+1} + \gamma \, \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)) \, e(s, a)\]</div>
<ol class="simple">
<li><p>All traces are exponentially decreased using the trace parameter <span class="math notranslate nohighlight">\(\lambda\)</span> (e.g. 0.7):</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
e(s, a) = \lambda \, \gamma \, e(s, a)
\]</div>
<p>All traces are reset to 0 at the beginning of an episode.</p>
<p><strong>Q:</strong> Implement eligibility traces in your Q(<span class="math notranslate nohighlight">\(\lambda\)</span>)-learning agent and see if it improves convergence. Train it with rendering on and observe how all Q-values are updated.</p>
<p><strong>Q:</strong> Vary the trace parameter <span class="math notranslate nohighlight">\(\lambda\)</span> and discuss its influence.</p>
<p><strong>Q:</strong> Increase the size of Gridworld to 10x10 and observe how long it takes to learn the optimal strategy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">GridWorld</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>Comment on the <strong>curse of dimensionality</strong> and the interest of tabular RL for complex tasks with large state spaces and sparse rewards (e.g. robotics).</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./5-exercises"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="ex10-Eligibilitytraces.html" title="previous page"><span class="section-number">10. </span>Eligibility traces</a>
    <a class='right-next' id="next-link" href="10-Eligibilitytraces-solution.html" title="next page"><span class="section-number">10.2. </span>Eligibility traces</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>