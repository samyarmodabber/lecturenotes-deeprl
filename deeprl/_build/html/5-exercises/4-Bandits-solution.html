

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bandits &#8212; Deep Reinforcement Learning</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-deeprl/5-exercises/4-Bandits-solution.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-deeprl/5-exercises/4-Bandits-solution.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Bandits" />
<meta property="og:description" content="Bandits  In this part, we will investigate the properties of the action selection schemes seen in the lecture and compare their properties:  greedy action selec" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-deeprl/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Deep Reinforcement Learning</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Deep Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ex1-Python.html">
   1. Introduction to Python
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/5-exercises/4-Bandits-solution.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/vitay/lecturenotes-deeprl/master?urlpath=tree/deeprl/5-exercises/4-Bandits-solution.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/vitay/lecturenotes-deeprl/blob/master/deeprl/5-exercises/4-Bandits-solution.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#greedy-action-selection">
   Greedy action selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#epsilon-greedy-action-selection">
   -
   <span class="math notranslate nohighlight">
    \(\epsilon\)
   </span>
   -greedy action selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#softmax-action-selection">
   Softmax action selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reward-distribution">
   Reward distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimistic-initialization">
   Optimistic initialization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reinforcement-comparison">
   Reinforcement comparison
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bandits">
<h1>Bandits<a class="headerlink" href="#bandits" title="Permalink to this headline">¶</a></h1>
<p>In this part, we will investigate the properties of the action selection schemes seen in the lecture and compare their properties:</p>
<ol class="simple">
<li><p>greedy action selection</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection</p></li>
<li><p>softmax action selection</p></li>
<li><p>reinforcement comparison</p></li>
</ol>
<p>Let’s do some imports:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Bandit</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_arms</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">var_Q</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">var_r</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_arms</span> <span class="o">=</span> <span class="n">nb_arms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_Q</span> <span class="o">=</span> <span class="n">var_Q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_r</span> <span class="o">=</span> <span class="n">var_r</span>
        
        <span class="c1"># Initialize the true Q-values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_Q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nb_arms</span><span class="p">)</span>
        
        <span class="c1"># Optimal action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a_star</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_star</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_star</span><span class="p">[</span><span class="n">action</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_r</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        
<span class="n">nb_actions</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bandit</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
    <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">bandit</span><span class="o">.</span><span class="n">Q_star</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q^*(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">means</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q_t(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_6_0.png" src="../_images/4-Bandits-solution_6_0.png" />
</div>
</div>
<div class="section" id="greedy-action-selection">
<h2>Greedy action selection<a class="headerlink" href="#greedy-action-selection" title="Permalink to this headline">¶</a></h2>
<p>In <strong>greedy action selection</strong>, we systematically chose the action with the highest estimated Q-value at each play (or randomly when there are ties):</p>
<div class="math notranslate nohighlight">
\[a_t = \text{argmax}_a Q_t(a)\]</div>
<p>We maintain estimates <span class="math notranslate nohighlight">\(Q_t\)</span> of the action values (initialized to 0) using the online formula:</p>
<div class="math notranslate nohighlight">
\[Q_{t+1}(a_t) = Q_t(a_t) + \alpha \, (r_{t} - Q_t(a_t))\]</div>
<p>when receiving the sampled reward <span class="math notranslate nohighlight">\(r_t\)</span> after taking the action <span class="math notranslate nohighlight">\(a_t\)</span>. The learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> can be set to 0.1 at first.</p>
<p>The algorithm simply alternates between these two steps for 1000 plays (or steps): take an action, update its Q-value.</p>
<p><strong>Q:</strong> Implement the greedy algorithm on the 5-armed bandit.</p>
<p>Your algorithm will look like this:</p>
<ul class="simple">
<li><p>Create a 5-armed bandit (mean of zero, variance of 1).</p></li>
<li><p>Initialize the estimated Q_values to 0 with an array of the same size (call it <code class="docutils literal notranslate"><span class="pre">Q_greedy</span></code> or something like that, not <code class="docutils literal notranslate"><span class="pre">Q_t</span></code>, as we will later make comparisons between the different algorithms).</p></li>
<li><p><strong>for</strong> 1000 plays:</p>
<ul>
<li><p>Select the greedy action <span class="math notranslate nohighlight">\(a_t^*\)</span> using the current estimates.</p></li>
<li><p>Sample a reward from <span class="math notranslate nohighlight">\(Q^*(a_t^*)\)</span> using the method <code class="docutils literal notranslate"><span class="pre">get_reward()</span></code> (variance of 1).</p></li>
<li><p>Update the estimated Q-value of the action taken.</p></li>
</ul>
</li>
</ul>
<p>Additionally, you will store the received rewards at each step in an initially empty list or a numpy array of the correct size and plot it in the end. You will also plot the true Q-values and the estimated Q-values at the end of the 1000 plays.</p>
<p><em>Tip:</em> to implement the argmax, do not rely on <code class="docutils literal notranslate"><span class="pre">np.argmax()</span></code>. If there are ties in the array, for example at the beginning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">x.argmax()</span></code> will return you the <strong>first occurrence</strong> of the maximum 0.0 of the array. In this case it will be the index 0, so you will always select the action 0 first.</p>
<p>It is much more efficient to retrieve the indices of <strong>all</strong> maxima and randomly select one of them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">np.where(x</span> <span class="pre">==</span> <span class="pre">x.max())</span></code> returns a list of indices where <code class="docutils literal notranslate"><span class="pre">x</span></code> is maximum. <code class="docutils literal notranslate"><span class="pre">rng.choice()</span></code> randomly selects one of them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Setup </span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Bandit</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

<span class="c1"># Estimates</span>
<span class="n">Q_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

<span class="c1"># Store the rewards after each step</span>
<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">)</span>

<span class="c1"># For 1000 plays</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>
    
    <span class="c1"># Select the action greedily w.r.t Q_t</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_greedy</span> <span class="o">==</span> <span class="n">Q_greedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Sample the reward</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">bandit</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    
    <span class="c1"># Store the received reward</span>
    <span class="n">rewards_greedy</span><span class="p">[</span><span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
    
    <span class="c1"># Update the Q-value estimate of the action</span>
    <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
    
<span class="c1"># Plot the Q-values and the evolution of rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">bandit</span><span class="o">.</span><span class="n">Q_star</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q^*(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">Q_greedy</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q_t(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_8_0.png" src="../_images/4-Bandits-solution_8_0.png" />
</div>
</div>
<p><strong>Q:</strong> Re-run your algorithm multiple times with different values of <code class="docutils literal notranslate"><span class="pre">Q^*</span></code> and observe:</p>
<ol class="simple">
<li><p>How much reward you get.</p></li>
<li><p>How your estimated Q-values in the end differ from the true Q-values.</p></li>
<li><p>Whether greedy action action selection finds the optimal action or not.</p></li>
</ol>
<p><strong>A:</strong> The plot with rewards is very noisy, you do not really see whether you have learned something because of the randomness of the rewards. More often than not, greedy action selection finds the optimal action, or least a not-that-bad action. The estimates <code class="docutils literal notranslate"><span class="pre">Q_t</span></code> have however nothing to see with the true Q-values, as you quickly select the same action and never update the other ones.</p>
<p>The evolution of the received rewards does not give a clear indication of the successful learning. To truly estimate the correctness of the algorithm, we have to average these received results ober many runs, e.g. 200.</p>
<p><strong>Q:</strong> Modify the previous cell to average the results over 200 runs. The true Q-values should be sampled every time. Store each received reward in a big <code class="docutils literal notranslate"><span class="pre">(200,</span> <span class="pre">1000)</span></code> matrix and plot the average received reward in the end.</p>
<p>Another important information, besides the mean received reward, is how often the selected action is the optimal action (which you can obtain with <code class="docutils literal notranslate"><span class="pre">a_star</span> <span class="pre">=</span> <span class="pre">np.random.choice(np.where(Q_star</span> <span class="pre">==</span> <span class="pre">Q_star.max())[0])</span></code>). Store this information in another big matrix (1.0 if the selected action is the optimal action, 0.0 otherwise) and plot the average evolution of the optimality.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># Select the action greedily w.r.t Q_t</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_greedy</span> <span class="o">==</span> <span class="n">Q_greedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="c1"># Sample the reward</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        
        <span class="c1"># Store the received reward</span>
        <span class="n">rewards_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        
        <span class="c1"># Optimal action?</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        
        <span class="c1"># Update the Q-value estimate of the action</span>
        <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_12_0.png" src="../_images/4-Bandits-solution_12_0.png" />
</div>
</div>
<p><strong>A:</strong> greedy action selection ends up selecting the optimal action 80% of the time, what is really not bad knowing that it starts at chance level (20% for 5 actions).</p>
</div>
<div class="section" id="epsilon-greedy-action-selection">
<h2>- <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection<a class="headerlink" href="#epsilon-greedy-action-selection" title="Permalink to this headline">¶</a></h2>
<p>The main drawback of greedy action selection is that it does not explore: as soon as it finds an action better than the others (with a sufficiently positive true Q-value, i.e. where the sampled rewards are mostly positive), it will keep selecting that action and avoid exploring the other options.</p>
<p>The estimated Q-value of the selected action will end up being quite correct, but those of the other actions will stay at 0.</p>
<p>In <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection, the greedy action <span class="math notranslate nohighlight">\(a_t^*\)</span> (with the highest estimated Q-value) will be selected with a probability <span class="math notranslate nohighlight">\(1-\epsilon\)</span>, the others with a probability of <span class="math notranslate nohighlight">\(\epsilon\)</span> altogether.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \pi(a) = \begin{cases} 1 - \epsilon \; \text{if} \; a = a_t^* \\ \frac{\epsilon}{|\mathcal{A}| - 1} \; \text{otherwise.} \end{cases}
\end{split}\]</div>
<p>If you have <span class="math notranslate nohighlight">\(|\mathcal{A}| = 5\)</span> actions, the four non-greedy actions will be selected with a probability of <span class="math notranslate nohighlight">\(\frac{\epsilon}{4}\)</span>.</p>
<p><strong>Q:</strong> Modify your code to implement <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection (with <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span> at first). Do not overwrite the arrays previously calculated (mean reward and optimal actions), as you will want to compare the two methods in a single plot.</p>
<p>To implement <span class="math notranslate nohighlight">\(\epsilon-\)</span>greedy, you need to:</p>
<ol class="simple">
<li><p>Select the greedy action <span class="math notranslate nohighlight">\(a = a^*_t\)</span>.</p></li>
<li><p>Draw a random number between 0 and 1 (<code class="docutils literal notranslate"><span class="pre">rng.random()</span></code>).</p></li>
<li><p>If this number is smaller than <span class="math notranslate nohighlight">\(\epsilon\)</span>, you need to select another action randomly in the remaining ones (<code class="docutils literal notranslate"><span class="pre">rng.choice()</span></code>).</p></li>
<li><p>Otherwise, keep the greedy action.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Exploration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># Select the action greedily w.r.t Q_t</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_egreedy</span> <span class="o">==</span> <span class="n">Q_egreedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">[</span><span class="n">actions</span> <span class="o">!=</span><span class="n">a</span><span class="p">])</span>
            
        <span class="c1"># Sample the reward</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        
        <span class="c1"># Store the received reward</span>
        <span class="n">rewards_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        
        <span class="c1"># Optimal action?</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        
        <span class="c1"># Update the Q-value estimate of the action</span>
        <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_15_0.png" src="../_images/4-Bandits-solution_15_0.png" />
</div>
</div>
<p><strong>Q:</strong> Compare the properties of greedy and <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy (speed, optimality, etc). Vary the value of the parameter <span class="math notranslate nohighlight">\(\epsilon\)</span> (0.0001 until 0.5) and conclude.</p>
<p><strong>A:</strong> Depending on the value of <span class="math notranslate nohighlight">\(\epsilon\)</span>, <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy can perform better that greedy in the end, but will necessitate more time at the beginning. If there is too much exploration, <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy can be even worse than greedy.</p>
</div>
<div class="section" id="softmax-action-selection">
<h2>Softmax action selection<a class="headerlink" href="#softmax-action-selection" title="Permalink to this headline">¶</a></h2>
<p>To avoid exploring actions which are clearly not optimal, another useful algorithm is <strong>softmax action selection</strong>. In this scheme, the estimated Q-values are ransformed into a probability distribution using the softmax opertion:</p>
<div class="math notranslate nohighlight">
\[
    \pi(a) = \frac{\exp \frac{Q_t(a)}{\tau}}{ \sum_b \exp \frac{Q_t(b)}{\tau}}
\]</div>
<p>For each action, the term <span class="math notranslate nohighlight">\(\exp \frac{Q_t(a)}{\tau}\)</span> is proportional to <span class="math notranslate nohighlight">\(Q_t(a)\)</span> but made positive. These terms are then normalized by the denominator in order to obtain a sum of 1, i.e. they are the parameters of a discrete probability distribution. The temperature <span class="math notranslate nohighlight">\(\tau\)</span> controls the level of exploration just as <span class="math notranslate nohighlight">\(\epsilon\)</span> for <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy.</p>
<p>In practice, <span class="math notranslate nohighlight">\(\exp \frac{Q_t(a)}{\tau}\)</span> can be very huge if the Q-values are high or the temperature is small, creating numerical instability (NaN). It is much more stable to substract the maximal Q-value from all Q-values before applying the softmax:</p>
<div class="math notranslate nohighlight">
\[
    \pi(a) = \frac{\exp \displaystyle\frac{Q_t(a) - \max_a Q_t(a)}{\tau}}{ \sum_a \exp \displaystyle\frac{Q_t(a) - \max_a Q_t(a)}{\tau}}
\]</div>
<p>This way, <span class="math notranslate nohighlight">\(Q_t(a) - \max_a Q_t(a)\)</span> is always negative, so its exponential is between 0 and 1.</p>
<p><strong>Q:</strong> Implement the softmax action selection (with <span class="math notranslate nohighlight">\(\tau=0.5\)</span> at first) and compare its performance to greedy and <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy. Vary the temperature <span class="math notranslate nohighlight">\(\tau\)</span> and find the best possible value. Conclude.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Exploration</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># Select the action with softmax            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">Q_softmax</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
            
        <span class="c1"># Sample the reward</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        
        <span class="c1"># Store the received reward</span>
        <span class="n">rewards_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        
        <span class="c1"># Optimal action?</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        
        <span class="c1"># Update the Q-value estimate of the action</span>
        <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_19_0.png" src="../_images/4-Bandits-solution_19_0.png" />
</div>
</div>
<p><strong>A:</strong> softmax loses less time than <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy exploring the really bad solutions, so it is optimal earlier. It can be more efficient and optimal than the other methods, but finding the right value for <span class="math notranslate nohighlight">\(\tau\)</span> (0.1 works well) is difficult: its optimum value depends on the scaling of Q, you cannot know it in advance…</p>
<p>The problem with this version of softmax (with a constant temperature) is that even after it has found the optimal action, it will still explore the other ones (although more rarely than at the beginning). The solution is to <strong>schedule</strong> the exploration parameter so that it explores a lot at the beginning (high temperature) and gradually switches to more exploitation (low temperature).</p>
<p>Many schemes are possible for that, the simplest one being to linearly decrease the temperature so that it starts with a value <span class="math notranslate nohighlight">\(\tau_\text{init}\)</span> and reaches <span class="math notranslate nohighlight">\(\tau_\text{final}\)</span> at the end of the 1000 plays:</p>
<div class="math notranslate nohighlight">
\[\tau = \tau_\text{init} + (\tau_\text{final} - \tau_\text{init}) \, (\frac{t}{1000})\]</div>
<p><strong>Q:</strong> Implement temperature scheduling for the softmax algorithm (<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy would be similar) with <span class="math notranslate nohighlight">\(\tau_\text{init}=0.3\)</span> and <span class="math notranslate nohighlight">\(\tau_\text{init} = 0.001\)</span>. Use different initial and final values. Conclude.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Exploration</span>
<span class="n">tau_init</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">tau_final</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_softmax_schedule</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_softmax_schedule</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_softmax_schedule</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># Scheduling of tau</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="n">tau_init</span> <span class="o">+</span> <span class="p">(</span><span class="n">tau_final</span> <span class="o">-</span> <span class="n">tau_init</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">step</span><span class="o">/</span><span class="n">nb_steps</span><span class="p">)</span>
        
        <span class="c1"># Select the action with softmax            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">Q_softmax_schedule</span> <span class="o">-</span> <span class="n">Q_softmax_schedule</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
            
        <span class="c1"># Sample the reward</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        
        <span class="c1"># Store the received reward</span>
        <span class="n">rewards_softmax_schedule</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        
        <span class="c1"># Optimal action?</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_softmax_schedule</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        
        <span class="c1"># Update the Q-value estimate of the action</span>
        <span class="n">Q_softmax_schedule</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_softmax_schedule</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax_schedule</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax (scheduled)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_softmax_schedule</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax (scheduled)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_22_0.png" src="../_images/4-Bandits-solution_22_0.png" />
</div>
</div>
</div>
<div class="section" id="reward-distribution">
<h2>Reward distribution<a class="headerlink" href="#reward-distribution" title="Permalink to this headline">¶</a></h2>
<p>We are now going to vary the reward distributions and investigate whether the experimental results we had previously when the true Q-values are in <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> and the rewards have a variance of 1 still hold.</p>
<p><strong>Q:</strong> If it is not the case already, create a new cell allowing to perform greedy, <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy and softmax action selection (1000 steps, 200 runs) at the same time. Use the best values you have found so far for <span class="math notranslate nohighlight">\(\epsilon\)</span> and <span class="math notranslate nohighlight">\(\tau\)</span> (no scheduling).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Exploration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_greedy</span> <span class="o">==</span> <span class="n">Q_greedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># epsilon-greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_egreedy</span> <span class="o">==</span> <span class="n">Q_egreedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">[</span><span class="n">actions</span> <span class="o">!=</span><span class="n">a</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># Softmax            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">Q_softmax</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_24_0.png" src="../_images/4-Bandits-solution_24_0.png" />
</div>
</div>
<p><strong>Q:</strong> Let’s now change the distribution of true Q-values from <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{N}(10, 10)\)</span> and re-run the algorithms. What happens and why? Modify the values of <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> and <code class="docutils literal notranslate"><span class="pre">tau</span></code> to try to get a better behavior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Exploration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">3.0</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_greedy</span> <span class="o">==</span> <span class="n">Q_greedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># epsilon-greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_egreedy</span> <span class="o">==</span> <span class="n">Q_egreedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">[</span><span class="n">actions</span> <span class="o">!=</span><span class="n">a</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># Softmax            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">Q_softmax</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_26_0.png" src="../_images/4-Bandits-solution_26_0.png" />
</div>
</div>
<p><strong>A:</strong> Greedy does not work anymore and stays at chance level. The first action it samples will probably have a non-zero reward, so its estimated Q-value becomes positive (initial estimate of 0) and it will stay the greedy action all along.</p>
<p><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy still works quite well (perhaps a bit slower as the estimates must go from 0 to 10), even with its default value of 0.1.</p>
<p>Softmax does not work unless you increase the temperature to 3 or so. The correct value of <code class="docutils literal notranslate"><span class="pre">tau</span></code> depends on the scaling of the Q-values, so it has to be adapted to every new problem, contrary to <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy. But with the correct value of <code class="docutils literal notranslate"><span class="pre">tau</span></code>, you get a good solution much earlier.</p>
</div>
<div class="section" id="optimistic-initialization">
<h2>Optimistic initialization<a class="headerlink" href="#optimistic-initialization" title="Permalink to this headline">¶</a></h2>
<p>The initial estimates of 0 are now very <strong>pessimistic</strong> compared to the average reward you can get (10). This was not the case in the original setup.</p>
<p><strong>Q:</strong> Let’s change the initial value of the estimates to 10 for each action for each algorithm. What happens? Conclude on the importance of reward scaling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Exploration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">3.0</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_greedy</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_egreedy</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_softmax</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_greedy</span> <span class="o">==</span> <span class="n">Q_greedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># epsilon-greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_egreedy</span> <span class="o">==</span> <span class="n">Q_egreedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">[</span><span class="n">actions</span> <span class="o">!=</span><span class="n">a</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># Softmax            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">Q_softmax</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_29_0.png" src="../_images/4-Bandits-solution_29_0.png" />
</div>
</div>
<p><strong>A:</strong> Now we are back to quite the same results as before (greedy might still be worse, but not at chance level anymore). This shows the importance of <strong>reward scaling</strong>: the amplitude of the rewards influences a lot the success of the different methods (<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy is more robust). This mean you need to know the mean expected reward in advance, but you are not supposed to know that as you have not sampled anything at the beginning…</p>
<p>Let’s now use <strong>optimistic initialization</strong>, i.e. initialize the estimates to a much higher value than what is realistic.</p>
<p><strong>Q:</strong> Implement optimistic initialization by initializing the estimates of all three algorithms to 20. What happens?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Exploration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">3.0</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_greedy</span> <span class="o">=</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_egreedy</span> <span class="o">=</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_softmax</span> <span class="o">=</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_greedy</span> <span class="o">==</span> <span class="n">Q_greedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># epsilon-greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_egreedy</span> <span class="o">==</span> <span class="n">Q_egreedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">[</span><span class="n">actions</span> <span class="o">!=</span><span class="n">a</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># Softmax            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">Q_softmax</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_32_0.png" src="../_images/4-Bandits-solution_32_0.png" />
</div>
</div>
<p><strong>A:</strong> with optimistic initialization, greedy action selection becomes the most efficient method: exploration is ensured by the fact that all actions be be executed at some point, as they can only be disappointing. The received rewards are always lower than the expectation at the beginning, so there is no need for additional exploration mechanisms. But it necessitates to know in advance what the maximal reward is…</p>
</div>
<div class="section" id="reinforcement-comparison">
<h2>Reinforcement comparison<a class="headerlink" href="#reinforcement-comparison" title="Permalink to this headline">¶</a></h2>
<p>The problem with the previous <strong>value-based</strong> methods is that the Q-value estimates depend on the absolute magnitude of the rewards (by definition). The hyperparameters of the learning algorithms (learning rate, exploration, initial values) will therefore be very different depending on the scaling of the rewards (between 0 and 1, between -100 and 100, etc).</p>
<p>A way to get rid of this dependency is to introduce <strong>preferences</strong> <span class="math notranslate nohighlight">\(p_t(a)\)</span> for each action instead of the estimated Q-values. Preferences should follow the Q-values: an action with a high Q-value should have a high Q-value and vice versa, but we do not care about its exact scaling.</p>
<p>In <strong>reinforcement comparison</strong>, we introduce a baseline <span class="math notranslate nohighlight">\(\tilde{r}_t\)</span> which is the average received reward <strong>regardless the action</strong>, i.e. there is a single value for the whole problem. This average reward is simply updated after each action with a moving average of the received rewards:</p>
<div class="math notranslate nohighlight">
\[\tilde{r}_{t+1} = \tilde{r}_{t} + \alpha \, (r_t - \tilde{r}_{t})\]</div>
<p>The average reward is used to update the preference for the action that was just executed:</p>
<div class="math notranslate nohighlight">
\[p_{t+1}(a_t) = p_{t}(a_t) + \beta \, (r_t - \tilde{r}_{t})\]</div>
<p>If the action lead to more reward than usual, its preference should be increased (good surprise). If the action lead to less reward than usual, its preference should be decreased (bad surprise).</p>
<p>Action selection is simply a softmax over the preferences, without the temperature parameter (as we do not care about the scaling):</p>
<div class="math notranslate nohighlight">
\[
    \pi (a) = \frac{\exp p_t(a)}{ \sum_b \exp p_t(b)}
\]</div>
<p><strong>Q:</strong> Implement reinforcement comparison (with <span class="math notranslate nohighlight">\(\alpha=\beta=0.1\)</span>) and compare it to the other methods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Exploration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">alpha_rc</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">beta_rc</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_rc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_rc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">p_rc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">mean_reward</span> <span class="o">=</span> <span class="mf">0.0</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_greedy</span> <span class="o">==</span> <span class="n">Q_greedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># epsilon-greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_egreedy</span> <span class="o">==</span> <span class="n">Q_egreedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">[</span><span class="n">actions</span> <span class="o">!=</span><span class="n">a</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># Softmax            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">Q_softmax</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># Reinforcement comparison            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">p_rc</span> <span class="o">-</span> <span class="n">p_rc</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
        <span class="n">proba_rc</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_rc</span><span class="p">)</span> 
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_rc</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_rc</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        <span class="n">p_rc</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">beta_rc</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">mean_reward</span><span class="p">)</span>        
        <span class="n">mean_reward</span> <span class="o">+=</span> <span class="n">alpha_rc</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">mean_reward</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_rc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;reinforcement comparison&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;epsilon-greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_rc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;reinforcement comparison&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_35_0.png" src="../_images/4-Bandits-solution_35_0.png" />
</div>
</div>
<p><strong>A:</strong> RC is slower at the beginning, but ends up being more optimal. We never estimate the Q-values, but we do not care about them, we only want to perform the correct actions. We also get rid of the temperature parameter.</p>
<p><strong>Q:</strong> Compare all methods with optimistic initialization. The true Q-values come from <span class="math notranslate nohighlight">\(\mathcal{N}(10, 10)\)</span>, the estimated Q-values are initialized to 20 for greedy, <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy and softmax, and the average reward is initialized to 20 for RC (the preferences are initialized at 0).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Exploration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">alpha_rc</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">beta_rc</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Setup</span>
<span class="n">nb_trials</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">nb_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Store all received rewards and the optimality in a big matrix</span>
<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="n">rewards_rc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>
<span class="n">optimal_rc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_trials</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">))</span>

<span class="c1"># 200 runs</span>
<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_trials</span><span class="p">):</span>
    
    <span class="c1"># True Q-values</span>
    <span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">a_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_star</span> <span class="o">==</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Estimates</span>
    <span class="n">Q_greedy</span> <span class="o">=</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_egreedy</span> <span class="o">=</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">Q_softmax</span> <span class="o">=</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">p_rc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>
    <span class="n">mean_reward</span> <span class="o">=</span> <span class="mf">20.0</span>
    
    <span class="c1"># For 1000 plays</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>    
        
        <span class="c1"># greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_greedy</span> <span class="o">==</span> <span class="n">Q_greedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_greedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_greedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># epsilon-greedy</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_egreedy</span> <span class="o">==</span> <span class="n">Q_egreedy</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">[</span><span class="n">actions</span> <span class="o">!=</span><span class="n">a</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_egreedy</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> 
        <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_egreedy</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># Softmax            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">Q_softmax</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_softmax</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">Q_softmax</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
        
        <span class="c1"># Reinforcement comparison            </span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="n">p_rc</span> <span class="o">-</span> <span class="n">p_rc</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
        <span class="n">proba_rc</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_rc</span><span class="p">)</span> 
        <span class="n">r</span> <span class="o">=</span> <span class="n">get_reward</span><span class="p">(</span><span class="n">Q_star</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">rewards_rc</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a_star</span><span class="p">:</span>
            <span class="n">optimal_rc</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>            
        <span class="n">p_rc</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">beta_rc</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">mean_reward</span><span class="p">)</span>        
        <span class="n">mean_reward</span> <span class="o">+=</span> <span class="n">alpha_rc</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">mean_reward</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="c1"># plt.plot(np.mean(rewards_egreedy, axis=0), label=&quot;epsilon-greedy&quot;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_rc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;reinforcement comparison&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Mean Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">15</span><span class="p">,</span><span class="mi">23</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">)</span>
<span class="c1"># plt.plot(np.mean(100*optimal_egreedy, axis=0), label=&quot;epsilon-greedy&quot;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">optimal_rc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;reinforcement comparison&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal action (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_38_0.png" src="../_images/4-Bandits-solution_38_0.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./5-exercises"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>