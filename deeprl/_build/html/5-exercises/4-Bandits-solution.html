

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4.2.2. Bandits &#8212; Deep Reinforcement Learning</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-deeprl/5-exercises/4-Bandits-solution.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Bandits (part 2)" href="ex5-Bandits2.html" />
    <link rel="prev" title="4.2.1. Bandits" href="4-Bandits.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-deeprl/5-exercises/4-Bandits-solution.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Bandits" />
<meta property="og:description" content="Bandits  In this part, we will investigate the properties of the action selection schemes seen in the lecture and compare their properties:  greedy action selec" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-deeprl/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Deep Reinforcement Learning</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Deep Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tabular RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/1-Bandits.html">
   1. Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/2-MDP.html">
   2. Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/3-DP.html">
   3. Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/4-MC.html">
   4. Monte-Carlo (MC) methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/5-TD.html">
   5. Temporal Difference learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/6-FA.html">
   6. Function approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/7-NN.html">
   7. Deep learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ex1-Python.html">
   1. Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex3-Sampling.html">
   3. Sampling
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="ex4-Bandits.html">
   4. Bandits (part 1)
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="4-Bandits.html">
     4.2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4.2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex5-Bandits2.html">
   5. Bandits (part 2)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex6-DP.html">
   6. Dynamic programming
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/5-exercises/4-Bandits-solution.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/vitay/lecturenotes-deeprl/master?urlpath=tree/deeprl/5-exercises/4-Bandits-solution.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/vitay/lecturenotes-deeprl/blob/master/deeprl/5-exercises/4-Bandits-solution.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#greedy-action-selection">
   4.2.2.1. Greedy action selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#epsilon-greedy-action-selection">
   4.2.2.2.
   <span class="math notranslate nohighlight">
    \(\epsilon\)
   </span>
   -greedy action selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#softmax-action-selection">
   4.2.2.3. Softmax action selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exploration-scheduling">
   4.2.2.4. Exploration scheduling
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bandits">
<h1><span class="section-number">4.2.2. </span>Bandits<a class="headerlink" href="#bandits" title="Permalink to this headline">¶</a></h1>
<p>In this part, we will investigate the properties of the action selection schemes seen in the lecture and compare their properties:</p>
<ol class="simple">
<li><p>greedy action selection</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection</p></li>
<li><p>softmax action selection</p></li>
</ol>
<p>Let’s re-use the definitions of the last exercise:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Bandit</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    n-armed bandit.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_actions</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std_Q</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">std_r</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param nb_actions: number of arms.</span>
<span class="sd">        :param mean: mean of the normal distribution for $Q^*$.</span>
<span class="sd">        :param std_Q: standard deviation of the normal distribution for $Q^*$.</span>
<span class="sd">        :param std_r: standard deviation of the normal distribution for the sampled rewards.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Store parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_actions</span> <span class="o">=</span> <span class="n">nb_actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">std_Q</span> <span class="o">=</span> <span class="n">std_Q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">std_r</span> <span class="o">=</span> <span class="n">std_r</span>
        
        <span class="c1"># Initialize the true Q-values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">std_Q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nb_actions</span><span class="p">)</span>
        
        <span class="c1"># Optimal action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a_star</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_star</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sampled a single reward from the bandit.</span>
<span class="sd">        </span>
<span class="sd">        :param action: the selected action.</span>
<span class="sd">        :return: a reward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_star</span><span class="p">[</span><span class="n">action</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">std_r</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_actions</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

<span class="n">all_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">):</span>
        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bandit</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
    <span class="n">all_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    
<span class="n">mean_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">bandit</span><span class="o">.</span><span class="n">Q_star</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q^*(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">mean_reward</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q_t(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">bandit</span><span class="o">.</span><span class="n">Q_star</span> <span class="o">-</span> <span class="n">mean_reward</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Absolute error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_4_0.png" src="../_images/4-Bandits-solution_4_0.png" />
</div>
</div>
<div class="section" id="greedy-action-selection">
<h2><span class="section-number">4.2.2.1. </span>Greedy action selection<a class="headerlink" href="#greedy-action-selection" title="Permalink to this headline">¶</a></h2>
<p>In <strong>greedy action selection</strong>, we systematically chose the action with the highest estimated Q-value at each play (or randomly when there are ties):</p>
<div class="math notranslate nohighlight">
\[a_t = \text{argmax}_a Q_t(a)\]</div>
<p>We maintain estimates <span class="math notranslate nohighlight">\(Q_t\)</span> of the action values (initialized to 0) using the online formula:</p>
<div class="math notranslate nohighlight">
\[Q_{t+1}(a_t) = Q_t(a_t) + \alpha \, (r_{t} - Q_t(a_t))\]</div>
<p>when receiving the sampled reward <span class="math notranslate nohighlight">\(r_t\)</span> after taking the action <span class="math notranslate nohighlight">\(a_t\)</span>. The learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> can be set to 0.1 at first.</p>
<p>The algorithm simply alternates between these two steps for 1000 plays (or steps): take an action, update its Q-value.</p>
<p><strong>Q:</strong> Implement the greedy algorithm on the 5-armed bandit.</p>
<p>Your algorithm will look like this:</p>
<ul class="simple">
<li><p>Create a 5-armed bandit (mean of zero, variance of 1).</p></li>
<li><p>Initialize the estimated Q-values to 0 with an array of the same size as the bandit.</p></li>
<li><p><strong>for</strong> 1000 plays:</p>
<ul>
<li><p>Select the greedy action <span class="math notranslate nohighlight">\(a_t^*\)</span> using the current estimates.</p></li>
<li><p>Sample a reward from <span class="math notranslate nohighlight">\(\mathcal{N}(Q^*(a_t^*), 1)\)</span>.</p></li>
<li><p>Update the estimated Q-value of the action taken.</p></li>
</ul>
</li>
</ul>
<p>Additionally, you will store the received rewards at each step in an initially empty list or a numpy array of the correct size and plot it in the end. You will also plot the true Q-values and the estimated Q-values at the end of the 1000 plays.</p>
<p><em>Tip:</em> to implement the argmax, do not rely on <code class="docutils literal notranslate"><span class="pre">np.argmax()</span></code>. If there are ties in the array, for example at the beginning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">x.argmax()</span></code> will return you the <strong>first occurrence</strong> of the maximum 0.0 of the array. In this case it will be the index 0, so you will always select the action 0 first.</p>
<p>It is much more efficient to retrieve the indices of <strong>all</strong> maxima and randomly select one of them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">np.where(x</span> <span class="pre">==</span> <span class="pre">x.max())</span></code> returns a list of indices where <code class="docutils literal notranslate"><span class="pre">x</span></code> is maximum. <code class="docutils literal notranslate"><span class="pre">rng.choice()</span></code> randomly selects one of them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Bandit</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

<span class="c1"># Estimates</span>
<span class="n">Q_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

<span class="c1"># Store the rewards after each step</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># For 1000 plays</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    
    <span class="c1"># Select the action greedily w.r.t Q_t</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Q_t</span> <span class="o">==</span> <span class="n">Q_t</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Sample the reward</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">bandit</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    
    <span class="c1"># Store the received reward</span>
    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    
    <span class="c1"># Update the Q-value estimate of the action</span>
    <span class="n">Q_t</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">Q_t</span><span class="p">[</span><span class="n">action</span><span class="p">])</span>
    
<span class="c1"># Plot the Q-values and the evolution of rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">bandit</span><span class="o">.</span><span class="n">Q_star</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q^*(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">Q_t</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q_t(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_6_0.png" src="../_images/4-Bandits-solution_6_0.png" />
</div>
</div>
<p><strong>Q:</strong> Re-run your algorithm multiple times with different values of <span class="math notranslate nohighlight">\(Q^*\)</span> (simply recreate the <code class="docutils literal notranslate"><span class="pre">Bandit</span></code>) and observe:</p>
<ol class="simple">
<li><p>How much reward you get.</p></li>
<li><p>How your estimated Q-values in the end differ from the true Q-values.</p></li>
<li><p>Whether greedy action action selection finds the optimal action or not.</p></li>
</ol>
<p><strong>A:</strong> The plot with rewards is very noisy, you do not really see whether you have learned something because of the randomness of the rewards. More often than not, greedy action selection finds the optimal action, or least a not-that-bad action. The estimates <code class="docutils literal notranslate"><span class="pre">Q_t</span></code> have however nothing to see with the true Q-values, as you quickly select the same action and never update the other ones.</p>
<p>Before going further, let’s turn the agent into a class for better reusability.</p>
<p><strong>Q:</strong> Create a <code class="docutils literal notranslate"><span class="pre">GreedyAgent</span></code> class taking the bandit as an argument as well as the learning rate <code class="docutils literal notranslate"><span class="pre">alpha=0.1</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">GreedyAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>The constructor should initialize the array of estimated Q-values <code class="docutils literal notranslate"><span class="pre">Q_t</span></code> and store it as an attribute.</p>
<p>Define a method <code class="docutils literal notranslate"><span class="pre">act(self)</span></code> that returns the index of the greedy action based on the current estimates, as well as a method <code class="docutils literal notranslate"><span class="pre">update(self,</span> <span class="pre">action,</span> <span class="pre">reward)</span></code> that allows to update the estimated Q-value of the action given the obtained reward. Define also a <code class="docutils literal notranslate"><span class="pre">train(self,</span> <span class="pre">nb_steps)</span></code> method that implements the complete training process for <code class="docutils literal notranslate"><span class="pre">nb_steps=1000</span></code> plays and returns the list of obtained rewards.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GreedyAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="c1"># TODO</span>
        
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>      
        <span class="n">action</span> <span class="o">=</span> <span class="c1"># TODO</span>
        <span class="k">return</span> <span class="n">action</span>
        
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="c1"># TODO</span>
        
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">):</span>
        <span class="c1"># TODO</span>
</pre></div>
</div>
<p>Re-run the experiment using this Greedy agent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GreedyAgent</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">bandit</span> <span class="o">=</span> <span class="n">bandit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        
        <span class="c1"># Estimated Q-values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bandit</span><span class="o">.</span><span class="n">nb_actions</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="n">action</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">action</span>
        
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span><span class="p">[</span><span class="n">action</span><span class="p">])</span>
    
        
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">):</span>
        
        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_steps</span><span class="p">):</span>

            <span class="c1"># Select the action </span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">()</span>

            <span class="c1"># Sample the reward</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

            <span class="c1"># Store the received reward</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

            <span class="c1"># Update the Q-value estimate of the action</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Bandit</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

<span class="c1"># Estimates</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">GreedyAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Train for 1000 plays</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    
<span class="c1"># Plot the Q-values and the evolution of rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">bandit</span><span class="o">.</span><span class="n">Q_star</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q^*(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">agent</span><span class="o">.</span><span class="n">Q_t</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q_t(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_11_0.png" src="../_images/4-Bandits-solution_11_0.png" />
</div>
</div>
<p><strong>Q:</strong> Modify the <code class="docutils literal notranslate"><span class="pre">train()</span></code> method so that it also returns a list of binary values (0 and 1) indicating for each play whether the agent chose the optimal action. Plot this list and observe the lack of exploration.</p>
<p><em>Hint:</em> the index of the optimal action is already stored in the bandit: <code class="docutils literal notranslate"><span class="pre">bandit.a_star</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GreedyAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">bandit</span> <span class="o">=</span> <span class="n">bandit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        
        <span class="c1"># Estimated Q-values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bandit</span><span class="o">.</span><span class="n">nb_actions</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="n">action</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">action</span>
        
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span><span class="p">[</span><span class="n">action</span><span class="p">])</span>
        
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_steps</span><span class="p">):</span>
        
        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">optimal</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>

            <span class="c1"># Select the action </span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">()</span>

            <span class="c1"># Sample the reward</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

            <span class="c1"># Store the received reward</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
            
            <span class="c1"># Optimal action</span>
            <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">bandit</span><span class="o">.</span><span class="n">a_star</span><span class="p">:</span>
                <span class="n">optimal</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">optimal</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

            <span class="c1"># Update the Q-value estimate of the action</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">optimal</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Bandit</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

<span class="c1"># Estimates</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">GreedyAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Store the rewards after each step</span>
<span class="n">rewards</span><span class="p">,</span> <span class="n">optimal</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    
<span class="c1"># Plot the Q-values and the evolution of rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">bandit</span><span class="o">.</span><span class="n">Q_star</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q^*(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">),</span> <span class="n">agent</span><span class="o">.</span><span class="n">Q_t</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Actions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Q_t(a)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_14_0.png" src="../_images/4-Bandits-solution_14_0.png" />
</div>
</div>
<p>The evolution of the received rewards and optimal actions does not give a clear indication of the successful learning, as it is strongly dependent on the true Q-values. To truly estimate the performance of the algorithm, we have to average these results over many runs, e.g. 200.</p>
<p><strong>Q:</strong> Run the learning procedure 200 times (new bandit and agent every time) and average the results. Give a unique name to these arrays (e.g. <code class="docutils literal notranslate"><span class="pre">rewards_greedy</span></code> and <code class="docutils literal notranslate"><span class="pre">optimal_greedy</span></code>) as we will do comparisons later. Compare the results with the lecture, where a 10-armed bandit was used.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of arms</span>
<span class="n">nb_actions</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">optimal_greedy</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>

    <span class="c1"># Bandit</span>
    <span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

    <span class="c1"># Estimates</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">GreedyAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="c1"># Store the rewards after each step</span>
    <span class="n">rewards</span><span class="p">,</span> <span class="n">optimal</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    
    <span class="n">rewards_greedy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">optimal_greedy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimal</span><span class="p">)</span>
    
<span class="n">rewards_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">optimal_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
<span class="c1"># Plot the Q-values and the evolution of rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_greedy</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_16_0.png" src="../_images/4-Bandits-solution_16_0.png" />
</div>
</div>
<p><strong>A:</strong> the greedy agent selects the optimal action around 80% of the time, vs. 50% for the 10-armed bandits. It is really not bad knowing that it starts at chance level (20% for 5 actions).</p>
</div>
<div class="section" id="epsilon-greedy-action-selection">
<h2><span class="section-number">4.2.2.2. </span><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection<a class="headerlink" href="#epsilon-greedy-action-selection" title="Permalink to this headline">¶</a></h2>
<p>The main drawback of greedy action selection is that it does not explore: as soon as it finds an action better than the others (with a sufficiently positive true Q-value, i.e. where the sampled rewards are mostly positive), it will keep selecting that action and avoid exploring the other options.</p>
<p>The estimated Q-value of the selected action will end up being quite correct, but those of the other actions will stay at 0.</p>
<p>In <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection, the greedy action <span class="math notranslate nohighlight">\(a_t^*\)</span> (with the highest estimated Q-value) will be selected with a probability <span class="math notranslate nohighlight">\(1-\epsilon\)</span>, the others with a probability of <span class="math notranslate nohighlight">\(\epsilon\)</span> altogether.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \pi(a) = \begin{cases} 1 - \epsilon \; \text{if} \; a = a_t^* \\ \frac{\epsilon}{|\mathcal{A}| - 1} \; \text{otherwise.} \end{cases}
\end{split}\]</div>
<p>If you have <span class="math notranslate nohighlight">\(|\mathcal{A}| = 5\)</span> actions, the four non-greedy actions will be selected with a probability of <span class="math notranslate nohighlight">\(\frac{\epsilon}{4}\)</span>.</p>
<p><strong>Q:</strong> Create a <code class="docutils literal notranslate"><span class="pre">EpsilonGreedyAgent</span></code> (possibly inheriting from <code class="docutils literal notranslate"><span class="pre">GreedyAgent</span></code> to reuse code) to implement <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection (with <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span> at first). Do not overwrite the arrays previously calculated (mean reward and optimal actions), as you will want to compare the two methods in a single plot.</p>
<p>To implement <span class="math notranslate nohighlight">\(\epsilon-\)</span>greedy, you need to:</p>
<ol class="simple">
<li><p>Select the greedy action <span class="math notranslate nohighlight">\(a = a^*_t\)</span>.</p></li>
<li><p>Draw a random number between 0 and 1 (<code class="docutils literal notranslate"><span class="pre">rng.random()</span></code>).</p></li>
<li><p>If this number is smaller than <span class="math notranslate nohighlight">\(\epsilon\)</span>, you need to select another action randomly in the remaining ones (<code class="docutils literal notranslate"><span class="pre">rng.choice()</span></code>).</p></li>
<li><p>Otherwise, keep the greedy action.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EpsilonGreedyAgent</span><span class="p">(</span><span class="n">GreedyAgent</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        
        <span class="c1"># List of actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bandit</span><span class="o">.</span><span class="n">nb_actions</span><span class="p">)</span>
        
        <span class="c1"># Call the constructor of GreedyAgent</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="n">action</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="k">if</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">!=</span> <span class="n">action</span><span class="p">])</span>
            
        <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of arms</span>
<span class="n">nb_actions</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Epsilon for exploration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">rewards_egreedy</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">optimal_egreedy</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>

    <span class="c1"># Bandit</span>
    <span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

    <span class="c1"># Estimates</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">EpsilonGreedyAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="c1"># Store the rewards after each step</span>
    <span class="n">rewards</span><span class="p">,</span> <span class="n">optimal</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    
    <span class="n">rewards_egreedy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">optimal_egreedy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimal</span><span class="p">)</span>
    
<span class="n">rewards_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">optimal_egreedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
<span class="c1"># Plot the Q-values and the evolution of rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\epsilon$-Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\epsilon$-Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_20_0.png" src="../_images/4-Bandits-solution_20_0.png" />
</div>
</div>
<p><strong>Q:</strong> Compare the properties of greedy and <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy (speed, optimality, etc). Vary the value of the parameter <span class="math notranslate nohighlight">\(\epsilon\)</span> (0.0001 until 0.5) and conclude.</p>
<p><strong>A:</strong> Depending on the value of <span class="math notranslate nohighlight">\(\epsilon\)</span>, <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy can perform better that greedy in the end, but will necessitate more time at the beginning. If there is too much exploration, <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy can be even worse than greedy.</p>
</div>
<div class="section" id="softmax-action-selection">
<h2><span class="section-number">4.2.2.3. </span>Softmax action selection<a class="headerlink" href="#softmax-action-selection" title="Permalink to this headline">¶</a></h2>
<p>To avoid exploring actions which are clearly not optimal, another useful algorithm is <strong>softmax action selection</strong>. In this scheme, the estimated Q-values are ransformed into a probability distribution using the softmax opertion:</p>
<div class="math notranslate nohighlight">
\[
    \pi(a) = \frac{\exp \frac{Q_t(a)}{\tau}}{ \sum_b \exp \frac{Q_t(b)}{\tau}}
\]</div>
<p>For each action, the term <span class="math notranslate nohighlight">\(\exp \frac{Q_t(a)}{\tau}\)</span> is proportional to <span class="math notranslate nohighlight">\(Q_t(a)\)</span> but made positive. These terms are then normalized by the denominator in order to obtain a sum of 1, i.e. they are the parameters of a discrete probability distribution. The temperature <span class="math notranslate nohighlight">\(\tau\)</span> controls the level of exploration just as <span class="math notranslate nohighlight">\(\epsilon\)</span> for <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy.</p>
<p>In practice, <span class="math notranslate nohighlight">\(\exp \frac{Q_t(a)}{\tau}\)</span> can be very huge if the Q-values are high or the temperature is small, creating numerical instability (NaN). It is much more stable to substract the maximal Q-value from all Q-values before applying the softmax:</p>
<div class="math notranslate nohighlight">
\[
    \pi(a) = \frac{\exp \displaystyle\frac{Q_t(a) - \max_a Q_t(a)}{\tau}}{ \sum_b \exp \displaystyle\frac{Q_t(b) - \max_b Q_t(b)}{\tau}}
\]</div>
<p>This way, <span class="math notranslate nohighlight">\(Q_t(a) - \max_a Q_t(a)\)</span> is always negative, so its exponential is between 0 and 1.</p>
<p><strong>Q:</strong> Implement the softmax action selection (with <span class="math notranslate nohighlight">\(\tau=0.5\)</span> at first) and compare its performance to greedy and <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy. Vary the temperature <span class="math notranslate nohighlight">\(\tau\)</span> and find the best possible value. Conclude.</p>
<p><em>Hint:</em> To select actions with different probabilities, check the doc of <code class="docutils literal notranslate"><span class="pre">rng.choice()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SoftmaxAgent</span><span class="p">(</span><span class="n">GreedyAgent</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        
        <span class="c1"># List of actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bandit</span><span class="o">.</span><span class="n">nb_actions</span><span class="p">)</span>
        
        <span class="c1"># Call the constructor of GreedyAgent</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
        
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>
        
        <span class="n">action</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
            
        <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of arms</span>
<span class="n">nb_actions</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Tau for exploration</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">rewards_softmax</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">optimal_softmax</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>

    <span class="c1"># Bandit</span>
    <span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

    <span class="c1"># Estimates</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">SoftmaxAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>

    <span class="c1"># Store the rewards after each step</span>
    <span class="n">rewards</span><span class="p">,</span> <span class="n">optimal</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    
    <span class="n">rewards_softmax</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">optimal_softmax</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimal</span><span class="p">)</span>
    
<span class="n">rewards_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">optimal_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
<span class="c1"># Plot the Q-values and the evolution of rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\epsilon$-Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\epsilon$-Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_25_0.png" src="../_images/4-Bandits-solution_25_0.png" />
</div>
</div>
<p><strong>A:</strong> softmax loses less time than <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy exploring the really bad solutions, so it is optimal earlier. It can be more efficient and optimal than the other methods, but finding the right value for <span class="math notranslate nohighlight">\(\tau\)</span> (0.1 works well) is difficult: its optimum value depends on the scaling of Q, you cannot know it in advance…</p>
</div>
<div class="section" id="exploration-scheduling">
<h2><span class="section-number">4.2.2.4. </span>Exploration scheduling<a class="headerlink" href="#exploration-scheduling" title="Permalink to this headline">¶</a></h2>
<p>The problem with this version of softmax (with a constant temperature) is that even after it has found the optimal action, it will still explore the other ones (although more rarely than at the beginning). The solution is to <strong>schedule</strong> the exploration parameter so that it explores a lot at the beginning (high temperature) and gradually switches to more exploitation (low temperature).</p>
<p>Many schemes are possible for that, the simplest one (<strong>exponential decay</strong>) being to multiply the value of <span class="math notranslate nohighlight">\(\tau\)</span> by a number very close to 1 after <strong>each</strong> play:</p>
<div class="math notranslate nohighlight">
\[\tau = \tau \times (1 - \tau_\text{decay})\]</div>
<p><strong>Q:</strong> Implement in a class <code class="docutils literal notranslate"><span class="pre">SoftmaxScheduledAgent</span></code> temperature scheduling for the softmax algorithm (<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy would be similar) with <span class="math notranslate nohighlight">\(\tau=1\)</span> initially and <span class="math notranslate nohighlight">\(\tau_\text{decay} = 0.01\)</span> (feel free to change these values). Plot the evolution of <code class="docutils literal notranslate"><span class="pre">tau</span></code> and of the standard deviation of the choices of the optimal action. Conclude.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SoftmaxScheduledAgent</span><span class="p">(</span><span class="n">SoftmaxAgent</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">tau_decay</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau_decay</span> <span class="o">=</span> <span class="n">tau_decay</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">tau_history</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># List of actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bandit</span><span class="o">.</span><span class="n">nb_actions</span><span class="p">)</span>
        
        <span class="c1"># Call the constructor of GreedyAgent</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
    
        
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="c1"># Action selection</span>
        <span class="n">logit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_t</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">proba_softmax</span> <span class="o">=</span> <span class="n">logit</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logit</span><span class="p">)</span>        
        <span class="n">action</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">proba_softmax</span><span class="p">)</span> 
        
        <span class="c1"># Decay tau</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau_decay</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Tau for exploration</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">tau_decay</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">rewards_softmaxscheduled</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">optimal_softmaxscheduled</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>

    <span class="c1"># Bandit</span>
    <span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

    <span class="c1"># Estimates</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">SoftmaxScheduledAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">tau_decay</span><span class="p">)</span>

    <span class="c1"># Store the rewards after each step</span>
    <span class="n">rewards</span><span class="p">,</span> <span class="n">optimal</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    
    <span class="n">rewards_softmaxscheduled</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">optimal_softmaxscheduled</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimal</span><span class="p">)</span>
    
    
<span class="n">rewards_softmaxscheduled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmaxscheduled</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">optimal_softmaxscheduled_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">optimal_softmaxscheduled</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">optimal_softmaxscheduled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">optimal_softmaxscheduled</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
<span class="c1"># Plot the Q-values and the evolution of rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\epsilon$-Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_softmaxscheduled</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax (scheduled)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\epsilon$-Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_softmaxscheduled</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax (scheduled)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">tau_history</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\t</span><span class="s2">au$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_softmaxscheduled_std</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Variance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_29_0.png" src="../_images/4-Bandits-solution_29_0.png" />
</div>
</div>
<p><strong>A:</strong> Scheduling drastically improves how often the optimal action is selected. In terms of mean reward, the difference is not that big, as there is often a “second best” action whose expected reward is close. We can see that the variance of the optimal action selection follows the parameter <span class="math notranslate nohighlight">\(\tau\)</span>.</p>
<p><strong>Q:</strong> Experiment with different schedules (initial values, decay rate) and try to find the best setting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Tau for exploration</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">10.0</span>
<span class="n">tau_decay</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="n">rewards_softmaxscheduled</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">optimal_softmaxscheduled</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>

    <span class="c1"># Bandit</span>
    <span class="n">bandit</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">)</span>

    <span class="c1"># Estimates</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">SoftmaxScheduledAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">tau_decay</span><span class="p">)</span>

    <span class="c1"># Store the rewards after each step</span>
    <span class="n">rewards</span><span class="p">,</span> <span class="n">optimal</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    
    <span class="n">rewards_softmaxscheduled</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">optimal_softmaxscheduled</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimal</span><span class="p">)</span>
    
    
<span class="n">rewards_softmaxscheduled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_softmaxscheduled</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">optimal_softmaxscheduled_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">optimal_softmaxscheduled</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">optimal_softmaxscheduled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">optimal_softmaxscheduled</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
<span class="c1"># Plot the Q-values and the evolution of rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_egreedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\epsilon$-Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_softmax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_softmaxscheduled</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax (scheduled)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_egreedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\epsilon$-Greedy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_softmax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_softmaxscheduled</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Softmax (scheduled)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Optimal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">tau_history</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\t</span><span class="s2">au$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_softmaxscheduled_std</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Plays&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Variance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4-Bandits-solution_32_0.png" src="../_images/4-Bandits-solution_32_0.png" />
</div>
</div>
<p><strong>A:</strong> no unique answer here, but a very high exploration parameter initially which decreases quite fast leads to very performant solutions. Take-home message: scheduling is very important, but it is quite difficult to find the optimal schedule.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./5-exercises"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="4-Bandits.html" title="previous page"><span class="section-number">4.2.1. </span>Bandits</a>
    <a class='right-next' id="next-link" href="ex5-Bandits2.html" title="next page"><span class="section-number">5. </span>Bandits (part 2)</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>