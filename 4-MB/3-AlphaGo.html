
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. AlphaGo &#8212; Deep Reinforcement Learning</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-deeprl/4-MB/3-AlphaGo.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1. Introduction to Python" href="../5-exercises/ex1-Python.html" />
    <link rel="prev" title="2. Learned world models" href="2-LearnedModels.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-deeprl/4-MB/3-AlphaGo.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="AlphaGo" />
<meta property="og:description" content="AlphaGo  The game of Go  Note  Play Go in Chemnitz:  https://www.facebook.com/GoClubChemnitz/    The Goban.  Go is an ancient two-opponents board game, where ea" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-deeprl/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Deep Reinforcement Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Deep Reinforcement Learning
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Tabular RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/1-Bandits.html">
   1. Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/2-MDP.html">
   2. Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/3-DP.html">
   3. Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/4-MC.html">
   4. Monte-Carlo (MC) methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/5-TD.html">
   5. Temporal Difference learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/6-FA.html">
   6. Function approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-tabular/7-NN.html">
   7. Deep learning
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Model-free RL
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/1-DQN.html">
   1. Deep Q-Learning (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/2-BeyondDQN.html">
   2. Beyond DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/3-PG.html">
   3. Policy gradient (PG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/4-A3C.html">
   4. Advantage actor-critic (A2C, A3C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/5-DDPG.html">
   5. Deep Deterministic Policy Gradient (DDPG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/6-PPO.html">
   6. Natural gradients (TRPO, PPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-MF/7-SAC.html">
   7. Maximum Entropy RL (SAC)
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Model-based RL
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1-MB.html">
   1. Model-based RL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-LearnedModels.html">
   2. Learned world models
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. AlphaGo
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex3-Sampling.html">
   3. Sampling
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-Sampling.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-Sampling-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex4-Bandits.html">
   4. Bandits (part 1)
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-Bandits.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-Bandits-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex5-Bandits2.html">
   5. Bandits (part 2)
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Bandits2.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Bandits2-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex6-DP.html">
   6. Dynamic programming
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-DP.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-DP-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex7-Gym.html">
   7. Gym environments
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-Gym.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-Gym-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex8-MC.html">
   8. Monte-Carlo control
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MonteCarlo.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MonteCarlo-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex9-TD.html">
   9. Q-learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-TD.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-TD-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex10-Eligibilitytraces.html">
   10. Eligibility traces
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-Eligibilitytraces.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-Eligibilitytraces-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex11-Keras.html">
   11. Keras
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-Keras.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-Keras-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex12-DQN.html">
   12. DQN
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-DQN.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-DQN-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/4-MB/3-AlphaGo.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-game-of-go">
   3.1. The game of Go
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#minimax-and-alpha-beta">
     3.1.1. Minimax and Alpha-Beta
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   3.2. AlphaGo
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#architecture">
     3.2.1. Architecture
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo-tree-search">
     3.2.2. Monte-Carlo Tree Search
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#selection-phase">
       3.2.2.1. Selection phase
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#expansion-phase">
       3.2.2.2. Expansion phase
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluaiton-phase">
       3.2.2.3. Evaluaiton phase
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#backup-phase">
       3.2.2.4. Backup phase
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#alphazero">
   3.3. AlphaZero
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#muzero">
   3.4. MuZero
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="alphago">
<h1><span class="section-number">3. </span>AlphaGo<a class="headerlink" href="#alphago" title="Permalink to this headline">¶</a></h1>
<div class="section" id="the-game-of-go">
<h2><span class="section-number">3.1. </span>The game of Go<a class="headerlink" href="#the-game-of-go" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Play Go in Chemnitz:</p>
<p><a class="reference external" href="https://www.facebook.com/GoClubChemnitz/">https://www.facebook.com/GoClubChemnitz/</a></p>
</div>
<div class="figure align-default" id="id14">
<a class="reference internal image-reference" href="../_images/go.png"><img alt="../_images/go.png" src="../_images/go.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.14 </span><span class="caption-text">The Goban.</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Go</strong> is an ancient two-opponents board game, where each player successively places stones on a 19x19 grid. When a stone is surrounded by four opponents, it dies. The goal is to ensure strategical position in order to cover the biggest territory. There are around <span class="math notranslate nohighlight">\(10^{170}\)</span> possible states and 250 actions available at each turn (<span class="math notranslate nohighlight">\(10^{761}\)</span> possible games), making it a much harder game than chess for a computer (35 possible actions, <span class="math notranslate nohighlight">\(10^{120}\)</span> possible games). A game lasts 150 moves on average (80 in chess). Up until 2015 and <strong>AlphaGo</strong>, Go AIs could not compete with world-class experts, and people usually considered AI would need at least another 20 years to solve it.</p>
<div class="section" id="minimax-and-alpha-beta">
<h3><span class="section-number">3.1.1. </span>Minimax and Alpha-Beta<a class="headerlink" href="#minimax-and-alpha-beta" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id15">
<a class="reference internal image-reference" href="../_images/minimax.png"><img alt="../_images/minimax.png" src="../_images/minimax.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.15 </span><span class="caption-text">Example of a game tree.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Minimax</strong> algorithm expand the whole game tree, simulating the moves of the MAX (you) and MIN (your opponent) players.
The final outcome (win or lose) is assigned to the leaves.
It allows to solve <strong>zero sum games</strong>: what MAX wins is lost by MIN, and vice-versa. We suppose MIN plays optimally (i.e. in his own interest).</p>
<div class="figure align-default" id="id16">
<a class="reference internal image-reference" href="../_images/minimax-sol.png"><img alt="../_images/minimax-sol.png" src="../_images/minimax-sol.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.16 </span><span class="caption-text">Minimax iteratively backpropagates the value of the leaves according to who plays.</span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
<p>The value of the leaves is propagated backwards to the starting position: MAX chooses the action leading to the state with the highest value, MIN does the opposite.
For most games, the tree becomes too huge for such a systematic search:</p>
<ul class="simple">
<li><p>The value of all states further than a couple of moves away are approximated by a <strong>heuristic function</strong>: the value <span class="math notranslate nohighlight">\(V(s)\)</span> of these states.</p></li>
<li><p>Obviously useless parts of the tree are pruned: <strong>Alpha-Beta</strong> algorithm.</p></li>
</ul>
<p>Alpha-Beta methods work well for simple problems where the complete game tree can be manipulated: Tic-Tac-Toe has only a couple of possible states and actions (<span class="math notranslate nohighlight">\(3^9 = 19000\)</span> states).</p>
<div class="figure align-default" id="id17">
<a class="reference internal image-reference" href="../_images/tictactoe.jpg"><img alt="../_images/tictactoe.jpg" src="../_images/tictactoe.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.17 </span><span class="caption-text">Game tree of tic-tac-toe.</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<p>It also works when precise heuristics can be derived in a reasonable time.
This is the principle of <strong>IBM DeepBlue</strong> which was the first Chess AI to beat a world champion (Garry Kasparov) in 1995.
Carefully engineered heuristics (with the help of chess masters) allowed DeepBlue to search 6 moves away what is the best situation it can arrive in.</p>
<p>But it does not work in Go because its branching factor (250 actions possible from each state) is to huge: the tree explodes very soon.
<span class="math notranslate nohighlight">\(250^{6} \approx 10^{15}\)</span>, so even if your processor evaluates 1 billion nodes per second, it would need 11 days to evaluate a single position 6 moves away…</p>
<div class="figure align-default" id="id18">
<a class="reference internal image-reference" href="../_images/gotree.png"><img alt="../_images/gotree.png" src="../_images/gotree.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.18 </span><span class="caption-text">Game (sub-)tree of Go. Source: <a class="reference external" href="https://www.quora.com/What-does-it-mean-that-AlphaGo-relied-on-Monte-Carlo-tree-search/answer/Kostis-Gourgoulias">https://www.quora.com/What-does-it-mean-that-AlphaGo-relied-on-Monte-Carlo-tree-search/answer/Kostis-Gourgoulias</a></span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="id1">
<h2><span class="section-number">3.2. </span>AlphaGo<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="section" id="architecture">
<h3><span class="section-number">3.2.1. </span>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h3>
<p>AlphaGo <a class="bibtex reference internal" href="../zreferences.html#silver2016" id="id2">[Silver et al., 2016]</a> uses four different neural networks:</p>
<ul class="simple">
<li><p>The <strong>rollout policy</strong> and the <strong>SL policy network</strong> use supervised learning to predict expert human moves in any state.</p></li>
<li><p>The <strong>RL policy network</strong> uses <strong>self-play</strong> and reinforcement learning to learn new strategies.</p></li>
<li><p>The <strong>value network</strong> learns to predict the outcome of a game (win/lose) from the current state.</p></li>
</ul>
<p>The rollout policy and the value network are used to guide stochastic tree exploration in <strong>Monte-Carlo Tree Search (MCTS)</strong> (MPC-like planning algorithm).</p>
<div class="figure align-default" id="id19">
<a class="reference internal image-reference" href="../_images/alphago.png"><img alt="../_images/alphago.png" src="../_images/alphago.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.19 </span><span class="caption-text">Architecture of AlphaGo <a class="bibtex reference internal" href="../zreferences.html#silver2016" id="id3">[Silver et al., 2016]</a>.</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<p>Supervised learning is used for bootstrapping the policy network.</p>
<p>A <strong>policy network</strong> <span class="math notranslate nohighlight">\(\rho_\sigma\)</span> is trained to predict human expert moves:</p>
<ul class="simple">
<li><p>30M expert games have been gathered: input is board configuration, output is the move played by the expert.</p></li>
<li><p>The CNN has 13 convolutional layers (5x5) and no max-pooling.</p></li>
<li><p>The accuracy at the end of learning is 57% (not bad, but not sufficient to beat experts).</p></li>
</ul>
<p>A faster <strong>rollout policy network</strong> <span class="math notranslate nohighlight">\(\rho_\pi\)</span> is also trained:</p>
<ul class="simple">
<li><p>Only one layer, views only part of the state (around the last opponent’s move).</p></li>
<li><p>Prediction accuracy of 24%.</p></li>
<li><p>Inference time is only 2 <span class="math notranslate nohighlight">\(\mu\)</span>s,  instead of 3 ms for the policy network <span class="math notranslate nohighlight">\(\rho_\sigma\)</span>.</p></li>
</ul>
<p>The SL policy network <span class="math notranslate nohighlight">\(\rho_\sigma\)</span> is used to initialize the weights of the <strong>RL policy network</strong> <span class="math notranslate nohighlight">\(\rho_\rho\)</span>, so it can start exploring from a decent policy.</p>
<ul class="simple">
<li><p>The RL policy network then plays against an <strong>older</strong> version of itself (<span class="math notranslate nohighlight">\(\approx\)</span> target network) to improve its policy, updating the weights using <strong>Policy Gradient</strong> (REINFORCE):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, R ]
\]</div>
<p>where <span class="math notranslate nohighlight">\(R\)</span> = +1 when the game is won, -1 otherwise.</p>
<p>The idea of playing against an older version of the same network (<strong>self-play</strong>) allows to learn offline.
The RL policy network already wins 85% of the time against the strongest AI at the time (Pachi), but not against expert humans.
A <strong>value network</strong> <span class="math notranslate nohighlight">\(\nu_\theta\)</span> finally learns to predict the outcome of a game (+1 when winning, -1 when losing) based on the self-play positions generated by the RL policy network.</p>
<div class="figure align-default" id="id20">
<a class="reference internal image-reference" href="../_images/alphago-policy.jpg"><img alt="../_images/alphago-policy.jpg" src="../_images/alphago-policy.jpg" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.20 </span><span class="caption-text">The policy network learns the probability of selecting different moves. Source: <a class="bibtex reference internal" href="../zreferences.html#silver2016" id="id4">[Silver et al., 2016]</a>.</span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id21">
<a class="reference internal image-reference" href="../_images/alphago-value.jpg"><img alt="../_images/alphago-value.jpg" src="../_images/alphago-value.jpg" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.21 </span><span class="caption-text">The value network learns to predict the value of any possible state under the learned policy. Source: <a class="bibtex reference internal" href="../zreferences.html#silver2016" id="id5">[Silver et al., 2016]</a>.</span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="monte-carlo-tree-search">
<h3><span class="section-number">3.2.2. </span>Monte-Carlo Tree Search<a class="headerlink" href="#monte-carlo-tree-search" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id22">
<a class="reference internal image-reference" href="../_images/mcts.png"><img alt="../_images/mcts.png" src="../_images/mcts.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.22 </span><span class="caption-text">Monte-Carlo Tree Search (MCTS). Source: <a class="bibtex reference internal" href="../zreferences.html#silver2016" id="id6">[Silver et al., 2016]</a>.</span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</div>
<p>The final AlphaGo player uses <strong>Monte-Carlo Tree Search</strong> (MCTS), which is an incremental tree search (depth-limited), biased by the Q-value of known transitions.
The game tree is traversed depth-first from the current state, but the order of the visits depends on the value of the transition.
MCTS was previously the standard approach for Go AIs, but based on expert moves only, not deep networks. One step of MCTS consists of four phases:</p>
<div class="section" id="selection-phase">
<h4><span class="section-number">3.2.2.1. </span>Selection phase<a class="headerlink" href="#selection-phase" title="Permalink to this headline">¶</a></h4>
<p>In the <strong>selection phase</strong>, a path is found in the tree of possible actions using <strong>Upper Confidence Bound</strong> (UCB).
The probability of selecting an action when sampling the tree depends on:</p>
<ul class="simple">
<li><p>Its Q-value <span class="math notranslate nohighlight">\(Q(s, a)\)</span> (as learned by MCTS): how likely this action leads to winning.</p></li>
<li><p>Its prior probability: how often human players would play it, given by the SL policy network <span class="math notranslate nohighlight">\(\rho_\sigma\)</span>.</p></li>
<li><p>Its number of visits <span class="math notranslate nohighlight">\(N(s, a)\)</span>: this ensures exploration during the sampling.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[a_t = \text{argmax}_a \, Q(s, a) + K \cdot \frac{P(s, a)}{1 + N(s, a)}\]</div>
</div>
<div class="section" id="expansion-phase">
<h4><span class="section-number">3.2.2.2. </span>Expansion phase<a class="headerlink" href="#expansion-phase" title="Permalink to this headline">¶</a></h4>
<p>In the <strong>expansion phase</strong>, a leaf state <span class="math notranslate nohighlight">\(s_L\)</span> of the game tree is reached.
The leaf is <strong>expanded</strong>, and the possible successors of that state are added to the tree.
One requires a <strong>model</strong> to know which states are possible successors, but this is very easy in Go.</p>
<div class="math notranslate nohighlight">
\[s_{t+1} = f(s_t, a_t)\]</div>
<p>The tree therefore grows every time a <strong>Monte-Carlo sampling</strong> (“episode”) is done.</p>
</div>
<div class="section" id="evaluaiton-phase">
<h4><span class="section-number">3.2.2.3. </span>Evaluaiton phase<a class="headerlink" href="#evaluaiton-phase" title="Permalink to this headline">¶</a></h4>
<p>In the <strong>evaluation phase</strong>, the leaf <span class="math notranslate nohighlight">\(s_L\)</span> is evaluated both by</p>
<ul class="simple">
<li><p>the RL value network <span class="math notranslate nohighlight">\(\nu_\theta\)</span> (how likely can we win from that state)</p></li>
<li><p>a random rollout until the end of the game using the fast rollout policy <span class="math notranslate nohighlight">\(\rho_\pi\)</span>.</p></li>
</ul>
<p>The random rollout consists in “emulating” the end of the game using the fast rollout policy network.
The rollout is of course imperfect, but complements the value network: they are more accurate together than alone!</p>
<div class="math notranslate nohighlight">
\[V(s_L) = (1 - \lambda)  \, \nu_\theta(s_L) + \lambda \, R_\text{rollout} \]</div>
<p>This solves the bias/variance trade-off.</p>
</div>
<div class="section" id="backup-phase">
<h4><span class="section-number">3.2.2.4. </span>Backup phase<a class="headerlink" href="#backup-phase" title="Permalink to this headline">¶</a></h4>
<p>In the <strong>backup phase</strong>, the Q-values of all actions taken when descending the tree are updated with the value of the leaf node:</p>
<div class="math notranslate nohighlight">
\[Q(s, a) = \frac{1}{N(s, a)} \sum_{i=1}^{n} V(s_L^i) \]</div>
<p>This is a Monte Carlo method: perform one episode and update the Q-value of all taken actions.
However, it never uses real rewards, only value estimates.
The Q-values are <strong>learned</strong> by using both the learned value of future states (value network) and internal simulations (rollout).</p>
<p>The four phases are then repeated as long as possible (time is limited in Go), to expand the game tree as efficiently as possible.
The game tree is repeatedly sampled and grows after each sample.
When the time is up, the greedy action (highest Q-value) in the initial state is chosen and played.
For the next move, the tree is reset and expanded again (MPC replanning).</p>
<p>In the end, during MCTS, only the value network <span class="math notranslate nohighlight">\(\nu_\theta\)</span>, the SL policy network <span class="math notranslate nohighlight">\(\rho_\sigma\)</span> and the fast rollout policy <span class="math notranslate nohighlight">\(\rho_\pi\)</span> are used.
The RL policy network <span class="math notranslate nohighlight">\(\rho_\rho\)</span> is only used to train the value network <span class="math notranslate nohighlight">\(\nu_\theta\)</span>. i.e. to predict which positions are interesting or not.
However, the RL policy network can discover new strategies by playing many times against itself, without relying on averaging expert moves like the previous approaches.</p>
<p>AlphaGo was able to beat Lee Sedol in 2016, 19 times World champion.
It relies on human knowledge to <strong>bootstrap</strong> a RL agent (supervised learning).
The RL agent discovers new strategies by using self-play: during the games against Lee Sedol, it was able to use <strong>novel</strong> moves which were never played before and surprised its opponent.
The neural networks are only used to guide random search using MCTS: the policy network alone is not able to beat grandmasters.
Training took several weeks on 1202 CPUs and 176 GPUs.</p>
<div class="admonition-but-is-go-that-hard-compared-to-robotics admonition">
<p class="admonition-title">But is Go that hard compared to robotics?</p>
<ol class="simple">
<li><p><strong>Fully deterministic.</strong> There is no noise in the rules of the game; if the two players take the same sequence of actions, the states along the way will always be the same.</p></li>
<li><p><strong>Fully observed.</strong> Each player has complete information and there are no hidden variables. For example, Texas hold’em does not satisfy this property because you cannot see the cards of the other player.</p></li>
<li><p>The action space is <strong>discrete</strong>. A number of unique moves are available. In contrast, in robotics you might want to instead emit continuous-valued torques at each joint.</p></li>
<li><p>We have access to a perfect <strong>simulator</strong> (the game itself), so the effects of any action are known exactly. This is a strong assumption that AlphaGo relies on quite strongly, but is also quite rare in other real-world problems.</p></li>
<li><p>Each episode/game is relatively short, of approximately 200 actions. This is a relatively <strong>short time horizon</strong> compared to other RL settings which may involve thousands (or more) of actions per episode.</p></li>
<li><p>The <strong>evaluation</strong> is clear, fast and allows a lot of trial-and-error experience. In other words, the agent can experience winning/losing millions of times, which allows is to learn, slowly but surely, as is common with deep neural network optimization.</p></li>
<li><p>There are huge datasets of human play game data available to <strong>bootstrap</strong> the learning, so AlphaGo doesn’t have to start from scratch.</p></li>
</ol>
<p>Source: <a class="reference external" href="https://medium.com/&#64;karpathy/alphago-in-context-c47718cb95a5">https://medium.com/&#64;karpathy/alphago-in-context-c47718cb95a5</a></p>
</div>
</div>
</div>
</div>
<div class="section" id="alphazero">
<h2><span class="section-number">3.3. </span>AlphaZero<a class="headerlink" href="#alphazero" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="id23">
<a class="reference internal image-reference" href="../_images/AlphaZero-perf.gif"><img alt="../_images/AlphaZero-perf.gif" src="../_images/AlphaZero-perf.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.23 </span><span class="caption-text">Performance of AlphaZero <a class="bibtex reference internal" href="../zreferences.html#silver2018" id="id7">[Silver et al., 2018]</a>. Source: <a class="reference external" href="https://deepmind.com/blog/alphago-zero-learning-scratch/">https://deepmind.com/blog/alphago-zero-learning-scratch/</a>.</span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</div>
<p>AlphaZero <a class="bibtex reference internal" href="../zreferences.html#silver2018" id="id8">[Silver et al., 2018]</a> totally skips the <strong>supervised learning</strong> part: the RL policy network starts self-play from scratch!</p>
<div class="figure align-default" id="id24">
<a class="reference internal image-reference" href="../_images/alphazero.jpg"><img alt="../_images/alphazero.jpg" src="../_images/alphazero.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.24 </span><span class="caption-text">Architecture of AlphaZero <a class="bibtex reference internal" href="../zreferences.html#silver2018" id="id9">[Silver et al., 2018]</a>.</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</div>
<p>The RL policy network uses MCTS to select moves, not a softmax-like selection as in AlphaGo.
The policy and value networks are merged into a <strong>two-headed monster</strong>: the convolutional residual layers are shared to predict both:</p>
<ul class="simple">
<li><p>The policy <span class="math notranslate nohighlight">\(\pi_\theta(s)\)</span>, which is only used to guide MCTS (prior of UCB).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[a_t = \text{argmax}_a \, Q(s, a) + K \cdot \frac{\pi_\theta(s, a)}{1 + N(s, a)}\]</div>
<ul class="simple">
<li><p>The state value <span class="math notranslate nohighlight">\(V_\varphi(s)\)</span> for the value of the leaves (no fast rollout).</p></li>
</ul>
<p>The loss function used to train the network is a <strong>compound loss</strong>:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta) = (R − V_\varphi(s))^2 - \pi_\text{MCTS}(s) \, \log \pi_\theta(s) + c ||\theta||^2
\]</div>
<p>The policy head <span class="math notranslate nohighlight">\(\pi_\theta(s)\)</span> learns to mimic the actions selected by MCTS by minimizing the cross-entropy (or KL).
The value network <span class="math notranslate nohighlight">\(V_\varphi(s)\)</span>  learns to predict the return by minimizing the mse.</p>
<div class="admonition-alphazero admonition">
<p class="admonition-title">AlphaZero</p>
<ol class="simple">
<li><p>Initialize neural network.</p></li>
<li><p>Play self-play games, using 1,600 MCTS simulations per move (which takes about 0.4 seconds).</p></li>
<li><p>Sample 2,048 positions from the most recent 500,000 games, along with whether the game was won or lost.</p></li>
<li><p>Train the neural network, using both A) the move evaluations produced by the MCTS lookahead search and B) whether the current player won or lost.</p></li>
<li><p>Finally, every 1,000 iterations of steps 3-4, evaluate the current neural network against the previous best version; if it wins at least 55% of the games, begin using it to generate self-play games instead of the prior version.</p></li>
</ol>
<p>Repeat steps 3-4 700,000 times, while the self-play games are continuously being played .</p>
<p>Source:<a class="reference external" href="https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef">https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef</a></p>
</div>
<p>By using a single network instead of four and learning faster, AlphaZero also greatly reduces the energy consumption.</p>
<div class="figure align-default" id="id25">
<a class="reference internal image-reference" href="../_images/alphazero-efficiency.png"><img alt="../_images/alphazero-efficiency.png" src="../_images/alphazero-efficiency.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.25 </span><span class="caption-text">Power consumption of AlphaZero <a class="bibtex reference internal" href="../zreferences.html#silver2018" id="id10">[Silver et al., 2018]</a>. Source: <a class="reference external" href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go">https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go</a>.</span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
<p><strong>The same algorithm can also play Chess and Shogi!</strong>
The network weights are reset for each game, but it uses the same architecture and hyperparameters.
After only 8 hours of training, AlphaZero beats Stockfish with 28-72-00, the best Chess AI at the time, which itself beats any human.
This proves the algorithm is generic and can be applied to any board game.</p>
<div class="figure align-default" id="id26">
<a class="reference internal image-reference" href="../_images/alphazero.gif"><img alt="../_images/alphazero.gif" src="../_images/alphazero.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.26 </span><span class="caption-text">AlphaZero can reach sota performance on Go, Cjess and Shogi. Source: <a class="reference external" href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go">https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go</a></span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="muzero">
<h2><span class="section-number">3.4. </span>MuZero<a class="headerlink" href="#muzero" title="Permalink to this headline">¶</a></h2>
<p>MuZero <a class="bibtex reference internal" href="../zreferences.html#schrittwieser2019" id="id11">[Schrittwieser et al., 2019]</a> is the latest extension of AlphaZero.
Instead of relying on a perfect simulator for the MCTS, it learns the dynamics model instead.</p>
<div class="math notranslate nohighlight">
\[s_{t+1}, r_{t+1} = f(s_t, a_t)\]</div>
<div class="figure align-default" id="id27">
<a class="reference internal image-reference" href="../_images/muzero.png"><img alt="../_images/muzero.png" src="../_images/muzero.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.27 </span><span class="caption-text">Architecture of MuZero <a class="bibtex reference internal" href="../zreferences.html#schrittwieser2019" id="id12">[Schrittwieser et al., 2019]</a>.</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
<p>MuZero is composed of three neural networks:</p>
<ul class="simple">
<li><p>The representation network <span class="math notranslate nohighlight">\(s= h(o_1, \ldots, o_t)\)</span> (encoder) transforming the history of observations into a state representation (<strong>latent space</strong>).</p></li>
<li><p>The dynamics model <span class="math notranslate nohighlight">\(s', r = g(s, a)\)</span> used to generate rollouts for MCTS.</p></li>
<li><p>The policy and value network <span class="math notranslate nohighlight">\(\pi, V = f(s)\)</span> learning the policy with PG.</p></li>
</ul>
<p>The dynamics model <span class="math notranslate nohighlight">\(s', r = g(s, a)\)</span> replaces the perfect simulator in MCTS.
It is used in the expansion phase of MCTS to add new nodes.
Importantly, nodes are <strong>latent representations</strong> of the observations, not observations directly.
This is a similar idea to <strong>World Models</strong> and <strong>PlaNet/Dreamer</strong>, which plan in the latent space of a VAE.
Selection in MCTS still follows an upper confidence bound using the learned policy <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<p>The actions taking during self-play are taken from the MCTS search as in AlphaZero.
Note that the network plays each turn: there is additional information about whether the network is playing white or black.
Self-played games are stored in a huge experience replay memory.</p>
<p>Finally, complete games sampled from the ERM are used to learn simultaneously the three networks <span class="math notranslate nohighlight">\(f\)</span>, <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span>:</p>
<p><img alt="" src="../_images/muzero-loss.png" /></p>
<p>MuZero beats AlphaZero on Chess, Go and Shogi, but also R2D2 on Atari games. The representation network <span class="math notranslate nohighlight">\(h\)</span> allows to encode the Atari frames in a compressed manner that allows planning over raw images.</p>
<div class="figure align-default" id="id28">
<a class="reference internal image-reference" href="../_images/muzero-results.png"><img alt="../_images/muzero-results.png" src="../_images/muzero-results.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.28 </span><span class="caption-text">Performance of MuZero <a class="bibtex reference internal" href="../zreferences.html#schrittwieser2019" id="id13">[Schrittwieser et al., 2019]</a>.</span><a class="headerlink" href="#id28" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./4-MB"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="2-LearnedModels.html" title="previous page"><span class="section-number">2. </span>Learned world models</a>
    <a class='right-next' id="next-link" href="../5-exercises/ex1-Python.html" title="next page"><span class="section-number">1. </span>Introduction to Python</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>